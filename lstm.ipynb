{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.12 64-bit ('nlp-4650-project': conda)"
    },
    "interpreter": {
      "hash": "140351e719ebabd8c89d647add28ebb80040bf2d1128912b99179fb480ea150d"
    },
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQXCAXtFvh5L",
        "outputId": "ada67459-6bb4-408c-e308-cccd5418b7bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/NLP/project"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/NLP/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbhpdgJevlV2"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWCHRlAbvtvn",
        "outputId": "0391815e-1da9-4d85-85bc-cba86eb99c9d"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.19.2)\n",
            "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.2.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (7.6.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.11.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.1.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.11.0)\n",
            "Requirement already satisfied: huggingface in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.0.1)\n",
            "Requirement already satisfied: transformers>3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (4.12.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (4.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (0.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 12)) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>3->-r requirements.txt (line 12)) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>3->-r requirements.txt (line 12)) (3.0.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 12)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 12)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 12)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 12)) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 12)) (1.7.1)\n",
            "Requirement already satisfied: jupyter-server~=1.4 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 3)) (1.12.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 3)) (4.9.1)\n",
            "Requirement already satisfied: nbclassic~=0.2 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 3)) (0.3.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 3)) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-server~=2.3 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 3)) (6.1)\n",
            "Requirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 3)) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.1->jupyterlab->-r requirements.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (5.6.1)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (5.1.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (22.3.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (7.1.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (1.8.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.12.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (21.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (5.1.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.1->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.3)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.1->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.1->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: json5 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 3)) (0.9.6)\n",
            "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 3)) (4.2.1)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 3)) (2.9.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 3)) (21.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 3)) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 3)) (0.18.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->jupyterlab-server~=2.3->jupyterlab->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from nbclassic~=0.2->jupyterlab->-r requirements.txt (line 3)) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook<7->nbclassic~=0.2->jupyterlab->-r requirements.txt (line 3)) (4.10.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r requirements.txt (line 4)) (3.5.2)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 3)) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 3)) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 3)) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyterlab->-r requirements.txt (line 3)) (0.2.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 8)) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (2.21)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter-server~=1.4->jupyterlab->-r requirements.txt (line 3)) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>3->-r requirements.txt (line 12)) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_3WDhl34Cqg"
      },
      "source": [
        "# Imports\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.data.metrics import bleu_score\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Plotting\n",
        "# for colab\n",
        "%matplotlib inline \n",
        "# for local notebook\n",
        "# %matplotlib notebook \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataset import *\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr_0il6Mm4Pr"
      },
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, vocab: torchtext.vocab.Vocab) -> None:\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "        self.vocab_stoi = vocab.get_stoi()\n",
        "        self.vocab_itos = vocab.get_itos()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index: int) -> torch.Tensor:\n",
        "        sample = self.df.iloc[index]\n",
        "        tweet = sample['text']\n",
        "        tweet_ch = [\"<BOS>\"] + list(tweet) + [\"<EOS>\"]\n",
        "        tweet_indices = [self.vocab_stoi.get(ch) or self.vocab_stoi['<UNK>'] for ch in tweet_ch]\n",
        "        tweet_tensor = torch.tensor(tweet_indices, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        return tweet_tensor"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C685c5DC-rZw"
      },
      "source": [
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers=1, use_embeds=False):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        # self.use_embeds = use_embeds\n",
        "        self.num_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.encoder = nn.Embedding(self.vocab_size, self.hidden_size)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.hidden_size,\n",
        "            self.hidden_size,\n",
        "            self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input)              # encode input using embedding layer\n",
        "        output, hidden = self.rnn(input, hidden) # get the next output and hidden state\n",
        "        output = self.decoder(output)            # predict distribution over next tokens\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxAGJ4xg_fpt"
      },
      "source": [
        "def sample_sequence(model, vocab, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "    vocab_stoi = vocab.get_stoi()\n",
        "    vocab_itos = vocab.get_itos()\n",
        "    inputs = torch.tensor([vocab_stoi[\"<BOS>\"]], dtype=torch.long, device=device)\n",
        "    hidden = model.init_hidden(1)\n",
        "    for p in range(max_len):\n",
        "        output, hidden = model(inputs.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = vocab_itos[top_i]\n",
        "        \n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char       \n",
        "        inputs = torch.tensor([top_i], dtype=torch.long, device=device)\n",
        "    return generated_sequence"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM264whjAHLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f25a17-7456-41eb-b6bc-6fcfea815ea6"
      },
      "source": [
        "df = load_dataset('elonmusk')\n",
        "tweets = df['text'].apply(lambda x: x.strip()).tolist()\n",
        "special_tokens = ['<BOS>', '<EOS>', '<UNK>']\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(tweets,\n",
        "                                                  min_freq=5,\n",
        "                                                  specials=special_tokens)\n",
        "vocab_stoi = vocab.get_stoi()\n",
        "vocab_itos = vocab.get_itos()\n",
        "vocab_size = len(vocab)\n",
        "dataset = TwitterDataset(df, vocab)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wUuCthDKJjM"
      },
      "source": [
        "def train(model, iterator, vocab, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "    model.zero_grad()\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        # get training set\n",
        "        avg_loss = 0\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "        for tweet in iterator:\n",
        "            tweet = tweet.view(1, -1).to(device)\n",
        "            target = tweet[:, 1:]\n",
        "            input = tweet[:, :-1]\n",
        "\n",
        "            # cleanup\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output, hidden = model(input, hidden)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "            hidden = hidden.detach()\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            it += 1 # increment iteration count\n",
        "            if it % print_every == 0:\n",
        "                print(\"[Epoch %d Iter %d] Loss %f\" % (e+1, it+1, float(avg_loss/print_every)))\n",
        "                print(\"    \" + sample_sequence(model, vocab))\n",
        "                avg_loss = 0"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7p2Or35KSIS"
      },
      "source": [
        "hidden_size = 256\n",
        "n_layers = 1\n",
        "batch_size = 1\n",
        "lr = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "data_iterator = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "model = TextGenerator(vocab_size, hidden_size, n_layers=n_layers).to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2UkCgibX8OB",
        "outputId": "dd63ed44-cba7-4013-fa41-0667314d8895"
      },
      "source": [
        "train(model, data_iterator, vocab, batch_size=batch_size, num_epochs=num_epochs, lr=lr, print_every=5000)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2 Iter 5001] Loss 0.742919\n",
            "    UI\"can the deen test astungely start by Dame — seened firs of Starlina lestory\n",
            "[Epoch 4 Iter 10001] Loss 0.200248\n",
            "    0\n",
            "[Epoch 5 Iter 15001] Loss 0.841226\n",
            "    \n",
            "[Epoch 7 Iter 20001] Loss 0.375952\n",
            "    , morning the Tesla Semi\n",
            "[Epoch 8 Iter 25001] Loss 0.977752\n",
            "    \n",
            "[Epoch 10 Iter 30001] Loss 0.542609\n",
            "    -69%9 us fairing there and better of the SpaceX Texas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6b2XDJLBuO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616c24f6-e76a-4618-ce4f-82684c5b6a85"
      },
      "source": [
        "print(sample_sequence(model, vocab, temperature=0.6))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " SpaceX coming soon.\n"
          ]
        }
      ]
    }
  ]
}