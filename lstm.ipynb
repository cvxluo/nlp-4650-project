{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('nlp-4650-project': conda)"
  },
  "interpreter": {
   "hash": "140351e719ebabd8c89d647add28ebb80040bf2d1128912b99179fb480ea150d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length,\n",
    "    ):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        \n",
    "        '''\n",
    "        self.index_to_word = defaultdict(list)\n",
    "        self.index_to_word = defaultdict(list)\n",
    "        for index, word in enumerate(self.uniq_words) :\n",
    "            self.index_to_word[index + 1] = word\n",
    "            self.word_to_index[word] = index + 1\n",
    "        '''\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self):\n",
    "        train_df = pd.read_csv('data/elonmusk/train.txt', error_bad_lines=False)\n",
    "        text = train_df['tweet'].str.cat(sep=' ')\n",
    "        return text\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(dataset, model, sequence_length, batch_size, max_epochs):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print({ 'epoch': epoch, 'loss': loss.item() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_8142/2351753315.py:11: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  self.words = self.load_words()\n",
      "b'Skipping line 18: expected 2 fields, saw 3\\nSkipping line 21: expected 2 fields, saw 5\\nSkipping line 28: expected 2 fields, saw 3\\nSkipping line 44: expected 2 fields, saw 3\\nSkipping line 46: expected 2 fields, saw 3\\nSkipping line 60: expected 2 fields, saw 3\\nSkipping line 74: expected 2 fields, saw 7\\nSkipping line 100: expected 2 fields, saw 5\\nSkipping line 101: expected 2 fields, saw 5\\nSkipping line 134: expected 2 fields, saw 4\\nSkipping line 147: expected 2 fields, saw 3\\nSkipping line 155: expected 2 fields, saw 5\\nSkipping line 164: expected 2 fields, saw 3\\nSkipping line 190: expected 2 fields, saw 3\\nSkipping line 213: expected 2 fields, saw 3\\nSkipping line 268: expected 2 fields, saw 4\\nSkipping line 289: expected 2 fields, saw 3\\nSkipping line 291: expected 2 fields, saw 5\\nSkipping line 317: expected 2 fields, saw 3\\nSkipping line 318: expected 2 fields, saw 3\\nSkipping line 324: expected 2 fields, saw 3\\nSkipping line 326: expected 2 fields, saw 3\\nSkipping line 332: expected 2 fields, saw 3\\nSkipping line 352: expected 2 fields, saw 3\\nSkipping line 357: expected 2 fields, saw 3\\nSkipping line 362: expected 2 fields, saw 3\\nSkipping line 383: expected 2 fields, saw 3\\nSkipping line 384: expected 2 fields, saw 4\\nSkipping line 391: expected 2 fields, saw 4\\nSkipping line 398: expected 2 fields, saw 4\\nSkipping line 406: expected 2 fields, saw 4\\nSkipping line 424: expected 2 fields, saw 4\\nSkipping line 425: expected 2 fields, saw 3\\nSkipping line 449: expected 2 fields, saw 3\\nSkipping line 451: expected 2 fields, saw 3\\nSkipping line 453: expected 2 fields, saw 3\\nSkipping line 456: expected 2 fields, saw 5\\nSkipping line 461: expected 2 fields, saw 5\\nSkipping line 464: expected 2 fields, saw 3\\nSkipping line 477: expected 2 fields, saw 3\\nSkipping line 491: expected 2 fields, saw 4\\nSkipping line 492: expected 2 fields, saw 3\\nSkipping line 514: expected 2 fields, saw 3\\nSkipping line 519: expected 2 fields, saw 3\\nSkipping line 527: expected 2 fields, saw 3\\nSkipping line 532: expected 2 fields, saw 3\\nSkipping line 536: expected 2 fields, saw 3\\nSkipping line 559: expected 2 fields, saw 3\\nSkipping line 562: expected 2 fields, saw 6\\nSkipping line 628: expected 2 fields, saw 3\\nSkipping line 639: expected 2 fields, saw 3\\nSkipping line 641: expected 2 fields, saw 3\\nSkipping line 646: expected 2 fields, saw 3\\nSkipping line 649: expected 2 fields, saw 7\\nSkipping line 686: expected 2 fields, saw 3\\nSkipping line 689: expected 2 fields, saw 5\\nSkipping line 713: expected 2 fields, saw 4\\nSkipping line 727: expected 2 fields, saw 4\\nSkipping line 731: expected 2 fields, saw 3\\nSkipping line 776: expected 2 fields, saw 4\\nSkipping line 800: expected 2 fields, saw 4\\nSkipping line 802: expected 2 fields, saw 3\\nSkipping line 816: expected 2 fields, saw 3\\nSkipping line 823: expected 2 fields, saw 3\\nSkipping line 829: expected 2 fields, saw 3\\nSkipping line 833: expected 2 fields, saw 3\\nSkipping line 843: expected 2 fields, saw 3\\nSkipping line 850: expected 2 fields, saw 3\\nSkipping line 854: expected 2 fields, saw 3\\nSkipping line 861: expected 2 fields, saw 3\\nSkipping line 862: expected 2 fields, saw 19\\nSkipping line 871: expected 2 fields, saw 3\\nSkipping line 883: expected 2 fields, saw 4\\nSkipping line 891: expected 2 fields, saw 3\\nSkipping line 893: expected 2 fields, saw 3\\nSkipping line 895: expected 2 fields, saw 3\\nSkipping line 909: expected 2 fields, saw 3\\nSkipping line 922: expected 2 fields, saw 5\\nSkipping line 938: expected 2 fields, saw 5\\nSkipping line 956: expected 2 fields, saw 3\\nSkipping line 962: expected 2 fields, saw 3\\nSkipping line 966: expected 2 fields, saw 4\\nSkipping line 967: expected 2 fields, saw 6\\nSkipping line 974: expected 2 fields, saw 3\\nSkipping line 989: expected 2 fields, saw 3\\nSkipping line 1000: expected 2 fields, saw 3\\nSkipping line 1003: expected 2 fields, saw 3\\nSkipping line 1008: expected 2 fields, saw 3\\nSkipping line 1013: expected 2 fields, saw 3\\nSkipping line 1024: expected 2 fields, saw 4\\nSkipping line 1033: expected 2 fields, saw 3\\nSkipping line 1042: expected 2 fields, saw 4\\nSkipping line 1045: expected 2 fields, saw 6\\nSkipping line 1087: expected 2 fields, saw 4\\nSkipping line 1101: expected 2 fields, saw 3\\nSkipping line 1110: expected 2 fields, saw 3\\nSkipping line 1119: expected 2 fields, saw 3\\nSkipping line 1126: expected 2 fields, saw 3\\nSkipping line 1157: expected 2 fields, saw 6\\nSkipping line 1160: expected 2 fields, saw 3\\nSkipping line 1182: expected 2 fields, saw 3\\nSkipping line 1207: expected 2 fields, saw 7\\nSkipping line 1214: expected 2 fields, saw 5\\nSkipping line 1222: expected 2 fields, saw 3\\nSkipping line 1234: expected 2 fields, saw 3\\nSkipping line 1260: expected 2 fields, saw 3\\nSkipping line 1268: expected 2 fields, saw 5\\nSkipping line 1271: expected 2 fields, saw 5\\nSkipping line 1272: expected 2 fields, saw 3\\nSkipping line 1283: expected 2 fields, saw 6\\nSkipping line 1284: expected 2 fields, saw 7\\nSkipping line 1291: expected 2 fields, saw 4\\nSkipping line 1318: expected 2 fields, saw 3\\nSkipping line 1331: expected 2 fields, saw 3\\nSkipping line 1349: expected 2 fields, saw 4\\nSkipping line 1351: expected 2 fields, saw 3\\nSkipping line 1360: expected 2 fields, saw 5\\nSkipping line 1362: expected 2 fields, saw 5\\nSkipping line 1364: expected 2 fields, saw 3\\nSkipping line 1365: expected 2 fields, saw 3\\nSkipping line 1369: expected 2 fields, saw 3\\nSkipping line 1373: expected 2 fields, saw 3\\nSkipping line 1398: expected 2 fields, saw 3\\nSkipping line 1403: expected 2 fields, saw 3\\nSkipping line 1467: expected 2 fields, saw 3\\nSkipping line 1482: expected 2 fields, saw 3\\nSkipping line 1484: expected 2 fields, saw 3\\nSkipping line 1529: expected 2 fields, saw 3\\nSkipping line 1573: expected 2 fields, saw 4\\nSkipping line 1577: expected 2 fields, saw 4\\nSkipping line 1583: expected 2 fields, saw 3\\nSkipping line 1605: expected 2 fields, saw 4\\nSkipping line 1607: expected 2 fields, saw 4\\nSkipping line 1634: expected 2 fields, saw 3\\nSkipping line 1635: expected 2 fields, saw 3\\nSkipping line 1660: expected 2 fields, saw 3\\nSkipping line 1668: expected 2 fields, saw 4\\nSkipping line 1672: expected 2 fields, saw 3\\nSkipping line 1681: expected 2 fields, saw 4\\nSkipping line 1685: expected 2 fields, saw 3\\nSkipping line 1688: expected 2 fields, saw 3\\nSkipping line 1701: expected 2 fields, saw 3\\nSkipping line 1710: expected 2 fields, saw 3\\nSkipping line 1711: expected 2 fields, saw 4\\nSkipping line 1712: expected 2 fields, saw 3\\nSkipping line 1722: expected 2 fields, saw 3\\nSkipping line 1723: expected 2 fields, saw 4\\nSkipping line 1727: expected 2 fields, saw 4\\nSkipping line 1744: expected 2 fields, saw 3\\nSkipping line 1745: expected 2 fields, saw 3\\nSkipping line 1754: expected 2 fields, saw 3\\nSkipping line 1766: expected 2 fields, saw 3\\nSkipping line 1806: expected 2 fields, saw 3\\nSkipping line 1808: expected 2 fields, saw 4\\nSkipping line 1816: expected 2 fields, saw 3\\nSkipping line 1818: expected 2 fields, saw 3\\nSkipping line 1831: expected 2 fields, saw 3\\nSkipping line 1870: expected 2 fields, saw 3\\nSkipping line 1877: expected 2 fields, saw 5\\nSkipping line 1878: expected 2 fields, saw 3\\nSkipping line 1883: expected 2 fields, saw 3\\nSkipping line 1900: expected 2 fields, saw 3\\nSkipping line 1902: expected 2 fields, saw 3\\nSkipping line 1905: expected 2 fields, saw 4\\nSkipping line 1913: expected 2 fields, saw 3\\nSkipping line 1916: expected 2 fields, saw 3\\nSkipping line 1939: expected 2 fields, saw 3\\nSkipping line 1942: expected 2 fields, saw 3\\nSkipping line 1982: expected 2 fields, saw 3\\nSkipping line 1986: expected 2 fields, saw 3\\nSkipping line 1987: expected 2 fields, saw 3\\nSkipping line 2002: expected 2 fields, saw 4\\nSkipping line 2034: expected 2 fields, saw 8\\nSkipping line 2039: expected 2 fields, saw 3\\nSkipping line 2043: expected 2 fields, saw 4\\nSkipping line 2050: expected 2 fields, saw 3\\nSkipping line 2069: expected 2 fields, saw 3\\nSkipping line 2083: expected 2 fields, saw 3\\nSkipping line 2093: expected 2 fields, saw 3\\nSkipping line 2097: expected 2 fields, saw 4\\nSkipping line 2100: expected 2 fields, saw 3\\nSkipping line 2129: expected 2 fields, saw 3\\nSkipping line 2130: expected 2 fields, saw 3\\nSkipping line 2134: expected 2 fields, saw 3\\nSkipping line 2138: expected 2 fields, saw 3\\nSkipping line 2142: expected 2 fields, saw 4\\nSkipping line 2151: expected 2 fields, saw 3\\nSkipping line 2153: expected 2 fields, saw 3\\nSkipping line 2162: expected 2 fields, saw 3\\n'\n",
      "{'epoch': 0, 'loss': 2.062647819519043}\n",
      "{'epoch': 1, 'loss': 1.7808923721313477}\n",
      "{'epoch': 2, 'loss': 1.5736982822418213}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'Space'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8142/821874948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Space'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8142/2693321542.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(dataset, model, text, next_words)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8142/2693321542.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Space'"
     ]
    }
   ],
   "source": [
    "sequence_length = 16\n",
    "batch_size = 100\n",
    "max_epochs = 3\n",
    "\n",
    "dataset = Dataset(sequence_length)\n",
    "model = Model(dataset_\n",
    "\n",
    "train\n",
    "\n",
    "train(dataset, sequence_length, batch_size, max_epochsl, args)\n",
    "print(predict(dataset, model, text=')pace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TZ*SS> TEStriserda hout besoly <EOS> <BOS> To to to the boonshallcriviancetter to slrygo on thre Lalt\n"
     ]
    }
   ],
   "source": [
    "print(''.join(predict(dataset, model, text='T')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}