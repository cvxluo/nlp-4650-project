{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.12 64-bit ('nlp-4650-project': conda)"
    },
    "interpreter": {
      "hash": "140351e719ebabd8c89d647add28ebb80040bf2d1128912b99179fb480ea150d"
    },
    "colab": {
      "name": "lstm2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbhpdgJevlV2"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0F2jtBm5hMp",
        "outputId": "75eaf6e2-a5c4-49a3-e365-43208df2a09a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/NLP/project"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/NLP/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWCHRlAbvtvn",
        "outputId": "e854e2b1-f4c7-4ac2-f086-d54f7d4b52bc"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.11.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.1.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: huggingface in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.0.1)\n",
            "Requirement already satisfied: transformers>3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (4.12.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.16.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.10.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (4.62.3)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (0.3.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 8)) (0.2.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 8)) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 8)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 8)) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 8)) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>3->-r requirements.txt (line 8)) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>3->-r requirements.txt (line 8)) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>3->-r requirements.txt (line 8)) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 9)) (3.8.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 9)) (2021.11.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 9)) (0.70.12.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 9)) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 9)) (0.3.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 8)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>3->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (4.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (1.7.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 9)) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>3->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>3->-r requirements.txt (line 8)) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24aidOZEzgMe"
      },
      "source": [
        "# Train with No Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5tjwZ5XIf3o",
        "outputId": "24fdeab9-6ba0-4173-d49c-ba03b46e66ec"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"elonmusk\" \\\n",
        "  --num_epochs 7 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --embedding_dim 300 \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=7, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=False, username='elonmusk')\n",
            "12/03/2021 20:11:11 - INFO - __main__ -   [Iter 2000] Loss 7.05  Val Loss: 6.84  Val Perplexity: 935.69\n",
            "12/03/2021 20:11:19 - INFO - __main__ -   [Iter 4000] Loss 5.68  Val Loss: 6.86  Val Perplexity: 957.50\n",
            "12/03/2021 20:11:27 - INFO - __main__ -   [Iter 6000] Loss 4.43  Val Loss: 7.17  Val Perplexity: 1303.53\n",
            "12/03/2021 20:11:36 - INFO - __main__ -   [Iter 8000] Loss 3.24  Val Loss: 7.38  Val Perplexity: 1602.83\n",
            "12/03/2021 20:11:44 - INFO - __main__ -   [Iter 10000] Loss 3.32  Val Loss: 7.46  Val Perplexity: 1738.93\n",
            "12/03/2021 20:11:52 - INFO - __main__ -   [Iter 12000] Loss 2.44  Val Loss: 7.79  Val Perplexity: 2410.31\n",
            "12/03/2021 20:12:01 - INFO - __main__ -   [Iter 14000] Loss 1.81  Val Loss: 8.02  Val Perplexity: 3047.44\n",
            "12/03/2021 20:12:09 - INFO - __main__ -   [Iter 16000] Loss 1.40  Val Loss: 8.28  Val Perplexity: 3936.40\n",
            "12/03/2021 20:12:16 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 20:12:16 - INFO - __main__ -     eval_loss = 8.249371213829575\n",
            "12/03/2021 20:12:16 - INFO - __main__ -     eval_perplexity = 3825.2198197673824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYVcueb3LxCc",
        "outputId": "d59b893c-02a7-4564-9dd2-266136122249"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"karpathy\" \\\n",
        "  --num_epochs 7 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --embedding_dim 300 \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=7, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=False, username='karpathy')\n",
            "12/03/2021 20:12:33 - INFO - __main__ -   [Iter 2000] Loss 7.13  Val Loss: 7.18  Val Perplexity: 1307.88\n",
            "12/03/2021 20:12:43 - INFO - __main__ -   [Iter 4000] Loss 5.84  Val Loss: 7.27  Val Perplexity: 1429.96\n",
            "12/03/2021 20:12:54 - INFO - __main__ -   [Iter 6000] Loss 5.78  Val Loss: 7.32  Val Perplexity: 1506.93\n",
            "12/03/2021 20:13:04 - INFO - __main__ -   [Iter 8000] Loss 4.68  Val Loss: 7.58  Val Perplexity: 1954.83\n",
            "12/03/2021 20:13:15 - INFO - __main__ -   [Iter 10000] Loss 3.48  Val Loss: 7.91  Val Perplexity: 2714.60\n",
            "12/03/2021 20:13:26 - INFO - __main__ -   [Iter 12000] Loss 3.61  Val Loss: 7.99  Val Perplexity: 2963.74\n",
            "12/03/2021 20:13:36 - INFO - __main__ -   [Iter 14000] Loss 2.63  Val Loss: 8.31  Val Perplexity: 4059.78\n",
            "12/03/2021 20:13:47 - INFO - __main__ -   [Iter 16000] Loss 1.82  Val Loss: 8.66  Val Perplexity: 5744.25\n",
            "12/03/2021 20:13:58 - INFO - __main__ -   [Iter 18000] Loss 1.97  Val Loss: 8.67  Val Perplexity: 5845.69\n",
            "12/03/2021 20:14:08 - INFO - __main__ -   [Iter 20000] Loss 1.36  Val Loss: 9.01  Val Perplexity: 8185.54\n",
            "12/03/2021 20:14:16 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 20:14:16 - INFO - __main__ -     eval_loss = 9.021451734717376\n",
            "12/03/2021 20:14:16 - INFO - __main__ -     eval_perplexity = 8278.786964023957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ2lgKyJMCut",
        "outputId": "5fd8c666-0c7f-43e0-8861-97d5806ac151"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"BarackObama\" \\\n",
        "  --num_epochs 7 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --embedding_dim 300 \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 6000     /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 13000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=7, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=False, username='BarackObama')\n",
            "12/03/2021 20:14:35 - INFO - __main__ -   [Iter 2000] Loss 5.76  Val Loss: 5.54  Val Perplexity: 253.51\n",
            "12/03/2021 20:14:48 - INFO - __main__ -   [Iter 4000] Loss 5.34  Val Loss: 5.28  Val Perplexity: 196.30\n",
            "12/03/2021 20:15:00 - INFO - __main__ -   [Iter 6000] Loss 5.08  Val Loss: 5.11  Val Perplexity: 165.93\n",
            "12/03/2021 20:15:12 - INFO - __main__ -   [Iter 8000] Loss 4.91  Val Loss: 5.03  Val Perplexity: 153.36\n",
            "12/03/2021 20:15:25 - INFO - __main__ -   [Iter 10000] Loss 4.78  Val Loss: 4.95  Val Perplexity: 141.60\n",
            "12/03/2021 20:15:37 - INFO - __main__ -   [Iter 12000] Loss 3.70  Val Loss: 5.01  Val Perplexity: 149.53\n",
            "12/03/2021 20:15:49 - INFO - __main__ -   [Iter 14000] Loss 3.67  Val Loss: 4.96  Val Perplexity: 142.70\n",
            "12/03/2021 20:16:02 - INFO - __main__ -   [Iter 16000] Loss 3.65  Val Loss: 5.00  Val Perplexity: 149.11\n",
            "12/03/2021 20:16:14 - INFO - __main__ -   [Iter 18000] Loss 3.64  Val Loss: 4.94  Val Perplexity: 139.92\n",
            "12/03/2021 20:16:26 - INFO - __main__ -   [Iter 20000] Loss 3.64  Val Loss: 4.94  Val Perplexity: 140.26\n",
            "12/03/2021 20:16:39 - INFO - __main__ -   [Iter 22000] Loss 2.91  Val Loss: 5.01  Val Perplexity: 150.45\n",
            "12/03/2021 20:16:51 - INFO - __main__ -   [Iter 24000] Loss 2.96  Val Loss: 5.08  Val Perplexity: 160.95\n",
            "12/03/2021 20:17:03 - INFO - __main__ -   [Iter 26000] Loss 2.97  Val Loss: 5.07  Val Perplexity: 158.43\n",
            "12/03/2021 20:17:16 - INFO - __main__ -   [Iter 28000] Loss 3.01  Val Loss: 5.07  Val Perplexity: 159.28\n",
            "12/03/2021 20:17:28 - INFO - __main__ -   [Iter 30000] Loss 3.03  Val Loss: 5.08  Val Perplexity: 161.28\n",
            "12/03/2021 20:17:41 - INFO - __main__ -   [Iter 32000] Loss 3.05  Val Loss: 5.04  Val Perplexity: 154.67\n",
            "12/03/2021 20:17:53 - INFO - __main__ -   [Iter 34000] Loss 2.40  Val Loss: 5.24  Val Perplexity: 189.17\n",
            "12/03/2021 20:18:05 - INFO - __main__ -   [Iter 36000] Loss 2.43  Val Loss: 5.21  Val Perplexity: 183.94\n",
            "12/03/2021 20:18:18 - INFO - __main__ -   [Iter 38000] Loss 2.48  Val Loss: 5.19  Val Perplexity: 180.10\n",
            "12/03/2021 20:18:30 - INFO - __main__ -   [Iter 40000] Loss 2.52  Val Loss: 5.24  Val Perplexity: 188.28\n",
            "12/03/2021 20:18:43 - INFO - __main__ -   [Iter 42000] Loss 2.56  Val Loss: 5.20  Val Perplexity: 180.45\n",
            "12/03/2021 20:18:55 - INFO - __main__ -   [Iter 44000] Loss 1.94  Val Loss: 5.34  Val Perplexity: 208.94\n",
            "12/03/2021 20:19:07 - INFO - __main__ -   [Iter 46000] Loss 2.01  Val Loss: 5.42  Val Perplexity: 224.76\n",
            "12/03/2021 20:19:20 - INFO - __main__ -   [Iter 48000] Loss 2.05  Val Loss: 5.39  Val Perplexity: 219.65\n",
            "12/03/2021 20:19:32 - INFO - __main__ -   [Iter 50000] Loss 2.10  Val Loss: 5.39  Val Perplexity: 220.07\n",
            "12/03/2021 20:19:44 - INFO - __main__ -   [Iter 52000] Loss 2.13  Val Loss: 5.35  Val Perplexity: 211.11\n",
            "12/03/2021 20:19:57 - INFO - __main__ -   [Iter 54000] Loss 1.66  Val Loss: 5.42  Val Perplexity: 226.72\n",
            "12/03/2021 20:20:09 - INFO - __main__ -   [Iter 56000] Loss 1.63  Val Loss: 5.49  Val Perplexity: 242.89\n",
            "12/03/2021 20:20:22 - INFO - __main__ -   [Iter 58000] Loss 1.69  Val Loss: 5.54  Val Perplexity: 254.47\n",
            "12/03/2021 20:20:34 - INFO - __main__ -   [Iter 60000] Loss 1.73  Val Loss: 5.58  Val Perplexity: 264.87\n",
            "12/03/2021 20:20:46 - INFO - __main__ -   [Iter 62000] Loss 1.78  Val Loss: 5.56  Val Perplexity: 259.37\n",
            "12/03/2021 20:20:59 - INFO - __main__ -   [Iter 64000] Loss 1.83  Val Loss: 5.56  Val Perplexity: 259.85\n",
            "12/03/2021 20:21:11 - INFO - __main__ -   [Iter 66000] Loss 1.35  Val Loss: 5.67  Val Perplexity: 290.78\n",
            "12/03/2021 20:21:24 - INFO - __main__ -   [Iter 68000] Loss 1.42  Val Loss: 5.73  Val Perplexity: 307.62\n",
            "12/03/2021 20:21:36 - INFO - __main__ -   [Iter 70000] Loss 1.46  Val Loss: 5.72  Val Perplexity: 304.20\n",
            "12/03/2021 20:21:48 - INFO - __main__ -   [Iter 72000] Loss 1.51  Val Loss: 5.73  Val Perplexity: 307.82\n",
            "12/03/2021 20:22:01 - INFO - __main__ -   [Iter 74000] Loss 1.55  Val Loss: 5.75  Val Perplexity: 313.95\n",
            "12/03/2021 20:22:10 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 20:22:10 - INFO - __main__ -     eval_loss = 5.712339516089308\n",
            "12/03/2021 20:22:10 - INFO - __main__ -     eval_perplexity = 302.57812726937453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn3VNkzQMFab",
        "outputId": "6984a49e-fa90-4097-e3ce-5cb949829b91"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"cnn\" \\\n",
        "  --num_epochs 7 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --embedding_dim 300 \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 12000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 22000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 29000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=7, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=False, username='cnn')\n",
            "12/03/2021 20:22:42 - INFO - __main__ -   [Iter 2000] Loss 7.38  Val Loss: 7.68  Val Perplexity: 2161.91\n",
            "12/03/2021 20:23:05 - INFO - __main__ -   [Iter 4000] Loss 7.13  Val Loss: 7.42  Val Perplexity: 1663.38\n",
            "12/03/2021 20:23:28 - INFO - __main__ -   [Iter 6000] Loss 6.99  Val Loss: 7.30  Val Perplexity: 1483.95\n",
            "12/03/2021 20:23:51 - INFO - __main__ -   [Iter 8000] Loss 6.87  Val Loss: 7.18  Val Perplexity: 1313.06\n",
            "12/03/2021 20:24:14 - INFO - __main__ -   [Iter 10000] Loss 6.79  Val Loss: 7.15  Val Perplexity: 1276.94\n",
            "12/03/2021 20:24:37 - INFO - __main__ -   [Iter 12000] Loss 6.72  Val Loss: 7.06  Val Perplexity: 1162.32\n",
            "12/03/2021 20:25:00 - INFO - __main__ -   [Iter 14000] Loss 6.66  Val Loss: 7.02  Val Perplexity: 1116.85\n",
            "12/03/2021 20:25:23 - INFO - __main__ -   [Iter 16000] Loss 6.61  Val Loss: 6.99  Val Perplexity: 1080.75\n",
            "12/03/2021 20:25:46 - INFO - __main__ -   [Iter 18000] Loss 6.57  Val Loss: 6.96  Val Perplexity: 1049.04\n",
            "12/03/2021 20:26:09 - INFO - __main__ -   [Iter 20000] Loss 6.53  Val Loss: 6.93  Val Perplexity: 1018.67\n",
            "12/03/2021 20:26:32 - INFO - __main__ -   [Iter 22000] Loss 6.50  Val Loss: 6.91  Val Perplexity: 1003.95\n",
            "12/03/2021 20:26:55 - INFO - __main__ -   [Iter 24000] Loss 6.47  Val Loss: 6.93  Val Perplexity: 1024.01\n",
            "12/03/2021 20:27:18 - INFO - __main__ -   [Iter 26000] Loss 5.44  Val Loss: 6.95  Val Perplexity: 1046.47\n",
            "12/03/2021 20:27:42 - INFO - __main__ -   [Iter 28000] Loss 5.43  Val Loss: 7.01  Val Perplexity: 1105.53\n",
            "12/03/2021 20:28:05 - INFO - __main__ -   [Iter 30000] Loss 5.47  Val Loss: 7.03  Val Perplexity: 1125.17\n",
            "12/03/2021 20:28:28 - INFO - __main__ -   [Iter 32000] Loss 5.48  Val Loss: 6.99  Val Perplexity: 1082.81\n",
            "12/03/2021 20:28:51 - INFO - __main__ -   [Iter 34000] Loss 5.49  Val Loss: 7.03  Val Perplexity: 1131.69\n",
            "12/03/2021 20:29:14 - INFO - __main__ -   [Iter 36000] Loss 5.51  Val Loss: 7.03  Val Perplexity: 1125.90\n",
            "12/03/2021 20:29:37 - INFO - __main__ -   [Iter 38000] Loss 5.51  Val Loss: 7.01  Val Perplexity: 1110.00\n",
            "12/03/2021 20:30:00 - INFO - __main__ -   [Iter 40000] Loss 5.53  Val Loss: 7.00  Val Perplexity: 1101.81\n",
            "12/03/2021 20:30:23 - INFO - __main__ -   [Iter 42000] Loss 5.55  Val Loss: 7.00  Val Perplexity: 1100.26\n",
            "12/03/2021 20:30:46 - INFO - __main__ -   [Iter 44000] Loss 5.56  Val Loss: 7.03  Val Perplexity: 1127.57\n",
            "12/03/2021 20:31:09 - INFO - __main__ -   [Iter 46000] Loss 5.58  Val Loss: 6.99  Val Perplexity: 1086.81\n",
            "12/03/2021 20:31:33 - INFO - __main__ -   [Iter 48000] Loss 5.59  Val Loss: 7.01  Val Perplexity: 1109.41\n",
            "12/03/2021 20:31:56 - INFO - __main__ -   [Iter 50000] Loss 4.96  Val Loss: 6.99  Val Perplexity: 1080.61\n",
            "12/03/2021 20:32:19 - INFO - __main__ -   [Iter 52000] Loss 4.74  Val Loss: 7.05  Val Perplexity: 1157.09\n",
            "12/03/2021 20:32:42 - INFO - __main__ -   [Iter 54000] Loss 4.77  Val Loss: 7.11  Val Perplexity: 1223.50\n",
            "12/03/2021 20:33:05 - INFO - __main__ -   [Iter 56000] Loss 4.81  Val Loss: 7.12  Val Perplexity: 1231.21\n",
            "12/03/2021 20:33:28 - INFO - __main__ -   [Iter 58000] Loss 4.84  Val Loss: 7.14  Val Perplexity: 1264.12\n",
            "12/03/2021 20:33:51 - INFO - __main__ -   [Iter 60000] Loss 4.87  Val Loss: 7.13  Val Perplexity: 1244.60\n",
            "12/03/2021 20:34:14 - INFO - __main__ -   [Iter 62000] Loss 4.90  Val Loss: 7.15  Val Perplexity: 1271.69\n",
            "12/03/2021 20:34:37 - INFO - __main__ -   [Iter 64000] Loss 4.92  Val Loss: 7.17  Val Perplexity: 1305.75\n",
            "12/03/2021 20:35:00 - INFO - __main__ -   [Iter 66000] Loss 4.95  Val Loss: 7.17  Val Perplexity: 1298.69\n",
            "12/03/2021 20:35:23 - INFO - __main__ -   [Iter 68000] Loss 4.98  Val Loss: 7.16  Val Perplexity: 1282.86\n",
            "12/03/2021 20:35:46 - INFO - __main__ -   [Iter 70000] Loss 5.00  Val Loss: 7.14  Val Perplexity: 1264.82\n",
            "12/03/2021 20:36:10 - INFO - __main__ -   [Iter 72000] Loss 5.03  Val Loss: 7.15  Val Perplexity: 1271.87\n",
            "12/03/2021 20:36:33 - INFO - __main__ -   [Iter 74000] Loss 5.05  Val Loss: 7.14  Val Perplexity: 1260.78\n",
            "12/03/2021 20:36:56 - INFO - __main__ -   [Iter 76000] Loss 4.13  Val Loss: 7.19  Val Perplexity: 1323.05\n",
            "12/03/2021 20:37:19 - INFO - __main__ -   [Iter 78000] Loss 4.16  Val Loss: 7.23  Val Perplexity: 1378.52\n",
            "12/03/2021 20:37:42 - INFO - __main__ -   [Iter 80000] Loss 4.21  Val Loss: 7.26  Val Perplexity: 1427.37\n",
            "12/03/2021 20:38:05 - INFO - __main__ -   [Iter 82000] Loss 4.26  Val Loss: 7.29  Val Perplexity: 1462.48\n",
            "12/03/2021 20:38:28 - INFO - __main__ -   [Iter 84000] Loss 4.32  Val Loss: 7.28  Val Perplexity: 1456.71\n",
            "12/03/2021 20:38:51 - INFO - __main__ -   [Iter 86000] Loss 4.36  Val Loss: 7.29  Val Perplexity: 1461.65\n",
            "12/03/2021 20:39:14 - INFO - __main__ -   [Iter 88000] Loss 4.40  Val Loss: 7.34  Val Perplexity: 1538.03\n",
            "12/03/2021 20:39:37 - INFO - __main__ -   [Iter 90000] Loss 4.44  Val Loss: 7.30  Val Perplexity: 1473.73\n",
            "12/03/2021 20:40:00 - INFO - __main__ -   [Iter 92000] Loss 4.48  Val Loss: 7.32  Val Perplexity: 1504.41\n",
            "12/03/2021 20:40:23 - INFO - __main__ -   [Iter 94000] Loss 4.51  Val Loss: 7.32  Val Perplexity: 1513.12\n",
            "12/03/2021 20:40:46 - INFO - __main__ -   [Iter 96000] Loss 4.54  Val Loss: 7.30  Val Perplexity: 1484.00\n",
            "12/03/2021 20:41:09 - INFO - __main__ -   [Iter 98000] Loss 4.57  Val Loss: 7.30  Val Perplexity: 1474.81\n",
            "12/03/2021 20:41:32 - INFO - __main__ -   [Iter 100000] Loss 3.47  Val Loss: 7.35  Val Perplexity: 1551.98\n",
            "12/03/2021 20:41:55 - INFO - __main__ -   [Iter 102000] Loss 3.73  Val Loss: 7.39  Val Perplexity: 1617.87\n",
            "12/03/2021 20:42:18 - INFO - __main__ -   [Iter 104000] Loss 3.77  Val Loss: 7.41  Val Perplexity: 1651.75\n",
            "12/03/2021 20:42:41 - INFO - __main__ -   [Iter 106000] Loss 3.82  Val Loss: 7.46  Val Perplexity: 1732.00\n",
            "12/03/2021 20:43:04 - INFO - __main__ -   [Iter 108000] Loss 3.88  Val Loss: 7.48  Val Perplexity: 1772.87\n",
            "12/03/2021 20:43:28 - INFO - __main__ -   [Iter 110000] Loss 3.92  Val Loss: 7.48  Val Perplexity: 1771.22\n",
            "12/03/2021 20:43:51 - INFO - __main__ -   [Iter 112000] Loss 3.96  Val Loss: 7.52  Val Perplexity: 1845.21\n",
            "12/03/2021 20:44:14 - INFO - __main__ -   [Iter 114000] Loss 4.01  Val Loss: 7.52  Val Perplexity: 1844.76\n",
            "12/03/2021 20:44:37 - INFO - __main__ -   [Iter 116000] Loss 4.05  Val Loss: 7.49  Val Perplexity: 1791.05\n",
            "12/03/2021 20:45:00 - INFO - __main__ -   [Iter 118000] Loss 4.09  Val Loss: 7.50  Val Perplexity: 1811.92\n",
            "12/03/2021 20:45:23 - INFO - __main__ -   [Iter 120000] Loss 4.12  Val Loss: 7.49  Val Perplexity: 1787.85\n",
            "12/03/2021 20:45:46 - INFO - __main__ -   [Iter 122000] Loss 4.16  Val Loss: 7.51  Val Perplexity: 1829.44\n",
            "12/03/2021 20:46:09 - INFO - __main__ -   [Iter 124000] Loss 4.19  Val Loss: 7.49  Val Perplexity: 1781.39\n",
            "12/03/2021 20:46:32 - INFO - __main__ -   [Iter 126000] Loss 3.36  Val Loss: 7.55  Val Perplexity: 1902.63\n",
            "12/03/2021 20:46:55 - INFO - __main__ -   [Iter 128000] Loss 3.37  Val Loss: 7.62  Val Perplexity: 2028.69\n",
            "12/03/2021 20:47:18 - INFO - __main__ -   [Iter 130000] Loss 3.42  Val Loss: 7.65  Val Perplexity: 2091.72\n",
            "12/03/2021 20:47:41 - INFO - __main__ -   [Iter 132000] Loss 3.47  Val Loss: 7.64  Val Perplexity: 2083.51\n",
            "12/03/2021 20:48:04 - INFO - __main__ -   [Iter 134000] Loss 3.52  Val Loss: 7.65  Val Perplexity: 2093.13\n",
            "12/03/2021 20:48:27 - INFO - __main__ -   [Iter 136000] Loss 3.57  Val Loss: 7.68  Val Perplexity: 2157.25\n",
            "12/03/2021 20:48:50 - INFO - __main__ -   [Iter 138000] Loss 3.62  Val Loss: 7.67  Val Perplexity: 2144.12\n",
            "12/03/2021 20:49:13 - INFO - __main__ -   [Iter 140000] Loss 3.67  Val Loss: 7.68  Val Perplexity: 2156.52\n",
            "12/03/2021 20:49:36 - INFO - __main__ -   [Iter 142000] Loss 3.71  Val Loss: 7.68  Val Perplexity: 2172.47\n",
            "12/03/2021 20:49:59 - INFO - __main__ -   [Iter 144000] Loss 3.76  Val Loss: 7.66  Val Perplexity: 2123.17\n",
            "12/03/2021 20:50:22 - INFO - __main__ -   [Iter 146000] Loss 3.79  Val Loss: 7.68  Val Perplexity: 2169.90\n",
            "12/03/2021 20:50:45 - INFO - __main__ -   [Iter 148000] Loss 3.83  Val Loss: 7.71  Val Perplexity: 2228.98\n",
            "12/03/2021 20:51:08 - INFO - __main__ -   [Iter 150000] Loss 3.02  Val Loss: 7.68  Val Perplexity: 2169.86\n",
            "12/03/2021 20:51:31 - INFO - __main__ -   [Iter 152000] Loss 3.06  Val Loss: 7.76  Val Perplexity: 2350.92\n",
            "12/03/2021 20:51:54 - INFO - __main__ -   [Iter 154000] Loss 3.11  Val Loss: 7.80  Val Perplexity: 2435.69\n",
            "12/03/2021 20:52:17 - INFO - __main__ -   [Iter 156000] Loss 3.16  Val Loss: 7.82  Val Perplexity: 2488.41\n",
            "12/03/2021 20:52:41 - INFO - __main__ -   [Iter 158000] Loss 3.21  Val Loss: 7.82  Val Perplexity: 2502.32\n",
            "12/03/2021 20:53:04 - INFO - __main__ -   [Iter 160000] Loss 3.26  Val Loss: 7.84  Val Perplexity: 2528.13\n",
            "12/03/2021 20:53:27 - INFO - __main__ -   [Iter 162000] Loss 3.31  Val Loss: 7.86  Val Perplexity: 2585.71\n",
            "12/03/2021 20:53:49 - INFO - __main__ -   [Iter 164000] Loss 3.36  Val Loss: 7.85  Val Perplexity: 2567.49\n",
            "12/03/2021 20:54:13 - INFO - __main__ -   [Iter 166000] Loss 3.40  Val Loss: 7.86  Val Perplexity: 2595.46\n",
            "12/03/2021 20:54:36 - INFO - __main__ -   [Iter 168000] Loss 3.44  Val Loss: 7.87  Val Perplexity: 2608.64\n",
            "12/03/2021 20:54:59 - INFO - __main__ -   [Iter 170000] Loss 3.48  Val Loss: 7.85  Val Perplexity: 2565.97\n",
            "12/03/2021 20:55:22 - INFO - __main__ -   [Iter 172000] Loss 3.52  Val Loss: 7.84  Val Perplexity: 2546.69\n",
            "12/03/2021 20:55:45 - INFO - __main__ -   [Iter 174000] Loss 3.56  Val Loss: 7.82  Val Perplexity: 2493.82\n",
            "12/03/2021 20:56:01 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 20:56:01 - INFO - __main__ -     eval_loss = 7.830698525505066\n",
            "12/03/2021 20:56:01 - INFO - __main__ -     eval_perplexity = 2516.686729437401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2HQpMw6MIAZ",
        "outputId": "03a69979-f9d7-4798-f101-dc16946eb844"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"wsj\" \\\n",
        "  --num_epochs 7 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --embedding_dim 300 \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 9000     /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 18000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 26000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=7, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=False, username='wsj')\n",
            "12/03/2021 20:56:33 - INFO - __main__ -   [Iter 2000] Loss 7.53  Val Loss: 7.20  Val Perplexity: 1336.73\n",
            "12/03/2021 20:56:55 - INFO - __main__ -   [Iter 4000] Loss 7.29  Val Loss: 7.03  Val Perplexity: 1124.46\n",
            "12/03/2021 20:57:17 - INFO - __main__ -   [Iter 6000] Loss 7.15  Val Loss: 6.98  Val Perplexity: 1074.12\n",
            "12/03/2021 20:57:39 - INFO - __main__ -   [Iter 8000] Loss 7.05  Val Loss: 6.79  Val Perplexity: 886.95\n",
            "12/03/2021 20:58:02 - INFO - __main__ -   [Iter 10000] Loss 6.97  Val Loss: 6.70  Val Perplexity: 816.15\n",
            "12/03/2021 20:58:24 - INFO - __main__ -   [Iter 12000] Loss 6.90  Val Loss: 6.69  Val Perplexity: 805.57\n",
            "12/03/2021 20:58:46 - INFO - __main__ -   [Iter 14000] Loss 6.85  Val Loss: 6.63  Val Perplexity: 758.70\n",
            "12/03/2021 20:59:08 - INFO - __main__ -   [Iter 16000] Loss 6.80  Val Loss: 6.58  Val Perplexity: 723.02\n",
            "12/03/2021 20:59:30 - INFO - __main__ -   [Iter 18000] Loss 6.75  Val Loss: 6.56  Val Perplexity: 709.58\n",
            "12/03/2021 20:59:53 - INFO - __main__ -   [Iter 20000] Loss 6.71  Val Loss: 6.53  Val Perplexity: 684.14\n",
            "12/03/2021 21:00:15 - INFO - __main__ -   [Iter 22000] Loss 6.68  Val Loss: 6.51  Val Perplexity: 669.41\n",
            "12/03/2021 21:00:37 - INFO - __main__ -   [Iter 24000] Loss 5.65  Val Loss: 6.52  Val Perplexity: 679.29\n",
            "12/03/2021 21:00:59 - INFO - __main__ -   [Iter 26000] Loss 5.64  Val Loss: 6.55  Val Perplexity: 701.47\n",
            "12/03/2021 21:01:21 - INFO - __main__ -   [Iter 28000] Loss 5.64  Val Loss: 6.57  Val Perplexity: 716.48\n",
            "12/03/2021 21:01:44 - INFO - __main__ -   [Iter 30000] Loss 5.64  Val Loss: 6.58  Val Perplexity: 718.59\n",
            "12/03/2021 21:02:06 - INFO - __main__ -   [Iter 32000] Loss 5.67  Val Loss: 6.59  Val Perplexity: 726.92\n",
            "12/03/2021 21:02:28 - INFO - __main__ -   [Iter 34000] Loss 5.68  Val Loss: 6.57  Val Perplexity: 713.91\n",
            "12/03/2021 21:02:50 - INFO - __main__ -   [Iter 36000] Loss 5.69  Val Loss: 6.55  Val Perplexity: 701.30\n",
            "12/03/2021 21:03:12 - INFO - __main__ -   [Iter 38000] Loss 5.70  Val Loss: 6.56  Val Perplexity: 703.93\n",
            "12/03/2021 21:03:34 - INFO - __main__ -   [Iter 40000] Loss 5.71  Val Loss: 6.54  Val Perplexity: 692.25\n",
            "12/03/2021 21:03:57 - INFO - __main__ -   [Iter 42000] Loss 5.72  Val Loss: 6.56  Val Perplexity: 707.39\n",
            "12/03/2021 21:04:19 - INFO - __main__ -   [Iter 44000] Loss 5.73  Val Loss: 6.54  Val Perplexity: 692.49\n",
            "12/03/2021 21:04:41 - INFO - __main__ -   [Iter 46000] Loss 5.73  Val Loss: 6.55  Val Perplexity: 699.99\n",
            "12/03/2021 21:05:03 - INFO - __main__ -   [Iter 48000] Loss 4.89  Val Loss: 6.59  Val Perplexity: 728.30\n",
            "12/03/2021 21:05:26 - INFO - __main__ -   [Iter 50000] Loss 4.87  Val Loss: 6.64  Val Perplexity: 763.48\n",
            "12/03/2021 21:05:48 - INFO - __main__ -   [Iter 52000] Loss 4.91  Val Loss: 6.65  Val Perplexity: 769.10\n",
            "12/03/2021 21:06:10 - INFO - __main__ -   [Iter 54000] Loss 4.94  Val Loss: 6.65  Val Perplexity: 771.66\n",
            "12/03/2021 21:06:32 - INFO - __main__ -   [Iter 56000] Loss 4.98  Val Loss: 6.66  Val Perplexity: 782.38\n",
            "12/03/2021 21:06:55 - INFO - __main__ -   [Iter 58000] Loss 5.01  Val Loss: 6.66  Val Perplexity: 780.18\n",
            "12/03/2021 21:07:17 - INFO - __main__ -   [Iter 60000] Loss 5.04  Val Loss: 6.67  Val Perplexity: 789.83\n",
            "12/03/2021 21:07:39 - INFO - __main__ -   [Iter 62000] Loss 5.08  Val Loss: 6.68  Val Perplexity: 799.88\n",
            "12/03/2021 21:08:01 - INFO - __main__ -   [Iter 64000] Loss 5.10  Val Loss: 6.66  Val Perplexity: 782.17\n",
            "12/03/2021 21:08:23 - INFO - __main__ -   [Iter 66000] Loss 5.12  Val Loss: 6.66  Val Perplexity: 783.66\n",
            "12/03/2021 21:08:46 - INFO - __main__ -   [Iter 68000] Loss 5.15  Val Loss: 6.67  Val Perplexity: 791.50\n",
            "12/03/2021 21:09:08 - INFO - __main__ -   [Iter 70000] Loss 5.17  Val Loss: 6.66  Val Perplexity: 782.80\n",
            "12/03/2021 21:09:30 - INFO - __main__ -   [Iter 72000] Loss 4.29  Val Loss: 6.75  Val Perplexity: 849.84\n",
            "12/03/2021 21:09:52 - INFO - __main__ -   [Iter 74000] Loss 4.31  Val Loss: 6.79  Val Perplexity: 886.91\n",
            "12/03/2021 21:10:14 - INFO - __main__ -   [Iter 76000] Loss 4.35  Val Loss: 6.79  Val Perplexity: 887.89\n",
            "12/03/2021 21:10:37 - INFO - __main__ -   [Iter 78000] Loss 4.40  Val Loss: 6.82  Val Perplexity: 919.74\n",
            "12/03/2021 21:10:59 - INFO - __main__ -   [Iter 80000] Loss 4.44  Val Loss: 6.82  Val Perplexity: 919.79\n",
            "12/03/2021 21:11:21 - INFO - __main__ -   [Iter 82000] Loss 4.48  Val Loss: 6.85  Val Perplexity: 944.55\n",
            "12/03/2021 21:11:43 - INFO - __main__ -   [Iter 84000] Loss 4.52  Val Loss: 6.84  Val Perplexity: 931.49\n",
            "12/03/2021 21:12:05 - INFO - __main__ -   [Iter 86000] Loss 4.56  Val Loss: 6.83  Val Perplexity: 929.58\n",
            "12/03/2021 21:12:28 - INFO - __main__ -   [Iter 88000] Loss 4.59  Val Loss: 6.83  Val Perplexity: 922.67\n",
            "12/03/2021 21:12:50 - INFO - __main__ -   [Iter 90000] Loss 4.62  Val Loss: 6.82  Val Perplexity: 919.70\n",
            "12/03/2021 21:13:12 - INFO - __main__ -   [Iter 92000] Loss 4.65  Val Loss: 6.83  Val Perplexity: 925.63\n",
            "12/03/2021 21:13:34 - INFO - __main__ -   [Iter 94000] Loss 4.68  Val Loss: 6.83  Val Perplexity: 924.33\n",
            "12/03/2021 21:13:56 - INFO - __main__ -   [Iter 96000] Loss 3.79  Val Loss: 6.91  Val Perplexity: 1000.39\n",
            "12/03/2021 21:14:19 - INFO - __main__ -   [Iter 98000] Loss 3.84  Val Loss: 6.93  Val Perplexity: 1019.44\n",
            "12/03/2021 21:14:41 - INFO - __main__ -   [Iter 100000] Loss 3.90  Val Loss: 6.96  Val Perplexity: 1050.88\n",
            "12/03/2021 21:15:03 - INFO - __main__ -   [Iter 102000] Loss 3.96  Val Loss: 6.98  Val Perplexity: 1073.18\n",
            "12/03/2021 21:15:25 - INFO - __main__ -   [Iter 104000] Loss 4.01  Val Loss: 6.97  Val Perplexity: 1069.49\n",
            "12/03/2021 21:15:47 - INFO - __main__ -   [Iter 106000] Loss 4.06  Val Loss: 6.99  Val Perplexity: 1081.61\n",
            "12/03/2021 21:16:10 - INFO - __main__ -   [Iter 108000] Loss 4.09  Val Loss: 7.01  Val Perplexity: 1102.40\n",
            "12/03/2021 21:16:32 - INFO - __main__ -   [Iter 110000] Loss 4.13  Val Loss: 6.98  Val Perplexity: 1077.73\n",
            "12/03/2021 21:16:54 - INFO - __main__ -   [Iter 112000] Loss 4.17  Val Loss: 6.99  Val Perplexity: 1085.57\n",
            "12/03/2021 21:17:16 - INFO - __main__ -   [Iter 114000] Loss 4.21  Val Loss: 6.97  Val Perplexity: 1065.53\n",
            "12/03/2021 21:17:38 - INFO - __main__ -   [Iter 116000] Loss 4.25  Val Loss: 7.00  Val Perplexity: 1095.48\n",
            "12/03/2021 21:18:01 - INFO - __main__ -   [Iter 118000] Loss 3.16  Val Loss: 6.99  Val Perplexity: 1087.64\n",
            "12/03/2021 21:18:23 - INFO - __main__ -   [Iter 120000] Loss 3.42  Val Loss: 7.06  Val Perplexity: 1168.32\n",
            "12/03/2021 21:18:45 - INFO - __main__ -   [Iter 122000] Loss 3.46  Val Loss: 7.10  Val Perplexity: 1210.11\n",
            "12/03/2021 21:19:07 - INFO - __main__ -   [Iter 124000] Loss 3.52  Val Loss: 7.15  Val Perplexity: 1268.02\n",
            "12/03/2021 21:19:29 - INFO - __main__ -   [Iter 126000] Loss 3.59  Val Loss: 7.15  Val Perplexity: 1274.18\n",
            "12/03/2021 21:19:52 - INFO - __main__ -   [Iter 128000] Loss 3.64  Val Loss: 7.14  Val Perplexity: 1264.83\n",
            "12/03/2021 21:20:14 - INFO - __main__ -   [Iter 130000] Loss 3.68  Val Loss: 7.15  Val Perplexity: 1275.88\n",
            "12/03/2021 21:20:36 - INFO - __main__ -   [Iter 132000] Loss 3.73  Val Loss: 7.16  Val Perplexity: 1291.25\n",
            "12/03/2021 21:20:58 - INFO - __main__ -   [Iter 134000] Loss 3.78  Val Loss: 7.16  Val Perplexity: 1288.95\n",
            "12/03/2021 21:21:21 - INFO - __main__ -   [Iter 136000] Loss 3.82  Val Loss: 7.17  Val Perplexity: 1293.76\n",
            "12/03/2021 21:21:43 - INFO - __main__ -   [Iter 138000] Loss 3.86  Val Loss: 7.16  Val Perplexity: 1288.11\n",
            "12/03/2021 21:22:05 - INFO - __main__ -   [Iter 140000] Loss 3.90  Val Loss: 7.15  Val Perplexity: 1267.95\n",
            "12/03/2021 21:22:28 - INFO - __main__ -   [Iter 142000] Loss 3.05  Val Loss: 7.18  Val Perplexity: 1314.92\n",
            "12/03/2021 21:22:50 - INFO - __main__ -   [Iter 144000] Loss 3.09  Val Loss: 7.24  Val Perplexity: 1397.93\n",
            "12/03/2021 21:23:12 - INFO - __main__ -   [Iter 146000] Loss 3.15  Val Loss: 7.28  Val Perplexity: 1450.54\n",
            "12/03/2021 21:23:34 - INFO - __main__ -   [Iter 148000] Loss 3.20  Val Loss: 7.31  Val Perplexity: 1496.38\n",
            "12/03/2021 21:23:57 - INFO - __main__ -   [Iter 150000] Loss 3.26  Val Loss: 7.34  Val Perplexity: 1547.04\n",
            "12/03/2021 21:24:19 - INFO - __main__ -   [Iter 152000] Loss 3.32  Val Loss: 7.33  Val Perplexity: 1531.51\n",
            "12/03/2021 21:24:41 - INFO - __main__ -   [Iter 154000] Loss 3.38  Val Loss: 7.32  Val Perplexity: 1515.29\n",
            "12/03/2021 21:25:03 - INFO - __main__ -   [Iter 156000] Loss 3.42  Val Loss: 7.32  Val Perplexity: 1504.86\n",
            "12/03/2021 21:25:26 - INFO - __main__ -   [Iter 158000] Loss 3.47  Val Loss: 7.33  Val Perplexity: 1532.87\n",
            "12/03/2021 21:25:48 - INFO - __main__ -   [Iter 160000] Loss 3.51  Val Loss: 7.32  Val Perplexity: 1516.26\n",
            "12/03/2021 21:26:10 - INFO - __main__ -   [Iter 162000] Loss 3.55  Val Loss: 7.31  Val Perplexity: 1488.02\n",
            "12/03/2021 21:26:32 - INFO - __main__ -   [Iter 164000] Loss 3.60  Val Loss: 7.32  Val Perplexity: 1508.66\n",
            "12/03/2021 21:26:49 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 21:26:49 - INFO - __main__ -     eval_loss = 7.307927451962132\n",
            "12/03/2021 21:26:49 - INFO - __main__ -     eval_perplexity = 1492.081571657841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9fl6_I19kpQ"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l06lhdCq9EEP",
        "outputId": "a9614f97-26ac-4fc8-b94b-3fd6a0934870"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"elonmusk\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 0: Tesla software V10. 0 comes with karaoke\n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 1: \n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 2: Tesla Supercharger network now energized from Cape Canaveral to California, but the second derivative bodes on the droneship. Just need the future.\n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 3: Just leaving SpaceX Starship build site in Boca\n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 4: Will be live streaming the Gigafactory launch party tonight. Check this eve.\n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 5: Falcon Heavy goes vertical\n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 6: Falcon Heavy goes vertical\n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 7: At Tesla Giga Shanghai NSFW!!\n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 8: Tesla deliveries & AP hardware chart by @ lexfridman at MIT\n",
            "12/03/2021 21:27:05 - INFO - __main__ -   Sample 9: Just leaving SpaceX Starship build site in Boca\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHT935-I9Vmo",
        "outputId": "ab9f6d9c-b0e3-449e-97f9-2a8b869ad4e3"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"karpathy\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 0: Awesome : \" The Drill of Depth \" ( Portable Kinect 3D Camera ) from David Cox lab ( @ neurobongo )\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 1: At last, Stanford presentation. Fun, will look at all. I like the same direction. \" 1\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 2: This is a motherfucking website. the guy has a point\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 3: I hate a new new trend about designs with ConvNetJS from a nice MLdata science blog : & video of your pet with Robots\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 4: One of my favorite funny YouTube clips : \" Everythings Amazing & Nobodys Happy \" ( fun starts ~) 1\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 5: Wow pretending to be blind is Really hard! 1\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 6: I know that take 1\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 7: # icra2013 robots batch two vine. co Andrej Karpathy' s post on Vine # icra2013 robots batch two\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 8: New Facebook feed looks good idea but I can' t help but it' s not right.\n",
            "12/03/2021 21:27:10 - INFO - __main__ -   Sample 9: I' m witnessing the worst ( and it doesn' t work ( and ) is a good stuff. And it' s a thing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F04BO-4i9XZ6",
        "outputId": "ba7947e2-836a-48b6-c264-afd2c7c57d13"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"BarackObama\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 0: President Obama : “ I ’ ll use the money we ’ re fighting to create the future that ’ s not going.”\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 1: President Obama : \" We have to renew our military around the world in our commitment to fight back against all of us.”\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 2: “ We believe we have a strong, middle - class opportunity for all that hard work and responsibility is rewarded.”— President Obama\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 3: FACT : Obama' s plan to reduce the deficit by $ 4 trillion to $ 4 trillion in 10 years. # BuffettRule\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 4: Have you heard?\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 5: Read the latest op - ed from @ nytimes : \" We can bring the job with a balanced plan that would make it harder for our people and our future.\"\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 6: President Obama : “ I am not going to cut carbon pollution power to the country. I still believe that change is not.”\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 7: \" A hard day ’ s work deserves a fair day — that ’ s what this is what we should do — not change — that ’ s about jobs — that ’ s why we will be close.\" # DemandAction\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 8: “ I am the voting with a balanced approach that women for $ 14, 500 in the states that ’ s in the United States.”\n",
            "12/03/2021 21:27:15 - INFO - __main__ -   Sample 9: \" I' m only here because the CleanWater we passed to talk about putting a balanced approach to deficit reduction.\" — President Obama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEM8LQ_p9ZSX",
        "outputId": "d8e5686b-f062-4400-bec9-19b86a28121e"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"cnn\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 0: Opinion : An e - commerce is likely to be a budget, writes @ jasonrileywsj\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 1: After years of a man, the attack, was the deaths of violence in the past four days of a attack on June 2012.\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 2: How to put the rule of violence\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 3: The U. S. has backed since the U. S. to five years ago. Will the new set of new rules?\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 4: :' The talk of' has been for them.\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 5: GM to Settle SEC America\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 6: As the world, we' re make a comeback :\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 7: For your neighborhood, being a slowing like a lot by \" no \" too much.\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 8: Opinion : How to stop the ones of the NFL in the House tax bill\n",
            "12/03/2021 21:27:20 - INFO - __main__ -   Sample 9: A new leader FDA ’ s phone on the internet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9Par9ur9aez",
        "outputId": "ef036ee3-f4f7-4466-f782-1337b42589f4"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"wsj\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 0: Man kills wife, self in Mount death 2\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 1: How did a # PoliticalTicker in a Twitter account of the @, and the the best and the sale of a # Rio2016 GIF\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 2: Man admits killing son, mother' s' found dead after 3\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 3: A school teacher, the man who helped her home with two weeks after decades accident.\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 4: \" A - Rod is a missing of a couple that will be on a large via @ SarahHoyeCNN\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 5: New York police have some ice resume for fall for # CNNdebate?\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 6: The top 10 tech ‘ fails ’ of 2011.\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 7: What' s on the Apple' s museum could be the next few hours before home before they may have been kidnapped on # CNNElections :\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 8: , Alabama wins Search for emergency, study says.\n",
            "12/03/2021 21:27:25 - INFO - __main__ -   Sample 9: The man who made Officials is giving an update on the set of a gun operation at the White House. Watch live on @ CNN TV or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HufNjkgQznG6"
      },
      "source": [
        "# Train With GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccgsAXUZz9Dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ad126a-0522-4b04-f82a-771ab327c8a5"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"elonmusk\" \\\n",
        "  --num_epochs 5 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --use_pretrained_embeddings True \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=5, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=True, username='elonmusk')\n",
            "12/03/2021 21:41:59 - INFO - __main__ -   Using pretrained twitter GloVe embeddings...\n",
            "12/03/2021 21:41:59 - INFO - torchtext.vocab.vectors -   Loading vectors from .vector_cache/glove.twitter.27B.200d.txt.pt\n",
            "12/03/2021 21:42:12 - INFO - __main__ -   [Iter 2000] Loss 7.17  Val Loss: 6.98  Val Perplexity: 1078.86\n",
            "12/03/2021 21:42:19 - INFO - __main__ -   [Iter 4000] Loss 6.29  Val Loss: 7.04  Val Perplexity: 1137.26\n",
            "12/03/2021 21:42:27 - INFO - __main__ -   [Iter 6000] Loss 5.66  Val Loss: 7.18  Val Perplexity: 1315.73\n",
            "12/03/2021 21:42:34 - INFO - __main__ -   [Iter 8000] Loss 4.87  Val Loss: 7.32  Val Perplexity: 1506.78\n",
            "12/03/2021 21:42:41 - INFO - __main__ -   [Iter 10000] Loss 4.92  Val Loss: 7.45  Val Perplexity: 1725.22\n",
            "12/03/2021 21:42:49 - INFO - __main__ -   [Iter 12000] Loss 4.14  Val Loss: 7.69  Val Perplexity: 2196.13\n",
            "12/03/2021 21:42:56 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 21:42:56 - INFO - __main__ -     eval_loss = 7.660266963021744\n",
            "12/03/2021 21:42:56 - INFO - __main__ -     eval_perplexity = 2122.323934967776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLfNYPcjz-nE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a156b76f-5bb2-4155-d63d-052ada1ed9ec"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"karpathy\" \\\n",
        "  --num_epochs 5 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --use_pretrained_embeddings True \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=5, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=True, username='karpathy')\n",
            "12/03/2021 21:43:02 - INFO - __main__ -   Using pretrained twitter GloVe embeddings...\n",
            "12/03/2021 21:43:02 - INFO - torchtext.vocab.vectors -   Loading vectors from .vector_cache/glove.twitter.27B.200d.txt.pt\n",
            "12/03/2021 21:43:18 - INFO - __main__ -   [Iter 2000] Loss 7.26  Val Loss: 7.38  Val Perplexity: 1597.97\n",
            "12/03/2021 21:43:27 - INFO - __main__ -   [Iter 4000] Loss 6.38  Val Loss: 7.40  Val Perplexity: 1638.10\n",
            "12/03/2021 21:43:36 - INFO - __main__ -   [Iter 6000] Loss 6.31  Val Loss: 7.39  Val Perplexity: 1621.89\n",
            "12/03/2021 21:43:45 - INFO - __main__ -   [Iter 8000] Loss 5.68  Val Loss: 7.61  Val Perplexity: 2012.60\n",
            "12/03/2021 21:43:54 - INFO - __main__ -   [Iter 10000] Loss 4.89  Val Loss: 7.97  Val Perplexity: 2879.57\n",
            "12/03/2021 21:44:03 - INFO - __main__ -   [Iter 12000] Loss 4.96  Val Loss: 7.99  Val Perplexity: 2960.95\n",
            "12/03/2021 21:44:13 - INFO - __main__ -   [Iter 14000] Loss 4.17  Val Loss: 8.29  Val Perplexity: 3987.54\n",
            "12/03/2021 21:44:24 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 21:44:24 - INFO - __main__ -     eval_loss = 8.304724268196455\n",
            "12/03/2021 21:44:24 - INFO - __main__ -     eval_perplexity = 4042.9272208157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD2glFkpz_sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564ec0c3-b7d2-40e4-be6d-db7387b6dda7"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"BarackObama\" \\\n",
        "  --num_epochs 5 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --use_pretrained_embeddings True \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 7000     /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=5, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=True, username='BarackObama')\n",
            "12/03/2021 21:44:31 - INFO - __main__ -   Using pretrained twitter GloVe embeddings...\n",
            "12/03/2021 21:44:31 - INFO - torchtext.vocab.vectors -   Loading vectors from .vector_cache/glove.twitter.27B.200d.txt.pt\n",
            "12/03/2021 21:44:48 - INFO - __main__ -   [Iter 2000] Loss 6.08  Val Loss: 5.91  Val Perplexity: 369.18\n",
            "12/03/2021 21:45:00 - INFO - __main__ -   [Iter 4000] Loss 5.73  Val Loss: 5.62  Val Perplexity: 276.42\n",
            "12/03/2021 21:45:11 - INFO - __main__ -   [Iter 6000] Loss 5.49  Val Loss: 5.52  Val Perplexity: 250.24\n",
            "12/03/2021 21:45:21 - INFO - __main__ -   [Iter 8000] Loss 5.32  Val Loss: 5.40  Val Perplexity: 220.92\n",
            "12/03/2021 21:45:32 - INFO - __main__ -   [Iter 10000] Loss 5.19  Val Loss: 5.25  Val Perplexity: 190.24\n",
            "12/03/2021 21:45:43 - INFO - __main__ -   [Iter 12000] Loss 4.20  Val Loss: 5.27  Val Perplexity: 193.90\n",
            "12/03/2021 21:45:54 - INFO - __main__ -   [Iter 14000] Loss 4.18  Val Loss: 5.26  Val Perplexity: 193.25\n",
            "12/03/2021 21:46:05 - INFO - __main__ -   [Iter 16000] Loss 4.18  Val Loss: 5.25  Val Perplexity: 190.89\n",
            "12/03/2021 21:46:16 - INFO - __main__ -   [Iter 18000] Loss 4.17  Val Loss: 5.23  Val Perplexity: 186.67\n",
            "12/03/2021 21:46:26 - INFO - __main__ -   [Iter 20000] Loss 4.16  Val Loss: 5.19  Val Perplexity: 180.19\n",
            "12/03/2021 21:46:37 - INFO - __main__ -   [Iter 22000] Loss 3.60  Val Loss: 5.24  Val Perplexity: 188.24\n",
            "12/03/2021 21:46:48 - INFO - __main__ -   [Iter 24000] Loss 3.56  Val Loss: 5.28  Val Perplexity: 196.44\n",
            "12/03/2021 21:46:59 - INFO - __main__ -   [Iter 26000] Loss 3.56  Val Loss: 5.26  Val Perplexity: 192.99\n",
            "12/03/2021 21:47:10 - INFO - __main__ -   [Iter 28000] Loss 3.60  Val Loss: 5.25  Val Perplexity: 190.97\n",
            "12/03/2021 21:47:21 - INFO - __main__ -   [Iter 30000] Loss 3.62  Val Loss: 5.27  Val Perplexity: 193.47\n",
            "12/03/2021 21:47:32 - INFO - __main__ -   [Iter 32000] Loss 3.62  Val Loss: 5.23  Val Perplexity: 186.12\n",
            "12/03/2021 21:47:43 - INFO - __main__ -   [Iter 34000] Loss 3.02  Val Loss: 5.31  Val Perplexity: 202.17\n",
            "12/03/2021 21:47:53 - INFO - __main__ -   [Iter 36000] Loss 3.07  Val Loss: 5.39  Val Perplexity: 218.53\n",
            "12/03/2021 21:48:04 - INFO - __main__ -   [Iter 38000] Loss 3.11  Val Loss: 5.36  Val Perplexity: 212.62\n",
            "12/03/2021 21:48:15 - INFO - __main__ -   [Iter 40000] Loss 3.14  Val Loss: 5.33  Val Perplexity: 207.11\n",
            "12/03/2021 21:48:26 - INFO - __main__ -   [Iter 42000] Loss 3.18  Val Loss: 5.35  Val Perplexity: 209.83\n",
            "12/03/2021 21:48:37 - INFO - __main__ -   [Iter 44000] Loss 2.62  Val Loss: 5.46  Val Perplexity: 235.05\n",
            "12/03/2021 21:48:48 - INFO - __main__ -   [Iter 46000] Loss 2.67  Val Loss: 5.50  Val Perplexity: 245.23\n",
            "12/03/2021 21:48:59 - INFO - __main__ -   [Iter 48000] Loss 2.71  Val Loss: 5.49  Val Perplexity: 242.81\n",
            "12/03/2021 21:49:10 - INFO - __main__ -   [Iter 50000] Loss 2.74  Val Loss: 5.52  Val Perplexity: 249.77\n",
            "12/03/2021 21:49:20 - INFO - __main__ -   [Iter 52000] Loss 2.78  Val Loss: 5.54  Val Perplexity: 254.97\n",
            "12/03/2021 21:49:35 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 21:49:35 - INFO - __main__ -     eval_loss = 5.479757159394403\n",
            "12/03/2021 21:49:35 - INFO - __main__ -     eval_perplexity = 239.78846992608078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me54MpNOz3tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6cd3412-a940-4124-9da7-7ed66c948fe8"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"cnn\" \\\n",
        "  --num_epochs 5 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --use_pretrained_embeddings True \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 9000     /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 19000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 27000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 31000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=5, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=True, username='cnn')\n",
            "12/03/2021 21:49:45 - INFO - __main__ -   Using pretrained twitter GloVe embeddings...\n",
            "12/03/2021 21:49:45 - INFO - torchtext.vocab.vectors -   Loading vectors from .vector_cache/glove.twitter.27B.200d.txt.pt\n",
            "12/03/2021 21:50:11 - INFO - __main__ -   [Iter 2000] Loss 7.46  Val Loss: 7.81  Val Perplexity: 2470.35\n",
            "12/03/2021 21:50:31 - INFO - __main__ -   [Iter 4000] Loss 7.26  Val Loss: 7.60  Val Perplexity: 1992.32\n",
            "12/03/2021 21:50:50 - INFO - __main__ -   [Iter 6000] Loss 7.13  Val Loss: 7.53  Val Perplexity: 1856.67\n",
            "12/03/2021 21:51:10 - INFO - __main__ -   [Iter 8000] Loss 7.05  Val Loss: 7.47  Val Perplexity: 1753.35\n",
            "12/03/2021 21:51:29 - INFO - __main__ -   [Iter 10000] Loss 6.98  Val Loss: 7.36  Val Perplexity: 1573.86\n",
            "12/03/2021 21:51:49 - INFO - __main__ -   [Iter 12000] Loss 6.92  Val Loss: 7.34  Val Perplexity: 1541.73\n",
            "12/03/2021 21:52:09 - INFO - __main__ -   [Iter 14000] Loss 6.87  Val Loss: 7.32  Val Perplexity: 1502.82\n",
            "12/03/2021 21:52:28 - INFO - __main__ -   [Iter 16000] Loss 6.82  Val Loss: 7.25  Val Perplexity: 1402.62\n",
            "12/03/2021 21:52:48 - INFO - __main__ -   [Iter 18000] Loss 6.79  Val Loss: 7.20  Val Perplexity: 1335.23\n",
            "12/03/2021 21:53:08 - INFO - __main__ -   [Iter 20000] Loss 6.75  Val Loss: 7.25  Val Perplexity: 1405.22\n",
            "12/03/2021 21:53:27 - INFO - __main__ -   [Iter 22000] Loss 6.72  Val Loss: 7.22  Val Perplexity: 1361.41\n",
            "12/03/2021 21:53:47 - INFO - __main__ -   [Iter 24000] Loss 6.69  Val Loss: 7.18  Val Perplexity: 1313.41\n",
            "12/03/2021 21:54:07 - INFO - __main__ -   [Iter 26000] Loss 5.94  Val Loss: 7.21  Val Perplexity: 1352.88\n",
            "12/03/2021 21:54:26 - INFO - __main__ -   [Iter 28000] Loss 5.92  Val Loss: 7.21  Val Perplexity: 1354.57\n",
            "12/03/2021 21:54:46 - INFO - __main__ -   [Iter 30000] Loss 5.94  Val Loss: 7.19  Val Perplexity: 1322.75\n",
            "12/03/2021 21:55:06 - INFO - __main__ -   [Iter 32000] Loss 5.94  Val Loss: 7.18  Val Perplexity: 1313.22\n",
            "12/03/2021 21:55:25 - INFO - __main__ -   [Iter 34000] Loss 5.95  Val Loss: 7.21  Val Perplexity: 1357.87\n",
            "12/03/2021 21:55:45 - INFO - __main__ -   [Iter 36000] Loss 5.95  Val Loss: 7.23  Val Perplexity: 1383.45\n",
            "12/03/2021 21:56:05 - INFO - __main__ -   [Iter 38000] Loss 5.96  Val Loss: 7.23  Val Perplexity: 1380.71\n",
            "12/03/2021 21:56:24 - INFO - __main__ -   [Iter 40000] Loss 5.97  Val Loss: 7.25  Val Perplexity: 1409.48\n",
            "12/03/2021 21:56:44 - INFO - __main__ -   [Iter 42000] Loss 5.98  Val Loss: 7.25  Val Perplexity: 1407.51\n",
            "12/03/2021 21:57:03 - INFO - __main__ -   [Iter 44000] Loss 5.99  Val Loss: 7.22  Val Perplexity: 1368.37\n",
            "12/03/2021 21:57:23 - INFO - __main__ -   [Iter 46000] Loss 5.99  Val Loss: 7.18  Val Perplexity: 1313.92\n",
            "12/03/2021 21:57:43 - INFO - __main__ -   [Iter 48000] Loss 6.00  Val Loss: 7.18  Val Perplexity: 1312.20\n",
            "12/03/2021 21:58:03 - INFO - __main__ -   [Iter 50000] Loss 5.81  Val Loss: 7.18  Val Perplexity: 1315.35\n",
            "12/03/2021 21:58:22 - INFO - __main__ -   [Iter 52000] Loss 5.41  Val Loss: 7.21  Val Perplexity: 1355.17\n",
            "12/03/2021 21:58:42 - INFO - __main__ -   [Iter 54000] Loss 5.41  Val Loss: 7.23  Val Perplexity: 1385.69\n",
            "12/03/2021 21:59:02 - INFO - __main__ -   [Iter 56000] Loss 5.43  Val Loss: 7.28  Val Perplexity: 1448.21\n",
            "12/03/2021 21:59:21 - INFO - __main__ -   [Iter 58000] Loss 5.45  Val Loss: 7.27  Val Perplexity: 1441.85\n",
            "12/03/2021 21:59:41 - INFO - __main__ -   [Iter 60000] Loss 5.47  Val Loss: 7.31  Val Perplexity: 1502.59\n",
            "12/03/2021 22:00:01 - INFO - __main__ -   [Iter 62000] Loss 5.48  Val Loss: 7.30  Val Perplexity: 1480.09\n",
            "12/03/2021 22:00:20 - INFO - __main__ -   [Iter 64000] Loss 5.51  Val Loss: 7.31  Val Perplexity: 1496.86\n",
            "12/03/2021 22:00:40 - INFO - __main__ -   [Iter 66000] Loss 5.53  Val Loss: 7.31  Val Perplexity: 1497.64\n",
            "12/03/2021 22:01:00 - INFO - __main__ -   [Iter 68000] Loss 5.55  Val Loss: 7.29  Val Perplexity: 1467.90\n",
            "12/03/2021 22:01:19 - INFO - __main__ -   [Iter 70000] Loss 5.57  Val Loss: 7.30  Val Perplexity: 1484.71\n",
            "12/03/2021 22:01:39 - INFO - __main__ -   [Iter 72000] Loss 5.59  Val Loss: 7.33  Val Perplexity: 1517.99\n",
            "12/03/2021 22:01:58 - INFO - __main__ -   [Iter 74000] Loss 5.60  Val Loss: 7.31  Val Perplexity: 1491.24\n",
            "12/03/2021 22:02:18 - INFO - __main__ -   [Iter 76000] Loss 4.86  Val Loss: 7.39  Val Perplexity: 1613.74\n",
            "12/03/2021 22:02:38 - INFO - __main__ -   [Iter 78000] Loss 4.90  Val Loss: 7.42  Val Perplexity: 1675.51\n",
            "12/03/2021 22:02:57 - INFO - __main__ -   [Iter 80000] Loss 4.95  Val Loss: 7.43  Val Perplexity: 1692.39\n",
            "12/03/2021 22:03:17 - INFO - __main__ -   [Iter 82000] Loss 4.99  Val Loss: 7.43  Val Perplexity: 1683.60\n",
            "12/03/2021 22:03:37 - INFO - __main__ -   [Iter 84000] Loss 5.03  Val Loss: 7.46  Val Perplexity: 1732.63\n",
            "12/03/2021 22:03:56 - INFO - __main__ -   [Iter 86000] Loss 5.06  Val Loss: 7.46  Val Perplexity: 1730.22\n",
            "12/03/2021 22:04:16 - INFO - __main__ -   [Iter 88000] Loss 5.08  Val Loss: 7.48  Val Perplexity: 1768.57\n",
            "12/03/2021 22:04:36 - INFO - __main__ -   [Iter 90000] Loss 5.11  Val Loss: 7.48  Val Perplexity: 1772.24\n",
            "12/03/2021 22:04:55 - INFO - __main__ -   [Iter 92000] Loss 5.14  Val Loss: 7.44  Val Perplexity: 1708.22\n",
            "12/03/2021 22:05:15 - INFO - __main__ -   [Iter 94000] Loss 5.16  Val Loss: 7.45  Val Perplexity: 1713.62\n",
            "12/03/2021 22:05:35 - INFO - __main__ -   [Iter 96000] Loss 5.19  Val Loss: 7.45  Val Perplexity: 1726.12\n",
            "12/03/2021 22:05:54 - INFO - __main__ -   [Iter 98000] Loss 5.21  Val Loss: 7.46  Val Perplexity: 1734.71\n",
            "12/03/2021 22:06:14 - INFO - __main__ -   [Iter 100000] Loss 4.60  Val Loss: 7.46  Val Perplexity: 1736.26\n",
            "12/03/2021 22:06:34 - INFO - __main__ -   [Iter 102000] Loss 4.47  Val Loss: 7.56  Val Perplexity: 1926.00\n",
            "12/03/2021 22:06:53 - INFO - __main__ -   [Iter 104000] Loss 4.51  Val Loss: 7.59  Val Perplexity: 1974.32\n",
            "12/03/2021 22:07:13 - INFO - __main__ -   [Iter 106000] Loss 4.55  Val Loss: 7.62  Val Perplexity: 2032.29\n",
            "12/03/2021 22:07:33 - INFO - __main__ -   [Iter 108000] Loss 4.59  Val Loss: 7.61  Val Perplexity: 2025.21\n",
            "12/03/2021 22:07:52 - INFO - __main__ -   [Iter 110000] Loss 4.62  Val Loss: 7.62  Val Perplexity: 2031.28\n",
            "12/03/2021 22:08:12 - INFO - __main__ -   [Iter 112000] Loss 4.66  Val Loss: 7.64  Val Perplexity: 2079.69\n",
            "12/03/2021 22:08:32 - INFO - __main__ -   [Iter 114000] Loss 4.69  Val Loss: 7.62  Val Perplexity: 2045.69\n",
            "12/03/2021 22:08:52 - INFO - __main__ -   [Iter 116000] Loss 4.73  Val Loss: 7.62  Val Perplexity: 2042.89\n",
            "12/03/2021 22:09:11 - INFO - __main__ -   [Iter 118000] Loss 4.76  Val Loss: 7.59  Val Perplexity: 1969.69\n",
            "12/03/2021 22:09:31 - INFO - __main__ -   [Iter 120000] Loss 4.79  Val Loss: 7.62  Val Perplexity: 2039.36\n",
            "12/03/2021 22:09:51 - INFO - __main__ -   [Iter 122000] Loss 4.82  Val Loss: 7.60  Val Perplexity: 2001.81\n",
            "12/03/2021 22:10:10 - INFO - __main__ -   [Iter 124000] Loss 4.85  Val Loss: 7.62  Val Perplexity: 2037.00\n",
            "12/03/2021 22:10:30 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 22:10:30 - INFO - __main__ -     eval_loss = 7.5901734711837765\n",
            "12/03/2021 22:10:30 - INFO - __main__ -     eval_perplexity = 1978.6567239008878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xelcHNNZz5WA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06ac8a6-82e0-4f53-ff8a-b39a1b17400b"
      },
      "source": [
        "!python run_lstm.py \\\n",
        "  --username \"wsj\" \\\n",
        "  --num_epochs 5 \\\n",
        "  --eval_every 2000 \\\n",
        "  --hidden_size 512 \\\n",
        "  --use_pretrained_embeddings True \\\n",
        "  --lr 0.0004"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/NLP/project/dataset.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[filter]\n",
            "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 9000     /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 17000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 23000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 28000    /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Namespace(bos_token='<BOS>', dropout=0.2, embedding_dim=300, eos_token='<EOS>', eval_every=2000, hidden_size=512, length=50, lr=0.0004, n_gpu=1, n_layers=1, no_cuda=False, num_epochs=5, output_dir='outputs', seed=42, train_split=0.8, use_pretrained_embeddings=True, username='wsj')\n",
            "12/03/2021 22:10:40 - INFO - __main__ -   Using pretrained twitter GloVe embeddings...\n",
            "12/03/2021 22:10:40 - INFO - torchtext.vocab.vectors -   Loading vectors from .vector_cache/glove.twitter.27B.200d.txt.pt\n",
            "12/03/2021 22:11:05 - INFO - __main__ -   [Iter 2000] Loss 7.63  Val Loss: 7.35  Val Perplexity: 1560.03\n",
            "12/03/2021 22:11:24 - INFO - __main__ -   [Iter 4000] Loss 7.41  Val Loss: 7.17  Val Perplexity: 1301.79\n",
            "12/03/2021 22:11:43 - INFO - __main__ -   [Iter 6000] Loss 7.28  Val Loss: 7.07  Val Perplexity: 1172.93\n",
            "12/03/2021 22:12:02 - INFO - __main__ -   [Iter 8000] Loss 7.18  Val Loss: 7.00  Val Perplexity: 1095.43\n",
            "12/03/2021 22:12:21 - INFO - __main__ -   [Iter 10000] Loss 7.12  Val Loss: 6.91  Val Perplexity: 1004.50\n",
            "12/03/2021 22:12:40 - INFO - __main__ -   [Iter 12000] Loss 7.06  Val Loss: 6.88  Val Perplexity: 969.29\n",
            "12/03/2021 22:12:59 - INFO - __main__ -   [Iter 14000] Loss 7.01  Val Loss: 6.84  Val Perplexity: 934.12\n",
            "12/03/2021 22:13:17 - INFO - __main__ -   [Iter 16000] Loss 6.97  Val Loss: 6.80  Val Perplexity: 896.07\n",
            "12/03/2021 22:13:36 - INFO - __main__ -   [Iter 18000] Loss 6.94  Val Loss: 6.79  Val Perplexity: 890.04\n",
            "12/03/2021 22:13:55 - INFO - __main__ -   [Iter 20000] Loss 6.90  Val Loss: 6.75  Val Perplexity: 853.20\n",
            "12/03/2021 22:14:14 - INFO - __main__ -   [Iter 22000] Loss 6.87  Val Loss: 6.72  Val Perplexity: 828.84\n",
            "12/03/2021 22:14:33 - INFO - __main__ -   [Iter 24000] Loss 6.05  Val Loss: 6.77  Val Perplexity: 871.37\n",
            "12/03/2021 22:14:52 - INFO - __main__ -   [Iter 26000] Loss 6.05  Val Loss: 6.80  Val Perplexity: 897.67\n",
            "12/03/2021 22:15:11 - INFO - __main__ -   [Iter 28000] Loss 6.08  Val Loss: 6.81  Val Perplexity: 911.06\n",
            "12/03/2021 22:15:30 - INFO - __main__ -   [Iter 30000] Loss 6.08  Val Loss: 6.78  Val Perplexity: 878.93\n",
            "12/03/2021 22:15:49 - INFO - __main__ -   [Iter 32000] Loss 6.09  Val Loss: 6.77  Val Perplexity: 875.62\n",
            "12/03/2021 22:16:08 - INFO - __main__ -   [Iter 34000] Loss 6.10  Val Loss: 6.81  Val Perplexity: 902.50\n",
            "12/03/2021 22:16:26 - INFO - __main__ -   [Iter 36000] Loss 6.11  Val Loss: 6.77  Val Perplexity: 870.91\n",
            "12/03/2021 22:16:45 - INFO - __main__ -   [Iter 38000] Loss 6.11  Val Loss: 6.76  Val Perplexity: 866.39\n",
            "12/03/2021 22:17:04 - INFO - __main__ -   [Iter 40000] Loss 6.12  Val Loss: 6.75  Val Perplexity: 849.91\n",
            "12/03/2021 22:17:23 - INFO - __main__ -   [Iter 42000] Loss 6.13  Val Loss: 6.76  Val Perplexity: 862.60\n",
            "12/03/2021 22:17:42 - INFO - __main__ -   [Iter 44000] Loss 6.14  Val Loss: 6.74  Val Perplexity: 846.29\n",
            "12/03/2021 22:18:01 - INFO - __main__ -   [Iter 46000] Loss 6.14  Val Loss: 6.73  Val Perplexity: 836.28\n",
            "12/03/2021 22:18:20 - INFO - __main__ -   [Iter 48000] Loss 5.53  Val Loss: 6.75  Val Perplexity: 850.84\n",
            "12/03/2021 22:18:38 - INFO - __main__ -   [Iter 50000] Loss 5.51  Val Loss: 6.79  Val Perplexity: 892.33\n",
            "12/03/2021 22:18:57 - INFO - __main__ -   [Iter 52000] Loss 5.52  Val Loss: 6.83  Val Perplexity: 924.06\n",
            "12/03/2021 22:19:16 - INFO - __main__ -   [Iter 54000] Loss 5.54  Val Loss: 6.85  Val Perplexity: 939.77\n",
            "12/03/2021 22:19:35 - INFO - __main__ -   [Iter 56000] Loss 5.57  Val Loss: 6.82  Val Perplexity: 917.07\n",
            "12/03/2021 22:19:54 - INFO - __main__ -   [Iter 58000] Loss 5.60  Val Loss: 6.86  Val Perplexity: 949.68\n",
            "12/03/2021 22:20:13 - INFO - __main__ -   [Iter 60000] Loss 5.62  Val Loss: 6.84  Val Perplexity: 931.87\n",
            "12/03/2021 22:20:32 - INFO - __main__ -   [Iter 62000] Loss 5.63  Val Loss: 6.85  Val Perplexity: 942.10\n",
            "12/03/2021 22:20:51 - INFO - __main__ -   [Iter 64000] Loss 5.66  Val Loss: 6.84  Val Perplexity: 931.68\n",
            "12/03/2021 22:21:09 - INFO - __main__ -   [Iter 66000] Loss 5.68  Val Loss: 6.82  Val Perplexity: 915.76\n",
            "12/03/2021 22:21:28 - INFO - __main__ -   [Iter 68000] Loss 5.70  Val Loss: 6.83  Val Perplexity: 927.39\n",
            "12/03/2021 22:21:47 - INFO - __main__ -   [Iter 70000] Loss 5.71  Val Loss: 6.83  Val Perplexity: 926.47\n",
            "12/03/2021 22:22:06 - INFO - __main__ -   [Iter 72000] Loss 4.99  Val Loss: 6.90  Val Perplexity: 994.21\n",
            "12/03/2021 22:22:25 - INFO - __main__ -   [Iter 74000] Loss 5.02  Val Loss: 6.92  Val Perplexity: 1016.85\n",
            "12/03/2021 22:22:43 - INFO - __main__ -   [Iter 76000] Loss 5.05  Val Loss: 6.95  Val Perplexity: 1038.92\n",
            "12/03/2021 22:23:02 - INFO - __main__ -   [Iter 78000] Loss 5.08  Val Loss: 6.94  Val Perplexity: 1032.94\n",
            "12/03/2021 22:23:21 - INFO - __main__ -   [Iter 80000] Loss 5.12  Val Loss: 6.98  Val Perplexity: 1070.48\n",
            "12/03/2021 22:23:40 - INFO - __main__ -   [Iter 82000] Loss 5.16  Val Loss: 6.94  Val Perplexity: 1034.11\n",
            "12/03/2021 22:23:59 - INFO - __main__ -   [Iter 84000] Loss 5.19  Val Loss: 6.95  Val Perplexity: 1045.74\n",
            "12/03/2021 22:24:17 - INFO - __main__ -   [Iter 86000] Loss 5.23  Val Loss: 6.93  Val Perplexity: 1023.42\n",
            "12/03/2021 22:24:36 - INFO - __main__ -   [Iter 88000] Loss 5.25  Val Loss: 6.94  Val Perplexity: 1032.47\n",
            "12/03/2021 22:24:55 - INFO - __main__ -   [Iter 90000] Loss 5.28  Val Loss: 6.96  Val Perplexity: 1048.56\n",
            "12/03/2021 22:25:14 - INFO - __main__ -   [Iter 92000] Loss 5.30  Val Loss: 6.95  Val Perplexity: 1047.14\n",
            "12/03/2021 22:25:33 - INFO - __main__ -   [Iter 94000] Loss 5.33  Val Loss: 6.96  Val Perplexity: 1051.31\n",
            "12/03/2021 22:25:52 - INFO - __main__ -   [Iter 96000] Loss 4.59  Val Loss: 7.04  Val Perplexity: 1136.63\n",
            "12/03/2021 22:26:10 - INFO - __main__ -   [Iter 98000] Loss 4.59  Val Loss: 7.06  Val Perplexity: 1165.29\n",
            "12/03/2021 22:26:29 - INFO - __main__ -   [Iter 100000] Loss 4.64  Val Loss: 7.09  Val Perplexity: 1201.81\n",
            "12/03/2021 22:26:48 - INFO - __main__ -   [Iter 102000] Loss 4.69  Val Loss: 7.11  Val Perplexity: 1220.88\n",
            "12/03/2021 22:27:07 - INFO - __main__ -   [Iter 104000] Loss 4.73  Val Loss: 7.14  Val Perplexity: 1255.48\n",
            "12/03/2021 22:27:26 - INFO - __main__ -   [Iter 106000] Loss 4.77  Val Loss: 7.12  Val Perplexity: 1235.30\n",
            "12/03/2021 22:27:44 - INFO - __main__ -   [Iter 108000] Loss 4.81  Val Loss: 7.09  Val Perplexity: 1196.22\n",
            "12/03/2021 22:28:03 - INFO - __main__ -   [Iter 110000] Loss 4.84  Val Loss: 7.14  Val Perplexity: 1258.01\n",
            "12/03/2021 22:28:22 - INFO - __main__ -   [Iter 112000] Loss 4.88  Val Loss: 7.14  Val Perplexity: 1263.72\n",
            "12/03/2021 22:28:41 - INFO - __main__ -   [Iter 114000] Loss 4.91  Val Loss: 7.10  Val Perplexity: 1212.56\n",
            "12/03/2021 22:29:00 - INFO - __main__ -   [Iter 116000] Loss 4.94  Val Loss: 7.09  Val Perplexity: 1203.91\n",
            "12/03/2021 22:29:24 - INFO - __main__ -   ***** Eval results *****\n",
            "12/03/2021 22:29:24 - INFO - __main__ -     eval_loss = 7.10003315632121\n",
            "12/03/2021 22:29:24 - INFO - __main__ -     eval_perplexity = 1212.0072595283848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3x3i0sM9tYa"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VI8br9F9tYa",
        "outputId": "15da8b24-f544-48da-debd-9dccecdffac4"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"elonmusk\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 0: At test flight legs about climate on Monday. This is real →?\n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 1: \n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 2: Tesla Model X video a release Powerwall car ever in the Atlantic\n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 3: \n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 4: Landing Tesla patents\n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 5: First video rocket factory in Texas\n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 6: The first launch is deleting live in Instructions )\n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 7: My upgrade details\n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 8: Model 3 Performance\n",
            "12/03/2021 22:29:34 - INFO - __main__ -   Sample 9: I want I can never\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvqboLvH9tYa",
        "outputId": "9fd82a20-b3c8-44a0-cf5a-536292b36ac7"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"karpathy\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 0: A company imagines of my nostalgia and Deep Learning from Joan Bruna on the trenches # icra2013 # sorrow 1\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 1: This is what for a bit but I' m starting a very corner. I like a bug for the PhD liaison :\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 2: Nice roundup of my nostalgia from Google talk by MIT CSAIL I can a lot that of the research here of code favorites :\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 3: I put up a bit more articles from 1800msfwdbwd than a lot of ago, I' m trying to realize that with his. Fun to train\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 4: Working one of a cat of the brain in the politics of entertainment\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 5: Also a nice idea from ECCV on a day - scale manufacturing. I want to spread on Vine, but I' m a few bug of 1\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 6: I feel all the new ones. I have a bit. I' m supposed about it' s the best head of code and received ) of this\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 7: I' m doing new about for training, but I think I' m so happy on my hands. I' m very good to go.\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 8: \" Why company Vision Meets \" Deep Learning : \" A Productivity : \" Is \" it' s fairly well!\n",
            "12/03/2021 22:29:44 - INFO - __main__ -   Sample 9: A cool roundup of images with Neural Networks the internet :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clKtZHbo9tYa",
        "outputId": "60fa8b4a-c242-4be4-88c5-c80f1853d71e"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"BarackObama\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 0: President Obama : \" We know what change looks like because of the most important that has come roaring to be a.\" - passed President Obama\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 1: \" We' ve got to make sure that every American is heard when they' ve got to work for us all gun.\" — President Obama\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 2: “ We know that we can be as one nation we can keep moving forward. We ’ re going to get out the vote this year.\" # SOTU\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 3: At an \" America' s second event meeting for President Obama in Chicago :\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 4: If you' re proud of what health care reform, add your name :\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 5: In New York, IN at the State of the Union. We watch. Watch live at\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 6: \" I want to make sure every person, the American people can make a difference in this country.\" — President Obama # SOTU\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 7: A special West Wing Week from Sudan :\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 8: President Obama : “ We know that our country is stronger when everybody when we should let Detroit go bankrupt. We ’ ve got more work to do. We keep moving forward.”\n",
            "12/03/2021 22:29:53 - INFO - __main__ -   Sample 9: This is a big choice for women in your name :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJS3GPtT9tYb",
        "outputId": "995504d9-3f12-4700-ed0f-f80831c5788b"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"cnn\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 0: Obama to call for school assault.\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 1: Judge OKs Casey Anthony released from hospital after collapse - year - old.\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 2: What ’ s your favorite with? @ Bourdain goes to the new few. He' s # gottadoit # vs. @ AC360\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 3: Police : An of' d boy' to be shot, 1 dead in 2\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 4: ' Study new' for action.\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 5: Obama : Clinton in potentially with U. S. official says.\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 6: First, is speaking now at the Olympics. @ Bourdain is on # January tonight, 9pm ET.\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 7: ' Passengers of Man' in Polls - death, after' s killing of dead, dies at 7\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 8: ' Pirates Is' for Obama on his speech.\n",
            "12/03/2021 22:30:02 - INFO - __main__ -   Sample 9: NASA CNN Gadhafi on CNN. comLive : Obama' s' health care bill' for freedom.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYyyDRbZ9tYb",
        "outputId": "7712d5ac-d18a-4c45-f7f7-43fab1b5506b"
      },
      "source": [
        "!python run_lstm_generation.py \\\n",
        "  --username \"wsj\" \\\n",
        "  --num_sequences 10 \\\n",
        "  --temperature 0.8 \\\n",
        "  --k 50 \\\n",
        "  --p 0.9 \\\n",
        "  --seed 52"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 0: Opinion : The SEC behind shooting back on survive ’ s capital - - Russia rule for an IPO\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 1: The new rate - service for an hour of California :\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 2: New York Times s new technology to pay up their 10 - year - old on the market.\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 3: The U. S. has is since a deal to lead the Fed ’ s largest companies - day after Sandy\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 4: Breaking : U. S. Postal Service said it will buy interest rates low - wage tax overhaul\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 5: The battle for America' s most - largest - ever by a group of week\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 6: Opinion : Trump ’ s tax cuts in House would Senate the list of Fed markets leader\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 7: A group of the world' s largest tech companies, China ’ s biggest tech companies are struggling to fall\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 8: The Dow Jones Industrial Average points after the World Cup of the highest level since the year after the company' s stock\n",
            "12/03/2021 22:30:11 - INFO - __main__ -   Sample 9: The most powerful hurricanes in the NBA' s, and a top - security coverage of the world :\n"
          ]
        }
      ]
    }
  ]
}