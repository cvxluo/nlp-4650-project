{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert2bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H45ZL2j68RYL",
        "d5XSWPed7uMo",
        "YahPVb4e8FDs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jhJ_qDS48VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9dffe7-8e89-47e8-de3a-723f6a816b8e"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/NLP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgNtwxjIVBg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4506508-9f4e-4878-93be-d0f343a9d67d"
      },
      "source": [
        "%pip install transformers datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 469 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 21.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 43.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.1 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9-qUamr5_wu",
        "outputId": "dcf2ea2c-73ab-47f4-a491-8de3082560f9"
      },
      "source": [
        "\n",
        "%pip install bert_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.10-py3-none-any.whl (59 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▌                          | 10 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 30 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 40 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 59 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert_score) (3.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0numpy in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.12.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert_score) (21.3)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert_score) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert_score) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert_score) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->bert_score) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (0.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.0numpy->bert_score) (3.6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0numpy->bert_score) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0numpy->bert_score) (1.1.0)\n",
            "Installing collected packages: bert-score\n",
            "Successfully installed bert-score-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO_i5qjvBOQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95c2684-2b83-4042-dd0e-784f82cf4c68"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  8 04:05:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02loMPQ5mk5d",
        "outputId": "4ccd61f3-6e1f-4477-f935-7f685682ccfb"
      },
      "source": [
        "!python run_mlm.py --output_dir=models/cnn --overwrite_output_dir --model_type=bert --model_name_or_path=bert-base-cased \\\n",
        "  --do_train --train_file=data/cnn/train.txt --validation_file=data/cnn/valid.txt --do_eval --per_device_train_batch_size=2 \\\n",
        "  --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/08/2021 04:08:40 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/08/2021 04:08:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/cnn/runs/Dec08_04-08-40_b82b51ab7198,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "output_dir=models/cnn,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/cnn,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "12/08/2021 04:08:40 - WARNING - datasets.builder - Using custom data configuration default-8c10e270da0bc8e9\n",
            "12/08/2021 04:08:40 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-8c10e270da0bc8e9/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-8c10e270da0bc8e9/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "100% 2/2 [00:00<00:00, 2425.86it/s]\n",
            "12/08/2021 04:08:40 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "12/08/2021 04:08:41 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 65.67it/s]\n",
            "12/08/2021 04:08:41 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "12/08/2021 04:08:41 - INFO - datasets.builder - Generating split train\n",
            "12/08/2021 04:08:41 - INFO - datasets.builder - Generating split validation\n",
            "12/08/2021 04:08:41 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-8c10e270da0bc8e9/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 303.02it/s]\n",
            "[INFO|file_utils.py:1753] 2021-12-08 04:08:41,740 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpthbso84j\n",
            "Downloading: 100% 570/570 [00:00<00:00, 548kB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-08 04:08:41,908 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|file_utils.py:1765] 2021-12-08 04:08:41,908 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:588] 2021-12-08 04:08:41,908 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:625] 2021-12-08 04:08:41,909 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1753] 2021-12-08 04:08:42,065 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8_94kuu3\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 28.1kB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-08 04:08:42,230 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1765] 2021-12-08 04:08:42,231 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:588] 2021-12-08 04:08:42,394 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:625] 2021-12-08 04:08:42,395 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1753] 2021-12-08 04:08:42,725 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_td17hpb\n",
            "Downloading: 100% 208k/208k [00:00<00:00, 1.51MB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-08 04:08:43,030 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1765] 2021-12-08 04:08:43,030 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1753] 2021-12-08 04:08:43,195 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptdv261pf\n",
            "Downloading: 100% 426k/426k [00:00<00:00, 2.44MB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-08 04:08:43,543 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1765] 2021-12-08 04:08:43,543 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-08 04:08:44,029 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-08 04:08:44,029 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-08 04:08:44,029 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-08 04:08:44,029 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-08 04:08:44,029 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:588] 2021-12-08 04:08:44,193 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:625] 2021-12-08 04:08:44,194 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1753] 2021-12-08 04:08:44,396 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppkqj25h6\n",
            "Downloading: 100% 416M/416M [00:18<00:00, 23.5MB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-08 04:09:03,016 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[INFO|file_utils.py:1765] 2021-12-08 04:09:03,016 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[INFO|modeling_utils.py:1351] 2021-12-08 04:09:03,016 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[WARNING|modeling_utils.py:1610] 2021-12-08 04:09:05,095 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1627] 2021-12-08 04:09:05,096 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "Running tokenizer on dataset line_by_line:   0% 0/22 [00:00<?, ?ba/s]12/08/2021 04:09:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8c10e270da0bc8e9/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-0e1eefd0df4f71bc.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 22/22 [00:01<00:00, 12.89ba/s]\n",
            "Running tokenizer on dataset line_by_line:   0% 0/7 [00:00<?, ?ba/s]12/08/2021 04:09:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8c10e270da0bc8e9/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-a33640e0f6831e05.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 7/7 [00:00<00:00, 12.77ba/s]\n",
            "[INFO|tokenization_utils_base.py:888] 2021-12-08 04:09:20,005 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-12-08 04:09:20,005 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-12-08 04:09:20,006 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "[INFO|trainer.py:541] 2021-12-08 04:09:20,209 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1196] 2021-12-08 04:09:20,219 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-12-08 04:09:20,220 >>   Num examples = 21868\n",
            "[INFO|trainer.py:1198] 2021-12-08 04:09:20,220 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1199] 2021-12-08 04:09:20,220 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-12-08 04:09:20,220 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-12-08 04:09:20,220 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-12-08 04:09:20,220 >>   Total optimization steps = 54670\n",
            "{'loss': 2.4624, 'learning_rate': 4.9542710810316444e-05, 'epoch': 0.05}\n",
            "  1% 500/54670 [01:16<2:24:00,  6.27it/s][INFO|trainer.py:1995] 2021-12-08 04:10:36,281 >> Saving model checkpoint to models/cnn/checkpoint-500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:10:36,289 >> Configuration saved in models/cnn/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:10:38,307 >> Model weights saved in models/cnn/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:10:38,314 >> tokenizer config file saved in models/cnn/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:10:38,319 >> Special tokens file saved in models/cnn/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.4549, 'learning_rate': 4.908542162063289e-05, 'epoch': 0.09}\n",
            "  2% 1000/54670 [02:41<2:12:54,  6.73it/s][INFO|trainer.py:1995] 2021-12-08 04:12:01,295 >> Saving model checkpoint to models/cnn/checkpoint-1000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:12:01,302 >> Configuration saved in models/cnn/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:12:03,461 >> Model weights saved in models/cnn/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:12:03,467 >> tokenizer config file saved in models/cnn/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:12:03,474 >> Special tokens file saved in models/cnn/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 2.3036, 'learning_rate': 4.8628132430949334e-05, 'epoch': 0.14}\n",
            "  3% 1500/54670 [04:05<2:12:36,  6.68it/s][INFO|trainer.py:1995] 2021-12-08 04:13:25,720 >> Saving model checkpoint to models/cnn/checkpoint-1500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:13:25,728 >> Configuration saved in models/cnn/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:13:27,923 >> Model weights saved in models/cnn/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:13:27,933 >> tokenizer config file saved in models/cnn/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:13:27,938 >> Special tokens file saved in models/cnn/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 2.3303, 'learning_rate': 4.8170843241265775e-05, 'epoch': 0.18}\n",
            "  4% 2000/54670 [05:32<2:09:22,  6.78it/s][INFO|trainer.py:1995] 2021-12-08 04:14:52,552 >> Saving model checkpoint to models/cnn/checkpoint-2000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:14:52,559 >> Configuration saved in models/cnn/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:14:54,738 >> Model weights saved in models/cnn/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:14:54,744 >> tokenizer config file saved in models/cnn/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:14:54,751 >> Special tokens file saved in models/cnn/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 2.477, 'learning_rate': 4.7713554051582224e-05, 'epoch': 0.23}\n",
            "  5% 2500/54670 [06:58<2:08:23,  6.77it/s][INFO|trainer.py:1995] 2021-12-08 04:16:18,314 >> Saving model checkpoint to models/cnn/checkpoint-2500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:16:18,321 >> Configuration saved in models/cnn/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:16:20,486 >> Model weights saved in models/cnn/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:16:20,493 >> tokenizer config file saved in models/cnn/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:16:20,498 >> Special tokens file saved in models/cnn/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 2.2813, 'learning_rate': 4.725626486189867e-05, 'epoch': 0.27}\n",
            "  5% 3000/54670 [08:24<2:12:35,  6.49it/s][INFO|trainer.py:1995] 2021-12-08 04:17:44,458 >> Saving model checkpoint to models/cnn/checkpoint-3000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:17:44,466 >> Configuration saved in models/cnn/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:17:46,579 >> Model weights saved in models/cnn/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:17:46,586 >> tokenizer config file saved in models/cnn/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:17:46,594 >> Special tokens file saved in models/cnn/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 2.3655, 'learning_rate': 4.6798975672215114e-05, 'epoch': 0.32}\n",
            "  6% 3500/54670 [09:48<2:04:02,  6.88it/s][INFO|trainer.py:1995] 2021-12-08 04:19:09,042 >> Saving model checkpoint to models/cnn/checkpoint-3500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:19:09,048 >> Configuration saved in models/cnn/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:19:11,127 >> Model weights saved in models/cnn/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:19:11,247 >> tokenizer config file saved in models/cnn/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:19:11,254 >> Special tokens file saved in models/cnn/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 2.3131, 'learning_rate': 4.6341686482531555e-05, 'epoch': 0.37}\n",
            "  7% 4000/54670 [11:13<2:14:04,  6.30it/s][INFO|trainer.py:1995] 2021-12-08 04:20:34,093 >> Saving model checkpoint to models/cnn/checkpoint-4000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:20:34,100 >> Configuration saved in models/cnn/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:20:36,251 >> Model weights saved in models/cnn/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:20:36,257 >> tokenizer config file saved in models/cnn/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:20:36,265 >> Special tokens file saved in models/cnn/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 2.2973, 'learning_rate': 4.5884397292847997e-05, 'epoch': 0.41}\n",
            "  8% 4500/54670 [12:41<2:08:33,  6.50it/s][INFO|trainer.py:1995] 2021-12-08 04:22:01,540 >> Saving model checkpoint to models/cnn/checkpoint-4500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:22:01,549 >> Configuration saved in models/cnn/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:22:03,735 >> Model weights saved in models/cnn/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:22:03,744 >> tokenizer config file saved in models/cnn/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:22:03,750 >> Special tokens file saved in models/cnn/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 2.2581, 'learning_rate': 4.5427108103164445e-05, 'epoch': 0.46}\n",
            "  9% 5000/54670 [14:06<2:03:06,  6.72it/s][INFO|trainer.py:1995] 2021-12-08 04:23:26,543 >> Saving model checkpoint to models/cnn/checkpoint-5000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:23:26,551 >> Configuration saved in models/cnn/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:23:28,704 >> Model weights saved in models/cnn/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:23:28,710 >> tokenizer config file saved in models/cnn/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:23:28,718 >> Special tokens file saved in models/cnn/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 2.3915, 'learning_rate': 4.4969818913480886e-05, 'epoch': 0.5}\n",
            " 10% 5500/54670 [15:31<1:59:55,  6.83it/s][INFO|trainer.py:1995] 2021-12-08 04:24:51,445 >> Saving model checkpoint to models/cnn/checkpoint-5500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:24:51,458 >> Configuration saved in models/cnn/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:24:53,707 >> Model weights saved in models/cnn/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:24:53,716 >> tokenizer config file saved in models/cnn/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:24:53,723 >> Special tokens file saved in models/cnn/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 2.3682, 'learning_rate': 4.451252972379733e-05, 'epoch': 0.55}\n",
            " 11% 6000/54670 [16:57<2:03:36,  6.56it/s][INFO|trainer.py:1995] 2021-12-08 04:26:17,707 >> Saving model checkpoint to models/cnn/checkpoint-6000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:26:17,715 >> Configuration saved in models/cnn/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:26:19,859 >> Model weights saved in models/cnn/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:26:19,866 >> tokenizer config file saved in models/cnn/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:26:19,872 >> Special tokens file saved in models/cnn/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 2.2733, 'learning_rate': 4.4055240534113776e-05, 'epoch': 0.59}\n",
            " 12% 6500/54670 [18:23<2:02:00,  6.58it/s][INFO|trainer.py:1995] 2021-12-08 04:27:43,825 >> Saving model checkpoint to models/cnn/checkpoint-6500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:27:43,832 >> Configuration saved in models/cnn/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:27:45,977 >> Model weights saved in models/cnn/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:27:45,984 >> tokenizer config file saved in models/cnn/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:27:45,993 >> Special tokens file saved in models/cnn/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 2.1857, 'learning_rate': 4.359795134443022e-05, 'epoch': 0.64}\n",
            " 13% 7000/54670 [19:49<2:05:04,  6.35it/s][INFO|trainer.py:1995] 2021-12-08 04:29:09,642 >> Saving model checkpoint to models/cnn/checkpoint-7000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:29:09,649 >> Configuration saved in models/cnn/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:29:11,792 >> Model weights saved in models/cnn/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:29:11,798 >> tokenizer config file saved in models/cnn/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:29:11,804 >> Special tokens file saved in models/cnn/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 2.2701, 'learning_rate': 4.3140662154746666e-05, 'epoch': 0.69}\n",
            " 14% 7500/54670 [21:14<1:57:48,  6.67it/s][INFO|trainer.py:1995] 2021-12-08 04:30:35,180 >> Saving model checkpoint to models/cnn/checkpoint-7500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:30:35,191 >> Configuration saved in models/cnn/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:30:37,433 >> Model weights saved in models/cnn/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:30:37,441 >> tokenizer config file saved in models/cnn/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:30:37,459 >> Special tokens file saved in models/cnn/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 2.3871, 'learning_rate': 4.268337296506311e-05, 'epoch': 0.73}\n",
            " 15% 8000/54670 [22:41<2:03:41,  6.29it/s][INFO|trainer.py:1995] 2021-12-08 04:32:01,313 >> Saving model checkpoint to models/cnn/checkpoint-8000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:32:01,323 >> Configuration saved in models/cnn/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:32:03,700 >> Model weights saved in models/cnn/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:32:03,707 >> tokenizer config file saved in models/cnn/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:32:03,712 >> Special tokens file saved in models/cnn/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 2.2193, 'learning_rate': 4.2226083775379556e-05, 'epoch': 0.78}\n",
            " 16% 8500/54670 [24:07<1:59:17,  6.45it/s][INFO|trainer.py:1995] 2021-12-08 04:33:27,738 >> Saving model checkpoint to models/cnn/checkpoint-8500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:33:27,756 >> Configuration saved in models/cnn/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:33:29,952 >> Model weights saved in models/cnn/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:33:29,961 >> tokenizer config file saved in models/cnn/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:33:29,971 >> Special tokens file saved in models/cnn/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 2.2145, 'learning_rate': 4.1768794585696e-05, 'epoch': 0.82}\n",
            " 16% 9000/54670 [25:32<1:51:54,  6.80it/s][INFO|trainer.py:1995] 2021-12-08 04:34:52,811 >> Saving model checkpoint to models/cnn/checkpoint-9000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:34:52,818 >> Configuration saved in models/cnn/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:34:55,238 >> Model weights saved in models/cnn/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:34:55,245 >> tokenizer config file saved in models/cnn/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:34:55,250 >> Special tokens file saved in models/cnn/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 2.1216, 'learning_rate': 4.131150539601244e-05, 'epoch': 0.87}\n",
            " 17% 9500/54670 [26:59<1:56:37,  6.45it/s][INFO|trainer.py:1995] 2021-12-08 04:36:20,261 >> Saving model checkpoint to models/cnn/checkpoint-9500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:36:20,269 >> Configuration saved in models/cnn/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:36:22,387 >> Model weights saved in models/cnn/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:36:22,472 >> tokenizer config file saved in models/cnn/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:36:22,477 >> Special tokens file saved in models/cnn/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 2.3319, 'learning_rate': 4.085421620632888e-05, 'epoch': 0.91}\n",
            " 18% 10000/54670 [28:24<1:51:20,  6.69it/s][INFO|trainer.py:1995] 2021-12-08 04:37:45,214 >> Saving model checkpoint to models/cnn/checkpoint-10000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:37:45,222 >> Configuration saved in models/cnn/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:37:47,447 >> Model weights saved in models/cnn/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:37:47,456 >> tokenizer config file saved in models/cnn/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:37:47,464 >> Special tokens file saved in models/cnn/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 2.2155, 'learning_rate': 4.039692701664533e-05, 'epoch': 0.96}\n",
            " 19% 10500/54670 [29:50<1:49:08,  6.74it/s][INFO|trainer.py:1995] 2021-12-08 04:39:10,810 >> Saving model checkpoint to models/cnn/checkpoint-10500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:39:10,817 >> Configuration saved in models/cnn/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:39:12,970 >> Model weights saved in models/cnn/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:39:12,982 >> tokenizer config file saved in models/cnn/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:39:12,987 >> Special tokens file saved in models/cnn/checkpoint-10500/special_tokens_map.json\n",
            "{'loss': 2.193, 'learning_rate': 3.993963782696177e-05, 'epoch': 1.01}\n",
            " 20% 11000/54670 [31:16<1:48:26,  6.71it/s][INFO|trainer.py:1995] 2021-12-08 04:40:36,670 >> Saving model checkpoint to models/cnn/checkpoint-11000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:40:36,676 >> Configuration saved in models/cnn/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:40:38,738 >> Model weights saved in models/cnn/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:40:38,747 >> tokenizer config file saved in models/cnn/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:40:38,752 >> Special tokens file saved in models/cnn/checkpoint-11000/special_tokens_map.json\n",
            "{'loss': 2.0873, 'learning_rate': 3.948234863727822e-05, 'epoch': 1.05}\n",
            " 21% 11500/54670 [32:42<1:46:31,  6.75it/s][INFO|trainer.py:1995] 2021-12-08 04:42:03,021 >> Saving model checkpoint to models/cnn/checkpoint-11500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:42:03,027 >> Configuration saved in models/cnn/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:42:05,182 >> Model weights saved in models/cnn/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:42:05,188 >> tokenizer config file saved in models/cnn/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:42:05,599 >> Special tokens file saved in models/cnn/checkpoint-11500/special_tokens_map.json\n",
            "{'loss': 2.0377, 'learning_rate': 3.902505944759466e-05, 'epoch': 1.1}\n",
            " 22% 12000/54670 [34:07<1:54:12,  6.23it/s][INFO|trainer.py:1995] 2021-12-08 04:43:28,013 >> Saving model checkpoint to models/cnn/checkpoint-12000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:43:28,021 >> Configuration saved in models/cnn/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:43:30,704 >> Model weights saved in models/cnn/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:43:30,711 >> tokenizer config file saved in models/cnn/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:43:30,716 >> Special tokens file saved in models/cnn/checkpoint-12000/special_tokens_map.json\n",
            "{'loss': 2.1726, 'learning_rate': 3.856777025791111e-05, 'epoch': 1.14}\n",
            " 23% 12500/54670 [35:33<1:48:01,  6.51it/s][INFO|trainer.py:1995] 2021-12-08 04:44:53,933 >> Saving model checkpoint to models/cnn/checkpoint-12500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:44:53,944 >> Configuration saved in models/cnn/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:44:56,126 >> Model weights saved in models/cnn/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:44:56,146 >> tokenizer config file saved in models/cnn/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:44:56,154 >> Special tokens file saved in models/cnn/checkpoint-12500/special_tokens_map.json\n",
            "{'loss': 2.1369, 'learning_rate': 3.811048106822755e-05, 'epoch': 1.19}\n",
            " 24% 13000/54670 [36:59<1:38:47,  7.03it/s][INFO|trainer.py:1995] 2021-12-08 04:46:19,282 >> Saving model checkpoint to models/cnn/checkpoint-13000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:46:19,289 >> Configuration saved in models/cnn/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:46:21,482 >> Model weights saved in models/cnn/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:46:21,488 >> tokenizer config file saved in models/cnn/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:46:21,494 >> Special tokens file saved in models/cnn/checkpoint-13000/special_tokens_map.json\n",
            "{'loss': 2.1234, 'learning_rate': 3.765319187854399e-05, 'epoch': 1.23}\n",
            " 25% 13500/54670 [38:24<1:49:29,  6.27it/s][INFO|trainer.py:1995] 2021-12-08 04:47:44,835 >> Saving model checkpoint to models/cnn/checkpoint-13500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:47:44,852 >> Configuration saved in models/cnn/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:47:46,986 >> Model weights saved in models/cnn/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:47:46,992 >> tokenizer config file saved in models/cnn/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:47:46,997 >> Special tokens file saved in models/cnn/checkpoint-13500/special_tokens_map.json\n",
            "{'loss': 2.0952, 'learning_rate': 3.719590268886043e-05, 'epoch': 1.28}\n",
            " 26% 14000/54670 [39:51<1:41:45,  6.66it/s][INFO|trainer.py:1995] 2021-12-08 04:49:12,219 >> Saving model checkpoint to models/cnn/checkpoint-14000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:49:12,230 >> Configuration saved in models/cnn/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:49:14,527 >> Model weights saved in models/cnn/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:49:14,533 >> tokenizer config file saved in models/cnn/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:49:14,540 >> Special tokens file saved in models/cnn/checkpoint-14000/special_tokens_map.json\n",
            "{'loss': 2.1256, 'learning_rate': 3.673861349917688e-05, 'epoch': 1.33}\n",
            " 27% 14500/54670 [41:18<1:41:07,  6.62it/s][INFO|trainer.py:1995] 2021-12-08 04:50:38,833 >> Saving model checkpoint to models/cnn/checkpoint-14500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:50:38,839 >> Configuration saved in models/cnn/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:50:41,250 >> Model weights saved in models/cnn/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:50:41,261 >> tokenizer config file saved in models/cnn/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:50:41,267 >> Special tokens file saved in models/cnn/checkpoint-14500/special_tokens_map.json\n",
            "{'loss': 2.156, 'learning_rate': 3.628132430949332e-05, 'epoch': 1.37}\n",
            " 27% 15000/54670 [42:44<1:37:07,  6.81it/s][INFO|trainer.py:1995] 2021-12-08 04:52:05,028 >> Saving model checkpoint to models/cnn/checkpoint-15000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:52:05,035 >> Configuration saved in models/cnn/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:52:07,244 >> Model weights saved in models/cnn/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:52:07,250 >> tokenizer config file saved in models/cnn/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:52:07,255 >> Special tokens file saved in models/cnn/checkpoint-15000/special_tokens_map.json\n",
            "{'loss': 2.0403, 'learning_rate': 3.5824035119809765e-05, 'epoch': 1.42}\n",
            " 28% 15500/54670 [44:10<1:41:47,  6.41it/s][INFO|trainer.py:1995] 2021-12-08 04:53:30,784 >> Saving model checkpoint to models/cnn/checkpoint-15500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:53:30,806 >> Configuration saved in models/cnn/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:53:33,036 >> Model weights saved in models/cnn/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:53:33,043 >> tokenizer config file saved in models/cnn/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:53:33,049 >> Special tokens file saved in models/cnn/checkpoint-15500/special_tokens_map.json\n",
            "{'loss': 1.9074, 'learning_rate': 3.536674593012621e-05, 'epoch': 1.46}\n",
            " 29% 16000/54670 [45:36<1:37:22,  6.62it/s][INFO|trainer.py:1995] 2021-12-08 04:54:57,133 >> Saving model checkpoint to models/cnn/checkpoint-16000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:54:57,140 >> Configuration saved in models/cnn/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:54:59,304 >> Model weights saved in models/cnn/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:54:59,321 >> tokenizer config file saved in models/cnn/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:54:59,333 >> Special tokens file saved in models/cnn/checkpoint-16000/special_tokens_map.json\n",
            "{'loss': 2.1224, 'learning_rate': 3.490945674044266e-05, 'epoch': 1.51}\n",
            " 30% 16500/54670 [47:02<1:30:30,  7.03it/s][INFO|trainer.py:1995] 2021-12-08 04:56:22,969 >> Saving model checkpoint to models/cnn/checkpoint-16500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:56:22,977 >> Configuration saved in models/cnn/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:56:25,166 >> Model weights saved in models/cnn/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:56:25,174 >> tokenizer config file saved in models/cnn/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:56:25,186 >> Special tokens file saved in models/cnn/checkpoint-16500/special_tokens_map.json\n",
            "{'loss': 2.0177, 'learning_rate': 3.44521675507591e-05, 'epoch': 1.55}\n",
            " 31% 17000/54670 [48:29<1:43:08,  6.09it/s][INFO|trainer.py:1995] 2021-12-08 04:57:49,365 >> Saving model checkpoint to models/cnn/checkpoint-17000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:57:49,371 >> Configuration saved in models/cnn/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:57:51,570 >> Model weights saved in models/cnn/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:57:51,577 >> tokenizer config file saved in models/cnn/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:57:51,585 >> Special tokens file saved in models/cnn/checkpoint-17000/special_tokens_map.json\n",
            "{'loss': 2.0653, 'learning_rate': 3.3994878361075544e-05, 'epoch': 1.6}\n",
            " 32% 17500/54670 [49:54<1:31:28,  6.77it/s][INFO|trainer.py:1995] 2021-12-08 04:59:14,691 >> Saving model checkpoint to models/cnn/checkpoint-17500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 04:59:14,698 >> Configuration saved in models/cnn/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 04:59:16,989 >> Model weights saved in models/cnn/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 04:59:17,418 >> tokenizer config file saved in models/cnn/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 04:59:17,423 >> Special tokens file saved in models/cnn/checkpoint-17500/special_tokens_map.json\n",
            "{'loss': 2.1259, 'learning_rate': 3.353758917139199e-05, 'epoch': 1.65}\n",
            " 33% 18000/54670 [51:20<1:36:47,  6.31it/s][INFO|trainer.py:1995] 2021-12-08 05:00:41,158 >> Saving model checkpoint to models/cnn/checkpoint-18000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:00:41,168 >> Configuration saved in models/cnn/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:00:43,464 >> Model weights saved in models/cnn/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:00:43,472 >> tokenizer config file saved in models/cnn/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:00:43,797 >> Special tokens file saved in models/cnn/checkpoint-18000/special_tokens_map.json\n",
            "{'loss': 2.0114, 'learning_rate': 3.3080299981708434e-05, 'epoch': 1.69}\n",
            " 34% 18500/54670 [52:47<1:33:13,  6.47it/s][INFO|trainer.py:1995] 2021-12-08 05:02:08,020 >> Saving model checkpoint to models/cnn/checkpoint-18500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:02:08,027 >> Configuration saved in models/cnn/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:02:10,335 >> Model weights saved in models/cnn/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:02:10,342 >> tokenizer config file saved in models/cnn/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:02:10,347 >> Special tokens file saved in models/cnn/checkpoint-18500/special_tokens_map.json\n",
            "{'loss': 2.1415, 'learning_rate': 3.2623010792024876e-05, 'epoch': 1.74}\n",
            " 35% 19000/54670 [54:14<1:29:35,  6.64it/s][INFO|trainer.py:1995] 2021-12-08 05:03:35,056 >> Saving model checkpoint to models/cnn/checkpoint-19000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:03:35,063 >> Configuration saved in models/cnn/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:03:37,349 >> Model weights saved in models/cnn/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:03:37,356 >> tokenizer config file saved in models/cnn/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:03:37,361 >> Special tokens file saved in models/cnn/checkpoint-19000/special_tokens_map.json\n",
            "{'loss': 2.0511, 'learning_rate': 3.216572160234132e-05, 'epoch': 1.78}\n",
            " 36% 19500/54670 [55:42<1:31:05,  6.43it/s][INFO|trainer.py:1995] 2021-12-08 05:05:02,886 >> Saving model checkpoint to models/cnn/checkpoint-19500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:05:02,894 >> Configuration saved in models/cnn/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:05:05,162 >> Model weights saved in models/cnn/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:05:05,171 >> tokenizer config file saved in models/cnn/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:05:05,570 >> Special tokens file saved in models/cnn/checkpoint-19500/special_tokens_map.json\n",
            "{'loss': 2.0729, 'learning_rate': 3.1708432412657766e-05, 'epoch': 1.83}\n",
            " 37% 20000/54670 [57:10<1:30:37,  6.38it/s][INFO|trainer.py:1995] 2021-12-08 05:06:30,269 >> Saving model checkpoint to models/cnn/checkpoint-20000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:06:30,281 >> Configuration saved in models/cnn/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:06:32,626 >> Model weights saved in models/cnn/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:06:32,634 >> tokenizer config file saved in models/cnn/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:06:32,639 >> Special tokens file saved in models/cnn/checkpoint-20000/special_tokens_map.json\n",
            "{'loss': 1.9747, 'learning_rate': 3.1251143222974214e-05, 'epoch': 1.87}\n",
            " 37% 20500/54670 [58:37<1:25:53,  6.63it/s][INFO|trainer.py:1995] 2021-12-08 05:07:57,331 >> Saving model checkpoint to models/cnn/checkpoint-20500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:07:57,340 >> Configuration saved in models/cnn/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:07:59,719 >> Model weights saved in models/cnn/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:07:59,737 >> tokenizer config file saved in models/cnn/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:07:59,742 >> Special tokens file saved in models/cnn/checkpoint-20500/special_tokens_map.json\n",
            "{'loss': 2.025, 'learning_rate': 3.0793854033290656e-05, 'epoch': 1.92}\n",
            " 38% 21000/54670 [1:00:03<1:24:39,  6.63it/s][INFO|trainer.py:1995] 2021-12-08 05:09:24,020 >> Saving model checkpoint to models/cnn/checkpoint-21000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:09:24,027 >> Configuration saved in models/cnn/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:09:26,176 >> Model weights saved in models/cnn/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:09:26,184 >> tokenizer config file saved in models/cnn/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:09:26,193 >> Special tokens file saved in models/cnn/checkpoint-21000/special_tokens_map.json\n",
            "{'loss': 2.0772, 'learning_rate': 3.0336564843607097e-05, 'epoch': 1.97}\n",
            " 39% 21500/54670 [1:01:28<1:22:26,  6.71it/s][INFO|trainer.py:1995] 2021-12-08 05:10:49,215 >> Saving model checkpoint to models/cnn/checkpoint-21500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:10:49,222 >> Configuration saved in models/cnn/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:10:51,434 >> Model weights saved in models/cnn/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:10:51,442 >> tokenizer config file saved in models/cnn/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:10:51,448 >> Special tokens file saved in models/cnn/checkpoint-21500/special_tokens_map.json\n",
            "{'loss': 1.9921, 'learning_rate': 2.9879275653923545e-05, 'epoch': 2.01}\n",
            " 40% 22000/54670 [1:02:54<1:20:44,  6.74it/s][INFO|trainer.py:1995] 2021-12-08 05:12:14,763 >> Saving model checkpoint to models/cnn/checkpoint-22000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:12:14,771 >> Configuration saved in models/cnn/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:12:16,990 >> Model weights saved in models/cnn/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:12:16,996 >> tokenizer config file saved in models/cnn/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:12:17,002 >> Special tokens file saved in models/cnn/checkpoint-22000/special_tokens_map.json\n",
            "{'loss': 1.8844, 'learning_rate': 2.9421986464239987e-05, 'epoch': 2.06}\n",
            " 41% 22500/54670 [1:04:19<1:18:40,  6.81it/s][INFO|trainer.py:1995] 2021-12-08 05:13:40,095 >> Saving model checkpoint to models/cnn/checkpoint-22500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:13:40,103 >> Configuration saved in models/cnn/checkpoint-22500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:13:42,388 >> Model weights saved in models/cnn/checkpoint-22500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:13:42,394 >> tokenizer config file saved in models/cnn/checkpoint-22500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:13:42,399 >> Special tokens file saved in models/cnn/checkpoint-22500/special_tokens_map.json\n",
            "{'loss': 1.8605, 'learning_rate': 2.896469727455643e-05, 'epoch': 2.1}\n",
            " 42% 23000/54670 [1:05:45<1:23:12,  6.34it/s][INFO|trainer.py:1995] 2021-12-08 05:15:05,679 >> Saving model checkpoint to models/cnn/checkpoint-23000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:15:05,692 >> Configuration saved in models/cnn/checkpoint-23000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:15:07,845 >> Model weights saved in models/cnn/checkpoint-23000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:15:07,850 >> tokenizer config file saved in models/cnn/checkpoint-23000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:15:07,859 >> Special tokens file saved in models/cnn/checkpoint-23000/special_tokens_map.json\n",
            "{'loss': 1.8441, 'learning_rate': 2.8507408084872877e-05, 'epoch': 2.15}\n",
            " 43% 23500/54670 [1:07:11<1:18:32,  6.61it/s][INFO|trainer.py:1995] 2021-12-08 05:16:31,439 >> Saving model checkpoint to models/cnn/checkpoint-23500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:16:31,448 >> Configuration saved in models/cnn/checkpoint-23500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:16:33,669 >> Model weights saved in models/cnn/checkpoint-23500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:16:33,677 >> tokenizer config file saved in models/cnn/checkpoint-23500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:16:33,683 >> Special tokens file saved in models/cnn/checkpoint-23500/special_tokens_map.json\n",
            "{'loss': 1.9108, 'learning_rate': 2.8050118895189322e-05, 'epoch': 2.19}\n",
            " 44% 24000/54670 [1:08:36<1:14:47,  6.83it/s][INFO|trainer.py:1995] 2021-12-08 05:17:57,174 >> Saving model checkpoint to models/cnn/checkpoint-24000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:17:57,181 >> Configuration saved in models/cnn/checkpoint-24000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:17:59,475 >> Model weights saved in models/cnn/checkpoint-24000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:17:59,481 >> tokenizer config file saved in models/cnn/checkpoint-24000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:17:59,485 >> Special tokens file saved in models/cnn/checkpoint-24000/special_tokens_map.json\n",
            "{'loss': 1.9501, 'learning_rate': 2.7592829705505763e-05, 'epoch': 2.24}\n",
            " 45% 24500/54670 [1:10:02<1:18:12,  6.43it/s][INFO|trainer.py:1995] 2021-12-08 05:19:22,515 >> Saving model checkpoint to models/cnn/checkpoint-24500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:19:22,524 >> Configuration saved in models/cnn/checkpoint-24500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:19:24,720 >> Model weights saved in models/cnn/checkpoint-24500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:19:24,729 >> tokenizer config file saved in models/cnn/checkpoint-24500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:19:24,734 >> Special tokens file saved in models/cnn/checkpoint-24500/special_tokens_map.json\n",
            "{'loss': 1.8914, 'learning_rate': 2.7135540515822205e-05, 'epoch': 2.29}\n",
            " 46% 25000/54670 [1:11:28<1:15:17,  6.57it/s][INFO|trainer.py:1995] 2021-12-08 05:20:49,143 >> Saving model checkpoint to models/cnn/checkpoint-25000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:20:49,155 >> Configuration saved in models/cnn/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:20:51,403 >> Model weights saved in models/cnn/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:20:51,410 >> tokenizer config file saved in models/cnn/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:20:51,415 >> Special tokens file saved in models/cnn/checkpoint-25000/special_tokens_map.json\n",
            "{'loss': 1.8351, 'learning_rate': 2.6678251326138653e-05, 'epoch': 2.33}\n",
            " 47% 25500/54670 [1:12:55<1:13:09,  6.64it/s][INFO|trainer.py:1995] 2021-12-08 05:22:15,447 >> Saving model checkpoint to models/cnn/checkpoint-25500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:22:15,455 >> Configuration saved in models/cnn/checkpoint-25500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:22:17,652 >> Model weights saved in models/cnn/checkpoint-25500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:22:17,658 >> tokenizer config file saved in models/cnn/checkpoint-25500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:22:17,663 >> Special tokens file saved in models/cnn/checkpoint-25500/special_tokens_map.json\n",
            "{'loss': 1.8737, 'learning_rate': 2.6220962136455095e-05, 'epoch': 2.38}\n",
            " 48% 26000/54670 [1:14:21<1:14:22,  6.42it/s][INFO|trainer.py:1995] 2021-12-08 05:23:41,975 >> Saving model checkpoint to models/cnn/checkpoint-26000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:23:41,982 >> Configuration saved in models/cnn/checkpoint-26000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:23:44,151 >> Model weights saved in models/cnn/checkpoint-26000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:23:44,158 >> tokenizer config file saved in models/cnn/checkpoint-26000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:23:44,175 >> Special tokens file saved in models/cnn/checkpoint-26000/special_tokens_map.json\n",
            "{'loss': 1.847, 'learning_rate': 2.576367294677154e-05, 'epoch': 2.42}\n",
            " 48% 26500/54670 [1:15:48<1:09:08,  6.79it/s][INFO|trainer.py:1995] 2021-12-08 05:25:08,312 >> Saving model checkpoint to models/cnn/checkpoint-26500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:25:08,320 >> Configuration saved in models/cnn/checkpoint-26500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:25:10,569 >> Model weights saved in models/cnn/checkpoint-26500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:25:10,578 >> tokenizer config file saved in models/cnn/checkpoint-26500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:25:10,583 >> Special tokens file saved in models/cnn/checkpoint-26500/special_tokens_map.json\n",
            "{'loss': 1.7593, 'learning_rate': 2.530638375708798e-05, 'epoch': 2.47}\n",
            " 49% 27000/54670 [1:17:14<1:11:06,  6.48it/s][INFO|trainer.py:1995] 2021-12-08 05:26:34,996 >> Saving model checkpoint to models/cnn/checkpoint-27000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:26:35,004 >> Configuration saved in models/cnn/checkpoint-27000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:26:37,479 >> Model weights saved in models/cnn/checkpoint-27000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:26:37,486 >> tokenizer config file saved in models/cnn/checkpoint-27000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:26:37,491 >> Special tokens file saved in models/cnn/checkpoint-27000/special_tokens_map.json\n",
            "{'loss': 1.8901, 'learning_rate': 2.4849094567404426e-05, 'epoch': 2.52}\n",
            " 50% 27500/54670 [1:18:40<1:06:00,  6.86it/s][INFO|trainer.py:1995] 2021-12-08 05:28:01,063 >> Saving model checkpoint to models/cnn/checkpoint-27500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:28:01,070 >> Configuration saved in models/cnn/checkpoint-27500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:28:03,257 >> Model weights saved in models/cnn/checkpoint-27500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:28:03,265 >> tokenizer config file saved in models/cnn/checkpoint-27500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:28:03,271 >> Special tokens file saved in models/cnn/checkpoint-27500/special_tokens_map.json\n",
            "{'loss': 1.83, 'learning_rate': 2.439180537772087e-05, 'epoch': 2.56}\n",
            " 51% 28000/54670 [1:20:06<1:09:02,  6.44it/s][INFO|trainer.py:1995] 2021-12-08 05:29:27,242 >> Saving model checkpoint to models/cnn/checkpoint-28000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:29:27,249 >> Configuration saved in models/cnn/checkpoint-28000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:29:29,448 >> Model weights saved in models/cnn/checkpoint-28000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:29:29,456 >> tokenizer config file saved in models/cnn/checkpoint-28000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:29:29,462 >> Special tokens file saved in models/cnn/checkpoint-28000/special_tokens_map.json\n",
            "{'loss': 1.8357, 'learning_rate': 2.3934516188037316e-05, 'epoch': 2.61}\n",
            " 52% 28500/54670 [1:21:33<1:04:51,  6.73it/s][INFO|trainer.py:1995] 2021-12-08 05:30:53,932 >> Saving model checkpoint to models/cnn/checkpoint-28500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:30:53,938 >> Configuration saved in models/cnn/checkpoint-28500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:30:56,070 >> Model weights saved in models/cnn/checkpoint-28500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:30:56,081 >> tokenizer config file saved in models/cnn/checkpoint-28500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:30:56,086 >> Special tokens file saved in models/cnn/checkpoint-28500/special_tokens_map.json\n",
            "{'loss': 1.8783, 'learning_rate': 2.347722699835376e-05, 'epoch': 2.65}\n",
            " 53% 29000/54670 [1:23:01<1:06:17,  6.45it/s][INFO|trainer.py:1995] 2021-12-08 05:32:22,009 >> Saving model checkpoint to models/cnn/checkpoint-29000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:32:22,017 >> Configuration saved in models/cnn/checkpoint-29000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:32:24,181 >> Model weights saved in models/cnn/checkpoint-29000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:32:24,187 >> tokenizer config file saved in models/cnn/checkpoint-29000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:32:24,193 >> Special tokens file saved in models/cnn/checkpoint-29000/special_tokens_map.json\n",
            "{'loss': 1.9067, 'learning_rate': 2.3019937808670202e-05, 'epoch': 2.7}\n",
            " 54% 29500/54670 [1:24:27<1:03:28,  6.61it/s][INFO|trainer.py:1995] 2021-12-08 05:33:47,365 >> Saving model checkpoint to models/cnn/checkpoint-29500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:33:47,372 >> Configuration saved in models/cnn/checkpoint-29500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:33:49,632 >> Model weights saved in models/cnn/checkpoint-29500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:33:49,640 >> tokenizer config file saved in models/cnn/checkpoint-29500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:33:49,646 >> Special tokens file saved in models/cnn/checkpoint-29500/special_tokens_map.json\n",
            "{'loss': 1.7683, 'learning_rate': 2.2562648618986647e-05, 'epoch': 2.74}\n",
            " 55% 30000/54670 [1:25:53<1:01:56,  6.64it/s][INFO|trainer.py:1995] 2021-12-08 05:35:13,510 >> Saving model checkpoint to models/cnn/checkpoint-30000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:35:13,517 >> Configuration saved in models/cnn/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:35:15,793 >> Model weights saved in models/cnn/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:35:15,800 >> tokenizer config file saved in models/cnn/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:35:16,144 >> Special tokens file saved in models/cnn/checkpoint-30000/special_tokens_map.json\n",
            "{'loss': 1.8378, 'learning_rate': 2.2105359429303092e-05, 'epoch': 2.79}\n",
            " 56% 30500/54670 [1:27:20<1:02:06,  6.49it/s][INFO|trainer.py:1995] 2021-12-08 05:36:40,909 >> Saving model checkpoint to models/cnn/checkpoint-30500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:36:40,916 >> Configuration saved in models/cnn/checkpoint-30500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:36:43,100 >> Model weights saved in models/cnn/checkpoint-30500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:36:43,108 >> tokenizer config file saved in models/cnn/checkpoint-30500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:36:43,114 >> Special tokens file saved in models/cnn/checkpoint-30500/special_tokens_map.json\n",
            "{'loss': 1.8031, 'learning_rate': 2.1648070239619537e-05, 'epoch': 2.84}\n",
            " 57% 31000/54670 [1:28:47<1:04:17,  6.14it/s][INFO|trainer.py:1995] 2021-12-08 05:38:07,278 >> Saving model checkpoint to models/cnn/checkpoint-31000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:38:07,284 >> Configuration saved in models/cnn/checkpoint-31000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:38:09,393 >> Model weights saved in models/cnn/checkpoint-31000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:38:09,400 >> tokenizer config file saved in models/cnn/checkpoint-31000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:38:09,406 >> Special tokens file saved in models/cnn/checkpoint-31000/special_tokens_map.json\n",
            "{'loss': 1.835, 'learning_rate': 2.1190781049935982e-05, 'epoch': 2.88}\n",
            " 58% 31500/54670 [1:30:12<59:59,  6.44it/s][INFO|trainer.py:1995] 2021-12-08 05:39:32,748 >> Saving model checkpoint to models/cnn/checkpoint-31500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:39:32,754 >> Configuration saved in models/cnn/checkpoint-31500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:39:34,982 >> Model weights saved in models/cnn/checkpoint-31500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:39:34,990 >> tokenizer config file saved in models/cnn/checkpoint-31500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:39:34,995 >> Special tokens file saved in models/cnn/checkpoint-31500/special_tokens_map.json\n",
            "{'loss': 1.8525, 'learning_rate': 2.0733491860252424e-05, 'epoch': 2.93}\n",
            " 59% 32000/54670 [1:31:38<58:08,  6.50it/s][INFO|trainer.py:1995] 2021-12-08 05:40:59,191 >> Saving model checkpoint to models/cnn/checkpoint-32000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:40:59,201 >> Configuration saved in models/cnn/checkpoint-32000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:41:01,523 >> Model weights saved in models/cnn/checkpoint-32000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:41:01,531 >> tokenizer config file saved in models/cnn/checkpoint-32000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:41:01,537 >> Special tokens file saved in models/cnn/checkpoint-32000/special_tokens_map.json\n",
            "{'loss': 1.8297, 'learning_rate': 2.027620267056887e-05, 'epoch': 2.97}\n",
            " 59% 32500/54670 [1:33:05<54:15,  6.81it/s][INFO|trainer.py:1995] 2021-12-08 05:42:26,249 >> Saving model checkpoint to models/cnn/checkpoint-32500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:42:26,261 >> Configuration saved in models/cnn/checkpoint-32500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:42:28,748 >> Model weights saved in models/cnn/checkpoint-32500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:42:28,757 >> tokenizer config file saved in models/cnn/checkpoint-32500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:42:28,766 >> Special tokens file saved in models/cnn/checkpoint-32500/special_tokens_map.json\n",
            "{'loss': 1.8105, 'learning_rate': 1.9818913480885314e-05, 'epoch': 3.02}\n",
            " 60% 33000/54670 [1:34:33<56:31,  6.39it/s][INFO|trainer.py:1995] 2021-12-08 05:43:54,264 >> Saving model checkpoint to models/cnn/checkpoint-33000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:43:54,272 >> Configuration saved in models/cnn/checkpoint-33000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:43:56,825 >> Model weights saved in models/cnn/checkpoint-33000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:43:56,832 >> tokenizer config file saved in models/cnn/checkpoint-33000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:43:56,837 >> Special tokens file saved in models/cnn/checkpoint-33000/special_tokens_map.json\n",
            "{'loss': 1.8006, 'learning_rate': 1.936162429120176e-05, 'epoch': 3.06}\n",
            " 61% 33500/54670 [1:36:01<56:40,  6.22it/s][INFO|trainer.py:1995] 2021-12-08 05:45:22,043 >> Saving model checkpoint to models/cnn/checkpoint-33500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:45:22,051 >> Configuration saved in models/cnn/checkpoint-33500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:45:24,341 >> Model weights saved in models/cnn/checkpoint-33500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:45:24,347 >> tokenizer config file saved in models/cnn/checkpoint-33500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:45:24,353 >> Special tokens file saved in models/cnn/checkpoint-33500/special_tokens_map.json\n",
            "{'loss': 1.6745, 'learning_rate': 1.89043351015182e-05, 'epoch': 3.11}\n",
            " 62% 34000/54670 [1:37:31<54:45,  6.29it/s][INFO|trainer.py:1995] 2021-12-08 05:46:51,544 >> Saving model checkpoint to models/cnn/checkpoint-34000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:46:51,551 >> Configuration saved in models/cnn/checkpoint-34000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:46:53,967 >> Model weights saved in models/cnn/checkpoint-34000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:46:53,974 >> tokenizer config file saved in models/cnn/checkpoint-34000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:46:53,979 >> Special tokens file saved in models/cnn/checkpoint-34000/special_tokens_map.json\n",
            "{'loss': 1.6981, 'learning_rate': 1.8447045911834645e-05, 'epoch': 3.16}\n",
            " 63% 34500/54670 [1:38:59<50:14,  6.69it/s][INFO|trainer.py:1995] 2021-12-08 05:48:19,774 >> Saving model checkpoint to models/cnn/checkpoint-34500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:48:19,781 >> Configuration saved in models/cnn/checkpoint-34500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:48:22,242 >> Model weights saved in models/cnn/checkpoint-34500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:48:22,249 >> tokenizer config file saved in models/cnn/checkpoint-34500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:48:22,255 >> Special tokens file saved in models/cnn/checkpoint-34500/special_tokens_map.json\n",
            "{'loss': 1.8232, 'learning_rate': 1.7989756722151087e-05, 'epoch': 3.2}\n",
            " 64% 35000/54670 [1:40:29<49:58,  6.56it/s][INFO|trainer.py:1995] 2021-12-08 05:49:49,550 >> Saving model checkpoint to models/cnn/checkpoint-35000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:49:49,563 >> Configuration saved in models/cnn/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:49:52,062 >> Model weights saved in models/cnn/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:49:52,069 >> tokenizer config file saved in models/cnn/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:49:52,074 >> Special tokens file saved in models/cnn/checkpoint-35000/special_tokens_map.json\n",
            "{'loss': 1.6376, 'learning_rate': 1.7532467532467535e-05, 'epoch': 3.25}\n",
            " 65% 35500/54670 [1:41:57<50:48,  6.29it/s][INFO|trainer.py:1995] 2021-12-08 05:51:17,656 >> Saving model checkpoint to models/cnn/checkpoint-35500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:51:17,667 >> Configuration saved in models/cnn/checkpoint-35500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:51:20,318 >> Model weights saved in models/cnn/checkpoint-35500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:51:20,325 >> tokenizer config file saved in models/cnn/checkpoint-35500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:51:20,331 >> Special tokens file saved in models/cnn/checkpoint-35500/special_tokens_map.json\n",
            "{'loss': 1.7512, 'learning_rate': 1.7075178342783976e-05, 'epoch': 3.29}\n",
            " 66% 36000/54670 [1:43:25<46:38,  6.67it/s][INFO|trainer.py:1995] 2021-12-08 05:52:45,690 >> Saving model checkpoint to models/cnn/checkpoint-36000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:52:45,715 >> Configuration saved in models/cnn/checkpoint-36000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:52:48,169 >> Model weights saved in models/cnn/checkpoint-36000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:52:48,177 >> tokenizer config file saved in models/cnn/checkpoint-36000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:52:48,184 >> Special tokens file saved in models/cnn/checkpoint-36000/special_tokens_map.json\n",
            "{'loss': 1.745, 'learning_rate': 1.661788915310042e-05, 'epoch': 3.34}\n",
            " 67% 36500/54670 [1:44:53<45:32,  6.65it/s][INFO|trainer.py:1995] 2021-12-08 05:54:13,867 >> Saving model checkpoint to models/cnn/checkpoint-36500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:54:13,876 >> Configuration saved in models/cnn/checkpoint-36500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:54:16,353 >> Model weights saved in models/cnn/checkpoint-36500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:54:16,361 >> tokenizer config file saved in models/cnn/checkpoint-36500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:54:16,367 >> Special tokens file saved in models/cnn/checkpoint-36500/special_tokens_map.json\n",
            "{'loss': 1.6951, 'learning_rate': 1.6160599963416863e-05, 'epoch': 3.38}\n",
            " 68% 37000/54670 [1:46:21<46:46,  6.30it/s][INFO|trainer.py:1995] 2021-12-08 05:55:41,945 >> Saving model checkpoint to models/cnn/checkpoint-37000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:55:41,954 >> Configuration saved in models/cnn/checkpoint-37000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:55:44,268 >> Model weights saved in models/cnn/checkpoint-37000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:55:44,276 >> tokenizer config file saved in models/cnn/checkpoint-37000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:55:44,281 >> Special tokens file saved in models/cnn/checkpoint-37000/special_tokens_map.json\n",
            "{'loss': 1.7571, 'learning_rate': 1.570331077373331e-05, 'epoch': 3.43}\n",
            " 69% 37500/54670 [1:47:49<43:32,  6.57it/s][INFO|trainer.py:1995] 2021-12-08 05:57:09,854 >> Saving model checkpoint to models/cnn/checkpoint-37500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:57:09,863 >> Configuration saved in models/cnn/checkpoint-37500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:57:12,237 >> Model weights saved in models/cnn/checkpoint-37500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:57:12,243 >> tokenizer config file saved in models/cnn/checkpoint-37500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:57:12,249 >> Special tokens file saved in models/cnn/checkpoint-37500/special_tokens_map.json\n",
            "{'loss': 1.7057, 'learning_rate': 1.5246021584049754e-05, 'epoch': 3.48}\n",
            " 70% 38000/54670 [1:49:17<44:01,  6.31it/s][INFO|trainer.py:1995] 2021-12-08 05:58:37,986 >> Saving model checkpoint to models/cnn/checkpoint-38000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 05:58:37,993 >> Configuration saved in models/cnn/checkpoint-38000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 05:58:40,395 >> Model weights saved in models/cnn/checkpoint-38000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 05:58:40,853 >> tokenizer config file saved in models/cnn/checkpoint-38000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 05:58:40,866 >> Special tokens file saved in models/cnn/checkpoint-38000/special_tokens_map.json\n",
            "{'loss': 1.6978, 'learning_rate': 1.4788732394366198e-05, 'epoch': 3.52}\n",
            " 70% 38500/54670 [1:50:46<41:56,  6.43it/s][INFO|trainer.py:1995] 2021-12-08 06:00:06,328 >> Saving model checkpoint to models/cnn/checkpoint-38500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:00:06,335 >> Configuration saved in models/cnn/checkpoint-38500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:00:08,558 >> Model weights saved in models/cnn/checkpoint-38500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:00:08,566 >> tokenizer config file saved in models/cnn/checkpoint-38500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:00:08,572 >> Special tokens file saved in models/cnn/checkpoint-38500/special_tokens_map.json\n",
            "{'loss': 1.6833, 'learning_rate': 1.4331443204682643e-05, 'epoch': 3.57}\n",
            " 71% 39000/54670 [1:52:14<40:28,  6.45it/s][INFO|trainer.py:1995] 2021-12-08 06:01:34,315 >> Saving model checkpoint to models/cnn/checkpoint-39000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:01:34,322 >> Configuration saved in models/cnn/checkpoint-39000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:01:36,717 >> Model weights saved in models/cnn/checkpoint-39000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:01:36,724 >> tokenizer config file saved in models/cnn/checkpoint-39000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:01:36,730 >> Special tokens file saved in models/cnn/checkpoint-39000/special_tokens_map.json\n",
            "{'loss': 1.5802, 'learning_rate': 1.3874154014999086e-05, 'epoch': 3.61}\n",
            " 72% 39500/54670 [1:53:43<39:07,  6.46it/s][INFO|trainer.py:1995] 2021-12-08 06:03:03,658 >> Saving model checkpoint to models/cnn/checkpoint-39500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:03:03,681 >> Configuration saved in models/cnn/checkpoint-39500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:03:06,534 >> Model weights saved in models/cnn/checkpoint-39500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:03:06,547 >> tokenizer config file saved in models/cnn/checkpoint-39500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:03:06,554 >> Special tokens file saved in models/cnn/checkpoint-39500/special_tokens_map.json\n",
            "{'loss': 1.7363, 'learning_rate': 1.341686482531553e-05, 'epoch': 3.66}\n",
            " 73% 40000/54670 [1:55:13<37:03,  6.60it/s][INFO|trainer.py:1995] 2021-12-08 06:04:33,582 >> Saving model checkpoint to models/cnn/checkpoint-40000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:04:33,590 >> Configuration saved in models/cnn/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:04:35,985 >> Model weights saved in models/cnn/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:04:36,363 >> tokenizer config file saved in models/cnn/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:04:36,369 >> Special tokens file saved in models/cnn/checkpoint-40000/special_tokens_map.json\n",
            "{'loss': 1.671, 'learning_rate': 1.2959575635631974e-05, 'epoch': 3.7}\n",
            " 74% 40500/54670 [1:56:42<39:14,  6.02it/s][INFO|trainer.py:1995] 2021-12-08 06:06:02,604 >> Saving model checkpoint to models/cnn/checkpoint-40500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:06:02,613 >> Configuration saved in models/cnn/checkpoint-40500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:06:04,911 >> Model weights saved in models/cnn/checkpoint-40500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:06:04,919 >> tokenizer config file saved in models/cnn/checkpoint-40500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:06:04,924 >> Special tokens file saved in models/cnn/checkpoint-40500/special_tokens_map.json\n",
            "{'loss': 1.7575, 'learning_rate': 1.2502286445948419e-05, 'epoch': 3.75}\n",
            " 75% 41000/54670 [1:58:10<32:53,  6.93it/s][INFO|trainer.py:1995] 2021-12-08 06:07:30,628 >> Saving model checkpoint to models/cnn/checkpoint-41000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:07:30,635 >> Configuration saved in models/cnn/checkpoint-41000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:07:32,819 >> Model weights saved in models/cnn/checkpoint-41000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:07:32,826 >> tokenizer config file saved in models/cnn/checkpoint-41000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:07:32,832 >> Special tokens file saved in models/cnn/checkpoint-41000/special_tokens_map.json\n",
            "{'loss': 1.7706, 'learning_rate': 1.2044997256264862e-05, 'epoch': 3.8}\n",
            " 76% 41500/54670 [1:59:38<33:30,  6.55it/s][INFO|trainer.py:1995] 2021-12-08 06:08:59,264 >> Saving model checkpoint to models/cnn/checkpoint-41500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:08:59,272 >> Configuration saved in models/cnn/checkpoint-41500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:09:01,551 >> Model weights saved in models/cnn/checkpoint-41500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:09:01,558 >> tokenizer config file saved in models/cnn/checkpoint-41500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:09:01,564 >> Special tokens file saved in models/cnn/checkpoint-41500/special_tokens_map.json\n",
            "{'loss': 1.6745, 'learning_rate': 1.1587708066581307e-05, 'epoch': 3.84}\n",
            " 77% 42000/54670 [2:01:08<31:35,  6.68it/s][INFO|trainer.py:1995] 2021-12-08 06:10:28,443 >> Saving model checkpoint to models/cnn/checkpoint-42000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:10:28,450 >> Configuration saved in models/cnn/checkpoint-42000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:10:30,841 >> Model weights saved in models/cnn/checkpoint-42000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:10:30,852 >> tokenizer config file saved in models/cnn/checkpoint-42000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:10:30,858 >> Special tokens file saved in models/cnn/checkpoint-42000/special_tokens_map.json\n",
            "{'loss': 1.6446, 'learning_rate': 1.113041887689775e-05, 'epoch': 3.89}\n",
            " 78% 42500/54670 [2:02:34<29:10,  6.95it/s][INFO|trainer.py:1995] 2021-12-08 06:11:55,030 >> Saving model checkpoint to models/cnn/checkpoint-42500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:11:55,038 >> Configuration saved in models/cnn/checkpoint-42500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:11:57,499 >> Model weights saved in models/cnn/checkpoint-42500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:11:57,508 >> tokenizer config file saved in models/cnn/checkpoint-42500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:11:57,517 >> Special tokens file saved in models/cnn/checkpoint-42500/special_tokens_map.json\n",
            "{'loss': 1.6879, 'learning_rate': 1.0673129687214195e-05, 'epoch': 3.93}\n",
            " 79% 43000/54670 [2:04:02<30:10,  6.45it/s][INFO|trainer.py:1995] 2021-12-08 06:13:22,802 >> Saving model checkpoint to models/cnn/checkpoint-43000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:13:22,810 >> Configuration saved in models/cnn/checkpoint-43000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:13:25,267 >> Model weights saved in models/cnn/checkpoint-43000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:13:25,274 >> tokenizer config file saved in models/cnn/checkpoint-43000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:13:25,280 >> Special tokens file saved in models/cnn/checkpoint-43000/special_tokens_map.json\n",
            "{'loss': 1.6123, 'learning_rate': 1.0215840497530639e-05, 'epoch': 3.98}\n",
            " 80% 43500/54670 [2:05:30<29:04,  6.40it/s][INFO|trainer.py:1995] 2021-12-08 06:14:50,898 >> Saving model checkpoint to models/cnn/checkpoint-43500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:14:50,905 >> Configuration saved in models/cnn/checkpoint-43500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:14:53,236 >> Model weights saved in models/cnn/checkpoint-43500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:14:53,243 >> tokenizer config file saved in models/cnn/checkpoint-43500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:14:53,249 >> Special tokens file saved in models/cnn/checkpoint-43500/special_tokens_map.json\n",
            "{'loss': 1.657, 'learning_rate': 9.758551307847083e-06, 'epoch': 4.02}\n",
            " 80% 44000/54670 [2:06:57<28:35,  6.22it/s][INFO|trainer.py:1995] 2021-12-08 06:16:17,426 >> Saving model checkpoint to models/cnn/checkpoint-44000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:16:17,433 >> Configuration saved in models/cnn/checkpoint-44000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:16:19,721 >> Model weights saved in models/cnn/checkpoint-44000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:16:19,730 >> tokenizer config file saved in models/cnn/checkpoint-44000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:16:20,137 >> Special tokens file saved in models/cnn/checkpoint-44000/special_tokens_map.json\n",
            "{'loss': 1.6351, 'learning_rate': 9.301262118163527e-06, 'epoch': 4.07}\n",
            " 81% 44500/54670 [2:08:23<26:22,  6.43it/s][INFO|trainer.py:1995] 2021-12-08 06:17:43,922 >> Saving model checkpoint to models/cnn/checkpoint-44500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:17:43,928 >> Configuration saved in models/cnn/checkpoint-44500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:17:46,103 >> Model weights saved in models/cnn/checkpoint-44500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:17:46,110 >> tokenizer config file saved in models/cnn/checkpoint-44500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:17:46,115 >> Special tokens file saved in models/cnn/checkpoint-44500/special_tokens_map.json\n",
            "{'loss': 1.5343, 'learning_rate': 8.843972928479972e-06, 'epoch': 4.12}\n",
            " 82% 45000/54670 [2:09:49<24:44,  6.51it/s][INFO|trainer.py:1995] 2021-12-08 06:19:10,076 >> Saving model checkpoint to models/cnn/checkpoint-45000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:19:10,083 >> Configuration saved in models/cnn/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:19:12,345 >> Model weights saved in models/cnn/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:19:12,352 >> tokenizer config file saved in models/cnn/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:19:12,358 >> Special tokens file saved in models/cnn/checkpoint-45000/special_tokens_map.json\n",
            "{'loss': 1.656, 'learning_rate': 8.386683738796415e-06, 'epoch': 4.16}\n",
            " 83% 45500/54670 [2:11:16<22:45,  6.71it/s][INFO|trainer.py:1995] 2021-12-08 06:20:36,640 >> Saving model checkpoint to models/cnn/checkpoint-45500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:20:36,647 >> Configuration saved in models/cnn/checkpoint-45500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:20:39,151 >> Model weights saved in models/cnn/checkpoint-45500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:20:39,158 >> tokenizer config file saved in models/cnn/checkpoint-45500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:20:39,163 >> Special tokens file saved in models/cnn/checkpoint-45500/special_tokens_map.json\n",
            "{'loss': 1.6068, 'learning_rate': 7.92939454911286e-06, 'epoch': 4.21}\n",
            " 84% 46000/54670 [2:12:41<21:59,  6.57it/s][INFO|trainer.py:1995] 2021-12-08 06:22:02,141 >> Saving model checkpoint to models/cnn/checkpoint-46000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:22:02,164 >> Configuration saved in models/cnn/checkpoint-46000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:22:04,298 >> Model weights saved in models/cnn/checkpoint-46000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:22:04,305 >> tokenizer config file saved in models/cnn/checkpoint-46000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:22:04,310 >> Special tokens file saved in models/cnn/checkpoint-46000/special_tokens_map.json\n",
            "{'loss': 1.5967, 'learning_rate': 7.472105359429303e-06, 'epoch': 4.25}\n",
            " 85% 46500/54670 [2:14:07<20:19,  6.70it/s][INFO|trainer.py:1995] 2021-12-08 06:23:27,752 >> Saving model checkpoint to models/cnn/checkpoint-46500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:23:27,759 >> Configuration saved in models/cnn/checkpoint-46500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:23:29,989 >> Model weights saved in models/cnn/checkpoint-46500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:23:29,995 >> tokenizer config file saved in models/cnn/checkpoint-46500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:23:30,000 >> Special tokens file saved in models/cnn/checkpoint-46500/special_tokens_map.json\n",
            "{'loss': 1.5105, 'learning_rate': 7.014816169745747e-06, 'epoch': 4.3}\n",
            " 86% 47000/54670 [2:15:33<19:29,  6.56it/s][INFO|trainer.py:1995] 2021-12-08 06:24:54,074 >> Saving model checkpoint to models/cnn/checkpoint-47000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:24:54,082 >> Configuration saved in models/cnn/checkpoint-47000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:24:56,280 >> Model weights saved in models/cnn/checkpoint-47000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:24:56,287 >> tokenizer config file saved in models/cnn/checkpoint-47000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:24:56,293 >> Special tokens file saved in models/cnn/checkpoint-47000/special_tokens_map.json\n",
            "{'loss': 1.5223, 'learning_rate': 6.557526980062191e-06, 'epoch': 4.34}\n",
            " 87% 47500/54670 [2:17:02<17:15,  6.93it/s][INFO|trainer.py:1995] 2021-12-08 06:26:22,381 >> Saving model checkpoint to models/cnn/checkpoint-47500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:26:22,390 >> Configuration saved in models/cnn/checkpoint-47500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:26:24,844 >> Model weights saved in models/cnn/checkpoint-47500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:26:24,851 >> tokenizer config file saved in models/cnn/checkpoint-47500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:26:24,857 >> Special tokens file saved in models/cnn/checkpoint-47500/special_tokens_map.json\n",
            "{'loss': 1.4468, 'learning_rate': 6.100237790378636e-06, 'epoch': 4.39}\n",
            " 88% 48000/54670 [2:18:28<16:06,  6.90it/s][INFO|trainer.py:1995] 2021-12-08 06:27:49,230 >> Saving model checkpoint to models/cnn/checkpoint-48000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:27:49,239 >> Configuration saved in models/cnn/checkpoint-48000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:27:51,624 >> Model weights saved in models/cnn/checkpoint-48000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:27:51,962 >> tokenizer config file saved in models/cnn/checkpoint-48000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:27:51,967 >> Special tokens file saved in models/cnn/checkpoint-48000/special_tokens_map.json\n",
            "{'loss': 1.5498, 'learning_rate': 5.642948600695079e-06, 'epoch': 4.44}\n",
            " 89% 48500/54670 [2:19:55<15:27,  6.65it/s][INFO|trainer.py:1995] 2021-12-08 06:29:15,733 >> Saving model checkpoint to models/cnn/checkpoint-48500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:29:15,741 >> Configuration saved in models/cnn/checkpoint-48500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:29:17,962 >> Model weights saved in models/cnn/checkpoint-48500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:29:17,968 >> tokenizer config file saved in models/cnn/checkpoint-48500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:29:17,974 >> Special tokens file saved in models/cnn/checkpoint-48500/special_tokens_map.json\n",
            "{'loss': 1.6812, 'learning_rate': 5.1856594110115235e-06, 'epoch': 4.48}\n",
            " 90% 49000/54670 [2:21:22<14:45,  6.40it/s][INFO|trainer.py:1995] 2021-12-08 06:30:42,270 >> Saving model checkpoint to models/cnn/checkpoint-49000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:30:42,277 >> Configuration saved in models/cnn/checkpoint-49000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:30:44,541 >> Model weights saved in models/cnn/checkpoint-49000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:30:44,548 >> tokenizer config file saved in models/cnn/checkpoint-49000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:30:44,554 >> Special tokens file saved in models/cnn/checkpoint-49000/special_tokens_map.json\n",
            "{'loss': 1.5556, 'learning_rate': 4.7283702213279675e-06, 'epoch': 4.53}\n",
            " 91% 49500/54670 [2:22:48<12:43,  6.77it/s][INFO|trainer.py:1995] 2021-12-08 06:32:09,235 >> Saving model checkpoint to models/cnn/checkpoint-49500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:32:09,243 >> Configuration saved in models/cnn/checkpoint-49500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:32:11,522 >> Model weights saved in models/cnn/checkpoint-49500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:32:11,529 >> tokenizer config file saved in models/cnn/checkpoint-49500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:32:11,535 >> Special tokens file saved in models/cnn/checkpoint-49500/special_tokens_map.json\n",
            "{'loss': 1.5215, 'learning_rate': 4.271081031644412e-06, 'epoch': 4.57}\n",
            " 91% 50000/54670 [2:24:16<11:45,  6.62it/s][INFO|trainer.py:1995] 2021-12-08 06:33:36,462 >> Saving model checkpoint to models/cnn/checkpoint-50000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:33:36,469 >> Configuration saved in models/cnn/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:33:39,161 >> Model weights saved in models/cnn/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:33:39,175 >> tokenizer config file saved in models/cnn/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:33:39,180 >> Special tokens file saved in models/cnn/checkpoint-50000/special_tokens_map.json\n",
            "{'loss': 1.6285, 'learning_rate': 3.8137918419608566e-06, 'epoch': 4.62}\n",
            " 92% 50500/54670 [2:25:43<10:57,  6.35it/s][INFO|trainer.py:1995] 2021-12-08 06:35:03,406 >> Saving model checkpoint to models/cnn/checkpoint-50500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:35:03,427 >> Configuration saved in models/cnn/checkpoint-50500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:35:06,249 >> Model weights saved in models/cnn/checkpoint-50500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:35:06,256 >> tokenizer config file saved in models/cnn/checkpoint-50500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:35:06,266 >> Special tokens file saved in models/cnn/checkpoint-50500/special_tokens_map.json\n",
            "{'loss': 1.477, 'learning_rate': 3.3565026522773006e-06, 'epoch': 4.66}\n",
            " 93% 51000/54670 [2:27:10<09:35,  6.38it/s][INFO|trainer.py:1995] 2021-12-08 06:36:30,603 >> Saving model checkpoint to models/cnn/checkpoint-51000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:36:30,613 >> Configuration saved in models/cnn/checkpoint-51000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:36:32,876 >> Model weights saved in models/cnn/checkpoint-51000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:36:32,883 >> tokenizer config file saved in models/cnn/checkpoint-51000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:36:32,889 >> Special tokens file saved in models/cnn/checkpoint-51000/special_tokens_map.json\n",
            "{'loss': 1.5986, 'learning_rate': 2.8992134625937443e-06, 'epoch': 4.71}\n",
            " 94% 51500/54670 [2:28:38<09:07,  5.79it/s][INFO|trainer.py:1995] 2021-12-08 06:37:59,104 >> Saving model checkpoint to models/cnn/checkpoint-51500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:37:59,111 >> Configuration saved in models/cnn/checkpoint-51500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:38:01,437 >> Model weights saved in models/cnn/checkpoint-51500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:38:01,445 >> tokenizer config file saved in models/cnn/checkpoint-51500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:38:01,454 >> Special tokens file saved in models/cnn/checkpoint-51500/special_tokens_map.json\n",
            "{'loss': 1.5628, 'learning_rate': 2.4419242729101884e-06, 'epoch': 4.76}\n",
            " 95% 52000/54670 [2:30:05<06:28,  6.87it/s][INFO|trainer.py:1995] 2021-12-08 06:39:26,117 >> Saving model checkpoint to models/cnn/checkpoint-52000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:39:26,123 >> Configuration saved in models/cnn/checkpoint-52000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:39:28,570 >> Model weights saved in models/cnn/checkpoint-52000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:39:28,578 >> tokenizer config file saved in models/cnn/checkpoint-52000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:39:28,583 >> Special tokens file saved in models/cnn/checkpoint-52000/special_tokens_map.json\n",
            "{'loss': 1.577, 'learning_rate': 1.9846350832266325e-06, 'epoch': 4.8}\n",
            " 96% 52500/54670 [2:31:32<05:36,  6.44it/s][INFO|trainer.py:1995] 2021-12-08 06:40:53,166 >> Saving model checkpoint to models/cnn/checkpoint-52500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:40:53,172 >> Configuration saved in models/cnn/checkpoint-52500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:40:55,523 >> Model weights saved in models/cnn/checkpoint-52500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:40:55,530 >> tokenizer config file saved in models/cnn/checkpoint-52500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:40:55,536 >> Special tokens file saved in models/cnn/checkpoint-52500/special_tokens_map.json\n",
            "{'loss': 1.483, 'learning_rate': 1.5273458935430768e-06, 'epoch': 4.85}\n",
            " 97% 53000/54670 [2:32:59<04:33,  6.11it/s][INFO|trainer.py:1995] 2021-12-08 06:42:19,895 >> Saving model checkpoint to models/cnn/checkpoint-53000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:42:19,902 >> Configuration saved in models/cnn/checkpoint-53000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:42:22,231 >> Model weights saved in models/cnn/checkpoint-53000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:42:22,238 >> tokenizer config file saved in models/cnn/checkpoint-53000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:42:22,243 >> Special tokens file saved in models/cnn/checkpoint-53000/special_tokens_map.json\n",
            "{'loss': 1.5529, 'learning_rate': 1.0700567038595209e-06, 'epoch': 4.89}\n",
            " 98% 53500/54670 [2:34:28<02:58,  6.54it/s][INFO|trainer.py:1995] 2021-12-08 06:43:49,071 >> Saving model checkpoint to models/cnn/checkpoint-53500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:43:49,079 >> Configuration saved in models/cnn/checkpoint-53500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:43:51,443 >> Model weights saved in models/cnn/checkpoint-53500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:43:51,450 >> tokenizer config file saved in models/cnn/checkpoint-53500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:43:51,461 >> Special tokens file saved in models/cnn/checkpoint-53500/special_tokens_map.json\n",
            "{'loss': 1.5361, 'learning_rate': 6.127675141759649e-07, 'epoch': 4.94}\n",
            " 99% 54000/54670 [2:35:56<01:43,  6.46it/s][INFO|trainer.py:1995] 2021-12-08 06:45:16,362 >> Saving model checkpoint to models/cnn/checkpoint-54000\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:45:16,369 >> Configuration saved in models/cnn/checkpoint-54000/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:45:18,726 >> Model weights saved in models/cnn/checkpoint-54000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:45:19,070 >> tokenizer config file saved in models/cnn/checkpoint-54000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:45:19,075 >> Special tokens file saved in models/cnn/checkpoint-54000/special_tokens_map.json\n",
            "{'loss': 1.5864, 'learning_rate': 1.5547832449240902e-07, 'epoch': 4.98}\n",
            "100% 54500/54670 [2:37:24<00:26,  6.52it/s][INFO|trainer.py:1995] 2021-12-08 06:46:44,474 >> Saving model checkpoint to models/cnn/checkpoint-54500\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:46:44,482 >> Configuration saved in models/cnn/checkpoint-54500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:46:46,746 >> Model weights saved in models/cnn/checkpoint-54500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:46:46,753 >> tokenizer config file saved in models/cnn/checkpoint-54500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:46:46,758 >> Special tokens file saved in models/cnn/checkpoint-54500/special_tokens_map.json\n",
            "100% 54670/54670 [2:38:01<00:00,  6.79it/s][INFO|trainer.py:1409] 2021-12-08 06:47:21,268 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 9481.0548, 'train_samples_per_second': 11.532, 'train_steps_per_second': 5.766, 'train_loss': 1.9002910775198063, 'epoch': 5.0}\n",
            "100% 54670/54670 [2:38:01<00:00,  5.77it/s]\n",
            "[INFO|trainer.py:1995] 2021-12-08 06:47:21,301 >> Saving model checkpoint to models/cnn\n",
            "[INFO|configuration_utils.py:417] 2021-12-08 06:47:21,308 >> Configuration saved in models/cnn/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-08 06:47:23,632 >> Model weights saved in models/cnn/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-08 06:47:23,640 >> tokenizer config file saved in models/cnn/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-08 06:47:23,645 >> Special tokens file saved in models/cnn/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     1.9003\n",
            "  train_runtime            = 2:38:01.05\n",
            "  train_samples            =      21868\n",
            "  train_samples_per_second =     11.532\n",
            "  train_steps_per_second   =      5.766\n",
            "12/08/2021 06:47:23 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:541] 2021-12-08 06:47:23,734 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2243] 2021-12-08 06:47:23,760 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-08 06:47:23,760 >>   Num examples = 6249\n",
            "[INFO|trainer.py:2248] 2021-12-08 06:47:23,760 >>   Batch size = 2\n",
            "100% 3125/3125 [01:23<00:00, 37.28it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_loss               =     1.7695\n",
            "  eval_runtime            = 0:01:23.80\n",
            "  eval_samples            =       6249\n",
            "  eval_samples_per_second =     74.563\n",
            "  eval_steps_per_second   =     37.288\n",
            "  perplexity              =      5.868\n",
            "[INFO|modelcard.py:449] 2021-12-08 06:48:47,883 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H45ZL2j68RYL"
      },
      "source": [
        "# Elon Musk\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEZ1QFtH8Wmk",
        "outputId": "f197df9c-7063-48ea-d753-f45b0adaadcf"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/elonmusk\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --repetition_penalty 2.0\\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/01/2021 21:57:41 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/elonmusk', model_type='bert', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=2.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "u you they, want if only Only - - - - + plus minus o : & and And For In All all of Of Now Todayday Tonight Day day today eve afternoon evening morning night on O\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "ays Alwaysless there there There 'ES s em Em out it a an ah uh er e EEOAAAICCSUSUSUUILADASADSDWDXL L S F D TT\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "nd & And … Ice ice by By Had the Thetheftfted to To Tendslaslasts s,, ( 1 1 )e mMpmamamamiamnam\n",
            "=== GENERATED SEQUENCE 4 ===\n",
            " O S R T M F L V G CC W P N & e t g i o m r f u U u en En At at art Art Arts Science Sciences So to would but it there an\n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "ally real true and is was ever a any by ByfofofwwwwewsunseeseeAOOSOCCACacUSususkEEALADANAHASARATACABA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWhlxN_28W4B",
        "outputId": "d9759322-c18a-4161-9bae-51d698c91259"
      },
      "source": [
        "!python run_metrics.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/elonmusk\" \\\n",
        "  --test_data_file \"data/elonmusk/test.txt\" \\\n",
        "  --length 50 \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_ref 50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/01/2021 22:00:01 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/elonmusk', model_type='bert', n_gpu=1, no_cuda=False, num_ref=50, output_dir='outputs', p=0.9, repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, test_data_file='data/elonmusk/test.txt')\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['SnowedededMMMMMMMMMM # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # ', 'Ok Ok Ok Ok Ok Ok Ok Ok Ok Ok Ok Ok Ok Ok Ok < “ < < < < < < < < < < < > > > > > > > > > > > > > > > > > > > > ', 'Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate Climate ( ) ) )!!!!', 'Paididididididididididididididididididid p p p p p p p p p c c c c c c c c c c c c c c c c c c c ', 'That That...................................,..........', 'Simulations & & → → → → → → → & → → → → ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ', 'Don ’ t Baby Baby Baby Baby Baby is is a s s s s s s s s s s s s s s s s s s s s s s > > > > > > > > > > > > > > > ', 'Teslaslaslaslaslaslaslaslaslaslaon V … s s s s S S s S s s s s S S S S S S S S S S S S S S S S S S S S S S ', 'New New New New New New Newiam “ > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > ', 'Attempting................................,...........', 'Painting Beautiful Beautiful Beautiful Beautiful Beautiful Beautiful Beautiful Beautiful Beautiful Beautiful Beautiful M S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S ', '78 % % % %vvavvv u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u ', 'Not Not Not Not Not Just Just Just Just just just. m m m m m m m m m m m m m m m en en en en en en en en en en en en en en en en en en en en en e', 'Just Just Just Just Just Just Just Just Just Just Just Just Just Just Just Just Just + < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > ', 'Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falconock Me MewwwnNANANANANANANANANANANANANANANANANANANANANANANANANANAN', 'This This is is is > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > ', 'Star Star Star Star Star Star Star Star Star Star Star Star Star Star Star Star > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > ', \"Don ’ t Just'''''''''''''''''''''''''''' s s s sOS < > < > > > > > > > > \", 'Got Got Got Got Got Gottt online online online : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : to to to o o o ', 'Flight Flight Flight Flight Flight Flight Flight Flight Flight Flight Flight Flight Flight Flightiamiamiamiamannannannannannannannannannannannannannannannannannannannannannannannannannannannannannannan', 'Moving Moving..............................................', '… < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < ', 'Model Model Model Model Model Model Model Model Model Model Model Model Model Model Model Model..................,, evidently but but and and but but but for...', 'Activistss & & & with with @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ ', 'Sleevingving s s s s s s en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en en e', 'Expectpectinging Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr Dr D', 'Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Falcon Levy,, but but then,, then then,,, thought in,,,,,,,, but but,,, almost but,,', 'CybertruckckAAAAAAAAAAAAAAAAA s is is the the the the the the the or or or a a a a a a and s s s s s s ', 'Aspirationallyallyally … 2018 2018 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2018 > > > > > > > > > > > > > > > > > > ', 'Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon [ [ ] ] ] ] ] > > > > > > ', 'Good Good Good Good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good good goo', 'Glad Glad - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Last Last Last Last Last Last Last Last Last < < < < < < < < < < < < < < < < < < < < < < < < < < < < > > > > > > > > > > > ', 'Extreme Extreme Extreme Extreme Extreme Extreme Heavy Heavy Heavy Heavy Heavy Heavy Heavy … w s re re re re re re re re re en en en en en en en en en en en en en en en en en en en en en en en e', 'Deususususususus De De De De De De De De De De De De De ) ) ) ) ) ) : : : : : : : : : : : : : : : : : : : : : ', 'Holding Holding a a a a but u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u u ', 'Just Just Just Just Just Just Just Just Just Just Just Just Just Just Just Just Just + + + > > > > > > > > > > > > > > > > > > > > > > > > > > > > ', 'Model Model Model Model Model Model Model Model Model Model Model Model Model Model Model Model!ra s s s s s s s t t an e e e e e e e e e e e e e e e e e e e e ', 'TeslaslaslaslaslaslaslaslaslaslaM … was was what what what what what what it it and and and and and and and and and and and and and and who who who it what the the the the and and an', 'Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon Dragon ( ( I I I ) ) ) ) ) ) ( (', 'SocialismismismismismismismismismismCBBBO is this where where where where where where where where where where and but them but sure sure sure sure a is but but but where where so it then and up u', 'Not Not Not Not Not Just Just Just Just Just Just Just just < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < ', 'Saw I I I R S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S ', 'Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy Happy. to to to to to to be be but but but though though though too though though though still also still still in in after and and and of of o', 'Three Three Three ThreeKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK double double double double double double double double double double double double double doubl', 'The....................,,, as as as,,,,,, or or,,,,,,,,,,,,,', 'Mission Mission Mission Mission Mission Mission Mission phone phone phone phone phone phone phone phone phone phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phones phone', 'Told............................,.........,........', 'Wish Wish Wish Wish Wish Wish Wish Wish Wish Wish Wish Wish Wish. B B B B E E S S S S S S S S S S S S S S S S S S S S S S S S S S S S ']\n",
            "['Snowed in at Mt. Rushmore @GoParks', 'Ok u can have a little emerald', 'Climate change deniers claim \"scientists disagree\", same rebuttal used by tobacco industry about lung cancer for decades', 'Paid respects to Masada earlier today. Live free or die.', 'That was a total non sequitur btw', 'Simulations show launch ok with bad valve. Still, better to stop & fix. Recalling rockets after launch is not an option.', 'Don’t Doubt ur Vibe m.', 'Tesla should have a mega rave cave under the Berlin Gigafatory', 'New Tesla referral program just released', 'Attempting bring up of thruster pods 2 and 4', 'Painting the name on the droneship ...', '78% of people want to gentrify Mordor', 'Not having a big gas guzzler engine in front means there’s room for trunks in front & back, plus longer crumple zone for best safety of any midsize car It’s crazy how much stuff fits in a Model 3 twitter.comAlguireTimsta…', 'Just wanted to say thanks to everyone for testifying yesterday in Austin. Will hang around after press conf to thank people in person.', 'Falcon 9 lifting off with 1.3 mmmillion pounds of force', 'This is nothing. In a few years, that bot will move so fast you’ll need a strobe light to see it. Sweet dreams… we dead', 'Star Light, Star Bright', 'Don’t defy DeFi', 'Got to regulate AIrobotics like we do food, drugs, aircraft & cars. Public risks require public oversight. Getting rid of the FAA wdn’t make flying safer. They’re there for good reason.', 'Flight computer aborted rocket hold down firing. Anomaly addressed. Cycling systems to countdown', \"Moving at ~23 times speed of sound, circling Earth every ~90 minutes View of an orbital sunset from Dragon's cupola\", '… going to moon very soon', 'Model S options are out! Performance in red and black for me. I will deliver my car in JuneJuly.', 'Activists should be pushing for more moderates to advise President, not fewer. How could having only extremists advise him possibly be good?', 'Sleeving SN2 dome in the high bay', 'Expect to reach preliminary conclusions regarding last flight by end of week. Will brief key customers & FAA, then post on our website.', 'Happy 4th of July!!', 'Falcon Heavy side boosters can use most of the same airframe as Falcon 9, but center core needs to be buffed up a lot for transfer loads.', 'Cybertruck prototype in New York this weekend', 'Aspirationally acerbic alliteration -> Bullshit baffles brains', 'Dragon will travel from south of New Zealand to Florida in ~37 minutes', 'Good article on the interplanetary transport system on Gizmodo', 'Glad you’re ok @tylerthecreator', 'Last known state for rocket boost stage is 360 ms, Mach 1.1, 8.5 km altitude and roll rate close to zero (v important!)', 'Extreme wind shear over Cape Canaveral. Feels like a sledgehammer when supersonic in the vertical. Hoping it changes …', 'Deus ex machine learning', 'Holding on solar array deployment until at least two thruster pods are active', 'Just fired our Superdraco escape rocket engine at full thrust! Needed to carry astronauts on Dragon', 'Model S Signature series sold out as of today', 'Tesla Solar just relaunched. Lmk what you think …', 'Dragon 2 is designed to be able to land anywhere in the solar system. Red Dragon Mars mission is the first test flight.', 'Socialism vs capitalism is not even the right question. What really matters is avoiding monopolies that restrict people’s freedom.', 'Not that this really matters. All current rocket tech, including ours, sucks. Only when it becomes fully reusable, will it not suck.', \"Saw The Dictator yesterday. Reminded me of Putin's reelection. Seems like only yesterday since he was last in power.\", 'Happy Thanksgiving!', 'Three Raptors on a Starship', 'The Model S beta endurance car just passed 150,000 miles on a single battery pack!', 'Mission looks good. Started deploying the 10 Iridium satellites. Rocket is stable on the droneship.', 'Told you AI was dangerous!! Massive semen explosion after blaze hits bull artificial insemination facility, firefighters forced to dodge \"projectiles\"', 'Wish there was something we could do to help the people of Syria.']\n",
            "12/01/2021 22:01:01 - INFO - __main__ -   ***** Metrics results *****\n",
            "12/01/2021 22:01:01 - INFO - __main__ -       bertscore_f1 = 0.7411973476409912\n",
            "12/01/2021 22:01:01 - INFO - __main__ -       bertscore_precision = 0.6840635538101196\n",
            "12/01/2021 22:01:01 - INFO - __main__ -       bertscore_recall = 0.8107326626777649\n",
            "12/01/2021 22:01:01 - INFO - __main__ -       bleu_3_score = 0.32267187561727184\n",
            "12/01/2021 22:01:01 - INFO - __main__ -       bleu_4_score = 0.4043182605545452\n",
            "12/01/2021 22:01:01 - INFO - __main__ -       bleu_5_score = 0.46482274651745636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5XSWPed7uMo"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Karpathy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjRiRd4H73MG",
        "outputId": "f6d8c3cc-ce72-447c-e0ee-e0ebfd97cd74"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/karpathy\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --repetition_penalty 2.0\\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/01/2021 21:39:05 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/karpathy', model_type='bert', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=2.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "[ ] } } } ) ) ) ) (': p a - - o O & and And + minus = to ssstatcalcalmalmsmtdvw w W \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "!!!!!!!!!!!!!!licit ( 2 2 : ) 1 phlvf f lo Loppspofifef\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            " al ilt c c b gggogugsggbongngnadagagginggigiagorgorgongegygicalgicddnghongogeolooloosooseoso\n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "> >? - + + + C C C,s * ~ about bit bits parts of in code programming with with using application from from at a some end End ; | { } \\ now there Ther\n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "@ @ : ( ) ) ) ) > < < < < < < < p awmmms -g gGddnLDdldsdosabbbedbilbsgsdadevevv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOrzlbH34-z_",
        "outputId": "8db478d6-1fff-4ec6-ee0b-124c93da4027"
      },
      "source": [
        "!python run_metrics.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/karpathy\" \\\n",
        "  --test_data_file \"data/karpathy/test.txt\" \\\n",
        "  --length 50 \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_ref 50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/01/2021 21:33:27 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/karpathy', model_type='bert', n_gpu=1, no_cuda=False, num_ref=50, output_dir='outputs', p=0.9, repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, test_data_file='data/karpathy/test.txt')\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "Downloading: 100% 482/482 [00:00<00:00, 535kB/s]\n",
            "Downloading: 100% 878k/878k [00:00<00:00, 5.28MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 3.21MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 7.49MB/s]\n",
            "Downloading: 100% 1.33G/1.33G [00:39<00:00, 36.0MB/s]\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up Up upup count count count count - - - - - - - - - - - ', 'RE : : : : : : : : : : : : p p p p p p p....... and another another another then then then then then another another another then then again again again again again again agai', 'I I I I I Imomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomom', 'A A A A A - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Yummymmy... - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Recurrentcurrentcurrent.... ( ( ( ) ) ) ) ) ) ) ( ( ( ( ) ) ) ) ) ) ) ) ) ) ) ( ( ( ( ) ) ) ) ) ) ) ) ) ) ) ', 'My My My My My My My My My My My My............!!!!!!!!!!!!!!!!!!! no no no no no n', '3D 3D 3D 3D 3D 3D 3D 3D 3D 3D,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,', 'Speech2Face : : : : : : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'I I I I I Imomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomom', 'Congratsts : : : : : bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat bat ba', 'Awesomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomed c c c c...... again again again once once again again agai', 'Working Working with with with with some some some some and but but it it it it its its its :... it it it it not may may may may may may may may might might all all all all except except except except except excep', 'Computer.... - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Kickstarterterterterterterterterterterterterterterterter But...,,, so so so ever ever ever ever ever ever ever ever ever except except but that that but but but but but but bu', 'A A A A A : holds : : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'A A A A A :,,,,,, then then,, follows another another another another another another to to be be... and.... but but another another another another another another another another another another another anothe', 'I I I I I Imomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomom', 'Google Google Google Google Google Google Google Google Google Google Google Google Google Google a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,', 'Wingsuituituituituituituit - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Haha : : music music... and and and and so so so so so and and and and and and and but but but but so so and and and and and and and and and and and and and and and and but bu', 'Research of of the the the each,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, uniform,,,, uniform uniform,', '\" The The ( ( ( ) ) ) ) ) ) ) ( ( ( ( ) ) ) ) ) ( ( ( ( ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ( ( ( ( ) ) ) ', 'is in int all all all all all except except except except except except except except except except except except except except except except except except except except except except except except which for except except except, except and and ( ( ( ( all except excep', 'By ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ a - - - - - - - - - - - - - - - - - - - - ', 'Ideallyllyllyllyllylly pnanananananananananananananananaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaonaon', 'I I I I I Imomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomom', 'A A A A A : that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that that tha', 'Sitting Sitting : : : : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'This This is is the the and ( ( ( ( ( ) ) ) ( ( ( all all except except except except except except except except except except except except also all. ) ) ) ) ) ) ) ) ) ) ) ) ) ', 'Visitedted co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co co c', 'Wednesday Wednesday : : : : : : : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Programming Programming Programming Programming Programming Programming programming ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ( ( : : : : : ) ) ) ) ) ) ) ) ) ', 'ICLRRRRRRRRRRRRRRRRRRRRRRRRRR.......,! no no no no no no no no no no no n', 'Does Does Does Does Does summary summary summary summary summary summary with with from : ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; } } ; { { } } } } } } } } { { { { { ', 'Wow : : : : a,,, all all all all but but but but but I I think think it it was was all all all all all but but but but but but but but all all all all but but but but but bu', \"I'm m m @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @.... it and and and and and and and and and and and and and and and and and and and an\", 'Hmm.. a that that that one one one one one one all all one one one one all and but but but but but but but again there there there there there there there there all all all all all but but but all all all al', 'yayyyyyy p p p p p p p p p p p p p p p p p p P p p p p p p p p p p p p p p p p p p p p p p ', 'My My My My My My My My My My My My record1.........!!!!!!!!!!!!!!!!!! - - - - - - - ', 'Former Ex Ex Ex - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'My My My My My My My My My My - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why why why why why why wh', 'hah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,', 'I I......,, again again again again again again again again again again again and and and again again again again and and and again again again and and and and again again again again again again again again again again agai', 'I I..... - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'As As As As As As As so so so so so as so so so as and or or or either either either either one one one even even even even even even even even even even even even even even even even even even even even eve', 'My My My My My My My My My My - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'iOS77777777777777777777777777777 p p p p p p p p P P P P P P P P P P ', 'A A A A A : :,,, and,,,,,,,,,,,,,, uniform,,,,,,, uniform uniform,,,,,,,,,,, or,']\n",
            "['Up next in Nature: Dendrites not passive compute units! Ion channels implicated as basic computational units. 1', 'RE: Boston Dynamics Spot video & people\\'s response to seeing it get kicked. I wonder if \"robot cruelty\" will be a thing, or \"robot rights\"', \"I registered for Google IO and was happy and then just noticed it overlaps Perfectly with this year's CVPR. Not cool\", \"A tweet! Just because I didn't for a while :) leaving to Vancouver in 2 hours. A slice of pizza is in order\", 'Yummy data! Microsoft Academic Search was awesome API resource until it was taken down a few months ago Microsoft releases data for Academic Graph, 37GB of \"paper-paper citations, author-paper, paper-topic and so forth\"', \"Recurrent Model of Visual Attention in Torch Doesn't quite convey how simple REINFORCE update is, but nice post torch.ch Torch | Recurrent Model of Visual Attention Torch is a scientific computing framework for LuaJIT.\", 'My by-a-margin favorite educational YouTuber ( @cgpgrey ) is starting a new podcast \"Hello Internet\" subbed', '3D printing on nanoscales. 3D printing is becoming cooler every day. 1', 'Speech2Face: Learning the Face Behind a Voice With increasingly largeeffective library of neural net encoders of any X and decoders of any Y, any source of paired data X,Y can give X2Y nets. And opens the door to many X2Y2Z2W...2X', 'I wish tax forms and related legal documents were written in English. I had to stumble on a 2-sentence Yahoo answers post to get this one', 'Congrats to good friend @RichardSocher and his new and exciting Deep Learning startup! a company to watch closely.', 'Awesome Google Research blog post on scaling up robotics ai.', 'Working hours: Get a Life. Research on relationship between hours worked & productivity [Economist] 1', 'Computer vision in the browser: Real time semi dense point tracking in javascript', 'Kickstarter has the most interesting \"About us\" page I\\'ve seen so far (drag horizontally)', 'A terrible demo of using Voice Input to control new Xbox Voice recognition sucks as input, IMO. Companies, pls stop', 'A bit late to the party here, but \"State of AI Report 2019\" is a nice overall ambitious attempt at summarizing AI for \"research\" a bit too much RL and a bit too little vision. Interesting that vision is patented so much more than other areas (p85) stateof.ai State of AI Report 2021 The State of AI Report analyses the most interesting developments in AI. Read and download here.', 'I accidentally sipped remains of my coffee from yesterday instead of coffee from today. It was unpleasant. 2', 'Google wants to cover the Earth with balloons that beam internet to Earth. This is the Google I like', 'Wingsuit videos are immune to getting old 1', 'Haha: \"50 People On ‘The Most Intellectual Joke I Know\\'\" (via @thenilly )', 'Research on solving Bongard problems: somehow these seem closely related to some core aspect of intelligence', '\"The Future of Online Identity is Decentralized\" web identity is in a very bad place. Current good+easy blend is to not use \"Sign in with...\" but a (paid) password manager. But some dedicated service has to \"factor it out\". +HN', 'is the chatter of the day. \"Is connectivity a human right?\" +NYT article', \"By the beard of Zeus! Anchorman 2 was announced today! I don't know how to put this but this is kind of a big deal.\", 'Ideally never absorb information without predicting it first. Then you can update both 1) your knowledge but also 2) your generative model.', 'I updated the yesterday. Now with even more conferences for even more headaches! Feedback welcome', 'A critique of pure learning and what ANNs can learn from animal brains \"a large component of an animal’s behavioral repertoire is not the result of clever learning algorithms—supervised or unsupervised—but of behavior programs already present at birth.\"', 'Sitting in bookstore in moment of nostalgia reading one of my favorites: Mechanics by Landau&Lifshitz. I remember taking this 5 years ago :(', 'This is from 2 days ago, but Curiosity has now been on Mars for 1 year. Some reflectionscelebration from NASA:', 'Visited Apple for the first time. Spotted Jony at lunch. #visitsuccess Now I know which Apple Watch band he uses and must get the same', 'Wednesday is my favorite day at work- 8 hour block of uninterrupted hacking time and no other scheduled bs.', 'Programming Languages homework finished. Lambda calculus and many many many brackets... #exhausted 1', 'ICLR papers are now up . This year have to use CMT to seesubmit comments', 'Does Passing A Small Current Through Your Brain Really Make You Smarter? hmmm... curious. skeptical.', 'Wow: Steve Ballmer (Microsoft CEO) to resign within a year. Stock up 7% right after announcement', \"I'm a bit of a noob at photography, but I've always enjoyed the subject and dabbled with it a bit in...\", 'Hmm. I became followed by a person with 0 followers, 0 tweets and 1 followee (me). Stalker!!! Lol', 'yay has reached 1,000 registered users! Growing few dozen a day.', \"My friend is fast enough at Rubik's cube to fit into a vine :) vine.co Andrej Karpathy's post on Vine My friend is fast enough at Rubik's cube to fit into a vine :) 2\", \"Former Google Reader Product Manager on why Google Reader was shut down Spoiler: yes, it's Google+. Ohhhhhhasfdasgfd\", 'My dad emailed me a link to a funny video of a cat. I think he just discovered YouTube. 1', 'Why Twitter is obsolete | Social Media (via @bradacker22 ) -- some good points. agree i think', 'hah, OH from friend: \"papers are often written in a way to hide embarrassingsloppy details and the fact that the ideas are very simple\"', 'I found a code base that goes against everything I believe in and stand for as a person. It pains to think that some CPU had to execute that', \"I reduced the price of one of my iOS apps from $1 to free, went from 200 downloadsmonth to 3000. Seems like people really can't afford a $1\", 'As far as I can tell my convnet predicts \"Erotica\" for woman\\'s face sideways. Man face doesn\\'t do it. Having too much fun with this', 'My top Diablo 3 heroes are called Sigmoid, ReLU and tanh. Now creating Maxout crusader. Maybe one day someone will get it. 5', \"iOS7 has early support for multipath TCP spec. (use of several conns at same time) This tech can't come soon enough\", 'A good essay from Aaron Swartz on productivity. I can relate a lot to what he says and need to improve here.']\n",
            "12/01/2021 21:35:00 - INFO - __main__ -   ***** Metrics results *****\n",
            "12/01/2021 21:35:00 - INFO - __main__ -       bertscore_f1 = 0.7387463450431824\n",
            "12/01/2021 21:35:00 - INFO - __main__ -       bertscore_precision = 0.6866886615753174\n",
            "12/01/2021 21:35:00 - INFO - __main__ -       bertscore_recall = 0.8008530139923096\n",
            "12/01/2021 21:35:00 - INFO - __main__ -       bleu_3_score = 0.3123650347611331\n",
            "12/01/2021 21:35:00 - INFO - __main__ -       bleu_4_score = 0.38507444501560734\n",
            "12/01/2021 21:35:00 - INFO - __main__ -       bleu_5_score = 0.4381762660554336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YahPVb4e8FDs"
      },
      "source": [
        "# Obama\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQQo6Z6EpKSr",
        "outputId": "ac2b7096-8bb5-4e7b-a888-86bee8e743f1"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/Obama\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --repetition_penalty 2.0\\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/01/2021 21:14:30 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/Obama', model_type='bert', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=2.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            " how very little small by orlma not toto do doing did could should need have had been was done accomplished with with of it that that much much many during ’'s s work working together Together now Now Mor\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            " Obama )s — – ” “ ‘'’ ”tt an ”'\" … … ” ” ” ” \"so Imagine me Me as As So I I am m headingraderade @ called Joe Joe442\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "the of question or Or answer Answer answered answering without with much little Little surprise surprises, — ‘ ” \" “ – ’'' it is was always never ever Ever told asked America Americans in In That You We And Go g\n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "sincety Week week Saturday weekend Sunday : : to by be name - - - - - - - - - - - up but,! again Again ) ) ) _ | — — – ” ’ “ ‘'you O\n",
            "=== GENERATED SEQUENCE 5 ===\n",
            " there!!? o O -y right Right There Here here so So : at @ @ VP FFF FinalacingCT RS.. but m IptrRCCKk K C4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIzKAY-98H_b",
        "outputId": "5b2680b6-8098-4b2d-a61d-0a5312c9313e"
      },
      "source": [
        "!python run_metrics.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/Obama\" \\\n",
        "  --test_data_file \"data/BarackObama/test.txt\" \\\n",
        "  --length 50 \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_ref 50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/01/2021 21:40:22 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/Obama', model_type='bert', n_gpu=1, no_cuda=False, num_ref=50, output_dir='outputs', p=0.9, repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, test_data_file='data/BarackObama/test.txt')\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['\" Even I I I I I. I in in,, or or or or or or or and and and and and and and and and or and and and and and and and and and and and and and and and and and and an', '\" You You,, so so so so so so so so so so so so so so so so and so so so so so so so so so so so so so so so so so so so so and so so so so so s', 'Ready..... series. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric Eric ) ( ( \" sometimes sometimes sometimes sometimes sometimes ] ] ] ] ] ] ] ] ] ', \"Bonnie's s s s s s s table!!........, lot lot lot and well well being being always always always there there there there there though though though and run to did well always run as a\", 'Bahamianianianianianianianianianianianianianianianianianianianianismismismismismismismismismismismismismismismismismismismismism in in to with them i', 'In in.................. and and it so so so so so and and so so so so so so so so so so so so so so so so so so s', '\" All All All Everything \" \"????????..... & & & & and in............. so so and so so and, and and an', 'Our Our Our Our Our We We We We We Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Wh', '\" This This This This The The The The The The The The The,,, and and and and and and and and and and and and and and and and, and and and and and and and and and and and and and and an', 'Help Help help help help it it it it this mom mom mom mom mom mom mom mom mom mom mom mom family mom mom baby baby baby child child mom mom mom mom bar bar mom mom mom mom baby baby baby child sister and mom mom mo', 'Today...... or or even so so so so so or to so so so so so and,, or or or or and or and, and so so so so and and and and so so so and and and an', 'EnjoyjoyingingNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN', '\" Few Few Few Few Few Few Few Few Few few with............, but but so so so so so and so so so so so so so so so so so so so so so s', 'Welcomingcomingcomingcomingcomingcomingcomingcoming daughter was she she........ lot lot very lot lot lot very lot but. there there there there there there and and and and and and and and and and an', 'The the the how the or and in in you you so so and there there there there there there there there there there there and and and and and so so so so so and and so and, always always and and and and and and an', 'Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health Health health Health health health health health health health health health car car toy gradually gradually gradually gradually gradually gradually gradually gradually gradually gradually gradually graduall', 'Watch Watch :...... as and or or as so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so s', 'WATCH : : : : : online online online online online online and as continuing as as as as as as as as as as as as as as world as as as as as as as as as as as as as as as as a', 'WATCH : : : :....., and so so so so so always always and and and and and and in in and and and and and whole and and and through always and and and, and always and and an', 'Thesshnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnhnh', '“ Every Pit Pit Pit pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Big Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit Pit,,,, & S S,,,', 'President President President president and as as so so as as as so so so so as so so so so so so so so so so so so so so so so so and so so so so so so so so so so so so so s', 'FACT : :....... so so then so so so so so so so so and so so so so so so so so so so so so so so so so so so so so so so so so so s', '\" The The I I I I and I I I I, said said said said not not but but but but but but but but but but but but but but but so so, and and and and and and and and and and and an', 'Michelle Michelle Michelle Michelle,,,,, left,, and and so so to,, and and and and and throughout always always always and always always always always always always always always and always always always always always always always always always always alway', '. @ OFA _ PA NL............ and,, and and and so so,. and and and and,, and and and so.. and and and, and and so s', 'Support SupportiveiveiveWWWWWWWWWWW WWWWWWW w w w w w w w w w Wwwwwwwwwwwwwwww', 'As As,,,,, and and and and and to to all and and and and and and and and and and and and and and and and and and and and and and and and and and and and, and run and and an', 'Good Good good good good good good good good good good good good good good good good good good good good and and and and and and so so so and and and and and and and and and so so and and and and and and and an', '\" These These These This This This And That That And And And And And And And And To And There There Which Home Out That There And And And To To To To To To To To To To To To To To To To To To T', 'In in...................... and. so and lot lot and lot and, and so and or so so as, and and in throughout person and an', 'President President President president and to to the the the the and and and and and that that that and And And And And And And And And and And And And And And And And That..... and and and so so so an', 'Right Right right right right and and and like you you up and and and and and and and and and and and and and and and and and and and and and and and and and and and, and and so and down down and and an', '\" In In In In In................... that so so so so so and and and so so so.......,... an', '\" Sergeant Sergeant,,,,,, and and,, & &ox as &, in in in - - - to to to from and and and and and or in to,, and and and through and and,,, fro', 'Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy Kathy......,. that.............', '\" I I I and and and that that...,.... so and and and so so and and and and and and and and and and and and and and and and and and and so and and and and and an', 'President President president president president president first first first - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Happeningningningningning from from within within within within within within within within within within within within as so so so so so so so so so so so so so so so so so so so so so so so so so so s', 'An An an states a a a A A A... here here here here here here here Here - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'Get..... that that that that down down down down and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and down and and an', 'Take TakennnnnnIIIIIIIIIIIIIIII........ so........ so......', '\" We We We ; [ ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] [ [ [ ] ] ] ] ] ] ] ] ] ] [ and & & & or or and & ways fo', 'Made Madennnnnnnnnnnnnnnndea triednnnnnnnnnnnn agoeadeadamentssssssaysaysealealealealea', 'Americans Americans and and and and and throughout throughout from from and and and always always always always always always and and and always always always always always always always always always always always and and up and and and and and always always and and and and an', 'FACT : :....,............. and so so so to to and and and and and and and and and and and always always so so and and and and and an', '\" America America is is who that that to more or or run running ran running running down that that that or or or or and said that, said and and and and and and and and and and and and and and and and and and an', 'Closing Co with..... etc etc etc. Each till till Each Each Each Each Each Each Each Each Each Each Here Here Here There There There There There Here Here That That that He Which Which And Each But Which Each But But Bu', 'Expandingdingding B........... and And. but but but and like like like.. So so so so.... as and but but so so so so so so then']\n",
            "['\"Even before taking office, I made clear that Iran would not be allowed to acquire a nuclear weapon on my watch.\" —President Obama #IranDeal', '\"You guys give me confidence. You guys give me hope. I really need all of you to get out and vote on Nov 2.\"', 'Ready for spring.', 'Eric has a story that you need to hear:', \"Bonnie's 23-year-old daughter was diagnosed with cancer. She wrote a letter everyone needs to read: #ACA\", 'Bahamian families and communities face a long recovery from Hurricane Dorian. And Americans are reaching out to help. Brittany Garvin is helping people reunite and evacuate.', \"In Indiana on the 40th anniv. of Dr. King's assassination. Asking everyone to continue his lifelong pursuit of social & economic justice.\", '\"All of our men and women in uniform around the world must know that they have our respect, our gratitude, and our full support.\" #SOTU', \"Our take: he's right. No major differences between Tea Party agenda and GOP candidates' policies.\", '\"This will be a year of action.\" —President Obama', 'Help organizers from across the country get the training needed to take their skills to the next level:', \"Today at p.m. ET: Don't miss President Obama's announcement on #climate change.\", 'Enjoy the holiday season with the ones you love. Michelle and I wish you a very Merry Christmas!', '\"Few things can have as negative an impact on our economy as climate change.\" —President Obama #ActOnClimate', \"Welcoming the nation's students back to school—watch today at noon ET:\", 'The final march for reform has begun. We need you to talk to your neighbors and help spread the facts on health reform:', 'Health care reform took generations of organizing. Say you were a part of this: (ht @OFA ) #8Million', 'Watch a livestream of the second inauguration here starting at 1am ET:', 'WATCH: President Obama discusses #CollegeOpportunity with students from across the country.', \"WATCH: The Obama administration supports a ban on conversion therapy to protect America's youth.\", 'The March 1st deadline is approaching. #GetCovered today:', '“Every time you fill up at the pump, they’re making money. They’re doing just fine.”—POTUS on ending Big Oil subsidies:', 'President Obama: \"Today, our businesses have created over 5 million new jobs. The American auto industry is back on top.\"', \"FACT: The Affordable Care Act allows young adults to stay on their parents' insurance until they are 26 years old.\", '\"The United States of America is the most powerful nation on Earth. Period.\" —President Obama #SOTU', 'Michelle and I send our condolences to the people of New Zealand. We grieve with you and the Muslim community. All of us must stand against hatred in all its forms.', '.@OFA_PA volunteers are building this campaign in the rain—reminding folks to vote in tomorrow’s primary:', 'Support the campaign from your front lawn with our 2012 yard signs:', 'As the June unemployment numbers are released, it is clear we are headed in the right direction, but we are not headed there fast enough.', 'Good news: The economy added 216,000 private sector jobs in May. #ActOnJobs', '\"These cuts are not smart. They will hurt our economy and cost us jobs—and Congress can turn them off at any time.\"', 'In Cedar Rapids, IA at Coe College for the MySpace MTV Presidential Dialogue. It will stream live at pm ET', 'President Obama: \"Over those 100 days or so, more than 100 times as many Americans have fallen victim to gun violence.\" #DemandAction', 'Right now @OFA is looking for new members for the team that will make 2014 a year to remember. Are you in?', '\"In no other nation on Earth could my story even be possible.\"', '\"Sergeant First Class Cory Remsburg never gives up, and he does not quit.\" —President Obama #SOTU', 'Kathy recalls #Nov4 2008: \"People danced; people sang. I had a feeling that the country had somehow changed in one evening.\"', '\"I promise you: You have a president who will take action to support working families.\" —President Obama #FamiliesSucceed', 'President Obama: \"Hello Colorado! Are you fired up? Are you ready to go?\" #AmericaForward', 'Happening now: President Obama is holding a press conference. Tune in.', 'An inside look at a GOP super PAC\\'s plan to \"do exactly what John McCain would not let us do\" in 2008:', 'Get a close look inside the first-ever @OFA Organizing Director Summit:', 'Take a look at what editorial boards across the country think of the Senate refusing to do its job: #DoYourJob', '\"We cannot ignore the problems that we have, but we can\\'t stop running the race.\" —President Obama', 'Made in the USA and all purchases go toward re-electing the President—take a look around the #Obama2012 store:', 'Americans want progress. This is about making it happen. Watch:', \"FACT: Romney's plan repeals Obamacare, won't protect families from insurance company abuses, and leaves 50 million more Americans uninsured.\", '\"America would have gone backwards. That\\'s not what we do. We move forward.\" —President Obama', \"Closing background check loopholes will save lives—and it's time for Congress to act. #DemandAction,\", \"Expanding #CollegeOpportunity is vital to America's success in a global economy:\"]\n",
            "12/01/2021 21:41:14 - INFO - __main__ -   ***** Metrics results *****\n",
            "12/01/2021 21:41:14 - INFO - __main__ -       bertscore_f1 = 0.7492843866348267\n",
            "12/01/2021 21:41:14 - INFO - __main__ -       bertscore_precision = 0.7001819610595703\n",
            "12/01/2021 21:41:14 - INFO - __main__ -       bertscore_recall = 0.8075769543647766\n",
            "12/01/2021 21:41:14 - INFO - __main__ -       bleu_3_score = 0.429026992489762\n",
            "12/01/2021 21:41:14 - INFO - __main__ -       bleu_4_score = 0.5166118378198921\n",
            "12/01/2021 21:41:14 - INFO - __main__ -       bleu_5_score = 0.5791474485898985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p_hRH1D8xSX"
      },
      "source": [
        "# WSJ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y1LBTqj9Gx6",
        "outputId": "7c6bfd74-8250-4ac7-c70c-645d2b809e7e"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/wsj\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --repetition_penalty 2.0\\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/02/2021 00:56:31 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/wsj', model_type='bert', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=2.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            " - - - - - - - - w WWWSSIS GODEDECE NESETELE DEAEAHAHHHHOMOWymYy YXLLIFIV\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "AT ATMMs 3 Three ofof the The 5 five best greatestest in In Music Fi Fifi fimic to, come coming years year on on on : : : i @m mms & \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            " & and 2 Two : : ; 1 4 6 0 33 - - +, 7 111220141316 2012 2012 2013 # # The Best best ofof 2016 2015 in at the the lates\n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "TTTBBM AMINGING R RIMRR & and n thee @ _ _ _ call callsn # # w WSsJWowawayyyew\n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "N R Y, — ‘ ’ ” “ ‘ “ – ” ” ” ” ’ ” ” ” ” ” ” ” through Through Through loop this this time season seasons of s change all everything it that Tha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mog2a4J9OXT",
        "outputId": "1a5450e9-ec08-4af1-b98b-67cb6becb17f"
      },
      "source": [
        "!python run_metrics.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/wsj\" \\\n",
        "  --test_data_file \"data/wsj/test.txt\" \\\n",
        "  --length 50 \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_ref 50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/02/2021 00:56:53 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/wsj', model_type='bert', n_gpu=1, no_cuda=False, num_ref=50, output_dir='outputs', p=0.9, repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, test_data_file='data/wsj/test.txt')\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['It ’ s s the the the the and and and and and and and the the how and and and and and and and and and and and and or and and and or or or course not himself not or or er er. fact the no', 'India ’ s s W W w w w w w w w w w w w w w w w w w w w ws w w w w mj w w w and and and and and and and and and and and and an', 'Take 5 5 5 5 5 5cccccccvenenenenenenenenenenenenenelea...................', 'T. a. T. u.,.. the the.... da da da da da da da da da da.....................', 'REI ’ s s its. k k k k da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da da d', 'Time Time - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'From From sources sources : : : w w w w w w w w and...... and and and and and and and and and and and and and and and and and and and and and and and and and and an', 'In In Tuctct Uuantuantualualual......... and and and and the the the the rather rather rather rather rather rather rather rather rather rather rather rather rather rather rather rather rather rather rather rather rathe', \"Netflix's s scing da da da da da da da da da da da da da da da da da da da da da da da da da da da any any..........unk..\", 'Opinion : : graphics @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ m theveveveveveveveveveveveveveveveveve....', 'Obama < < B B B B B Bson B Blas B B Bunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkun', 'Breaking : : : Incmomomomomomomomokokolelelelele or or or....... the.................', 'A A A A A A or A A A A A A A or.................................', 'The The the not it that or or or or or or or or or or or or or speech or or speech speech speech stage stage,, and and and and and or, other that that speech is not not not itself occur alone.', 'All All - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'General General General General General someone with with who who will will of of all all this this this this for for and and and and and and and and and and and and and and and and and and and and and and and and and and an', 'Trump Trump Trump Trump Trump the the the the no no the the the or either an an an an an a an an a a a... the the the it.. the a an an.. a a.. any. a', 'Chesapeake See See See See See See See See itke,, and and,, and and and or or,,, or or or or,, or or or,,, or or.........', 'Goldman Goldman Goldman Goldman Goldman Goldman Goldmanhauuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu', 'More More More More More More More R Gununununununununununumumumumumumumumumumumumumumumumumumumumumumumuuuuuu', 'Centristristristristristristrist si mcccccc hveuuuuuuuuuuuuuuuuuuuuuuuuuuuuu', 'During The The most most most most likely likely likely likely likely likely likely a,,,,,,,,,. as,,,. or or................', 'Florida Florida color state state state state state state state state all state state state state state state state state all all all all ofmusmusmusmusmusmusmusmusmusmusmusmusmusmusmusmusmusmususmusmusmusmusmu', 'The The The < < < < the the the.... the the........ and and and that that. very.............. and and.', 'Open Open Open Open Open open cellOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOSOS.OSOSOSOSOSOSO', 'Rockets The The & & & & & a a a a the and and and and and and and and and and and and and and and and and and himself himself his himself himself himself himself himself himself himself himself himself himself himself himself he one himsel', 'US < > > > > <........te...e......tetetetetetetetetetetetetetetetetetetetetetet', 'As As as as as as as as as as as really really really really really really really really really actually really really actually actually actually actually really really really actually really actually really really actually actually actually actually actually actually actually actually actually actually actually actually actually actuall', 'Law Law Law Law Law Law Law Law Law.,,,,. and and of of of of of at at was that the the the most was was was it was itself itself itself actually actually actually actually fact fact....', \"BP's s s s z z z z z z z z z z Z Zzele z z z z z z z and and and and and a a an.. and and and and and and the the and plus :\", 'The The #aw @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ our our our our our....................', 'Jessica The The Is Law Law Law,,,,,,,,,, the the the... you,, too too,, the the the....., more more more more more more more really us u', 'Stop Stop Stop Stop Stop Stop Stop Stop Stop Stop stop stop stop stop stop stop stop stop...... all all our our everyone an as as you you the or or or or and you you you or or or or or o', 'Yummmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmredededededededededed', 'Is Is Is Is Is Is \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ', '7 7 7 7 7 7 7 7 or or????????..............................', 'The the is word word word word word word,,,,,,,,,,,,,...,,, 6 6....... the or...... you a and an', 'Opinion : : Photo #.................cite........ or..............', 'Grupoooo Ce Ce Ce Ce languageririan and and and and and and and and and and and and and and er er er er er er er er er er er er er er er er er er er er e', 'Want Want Want to to to Be Be her - - - - and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and an', 'Three Three others others for for for for for for and and and and and and and and and and and and himself himself himself others and and and and and itself itself itself itself itself itself itself itself itself itself itself itself itself itself itself itself itself itsel', 'Trump Trump Trump and and and and and and and and and and and and and and and and and himself rather and Rather..... rather rather it and was was it rather rather nor nor nor....... i', 'Opinion : : graphicsMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMypeypeypeypeypeypeypeypeypeyp', 'Consumer Consumer Consumer Consumer Consumer Consumer Consumer Consumer Consumer Consumer Y............le........................', \"GM's s s sreareareacinecinecinecinecinecinecinecinecinecinecinecinecineciciciciciciciciciciciciciciciciciciciciciciciciuu\", 'Ohio Ohio > > > Col Col.. of at from ; ; ; ;............. ;..................', 'Federal. Bil... only only.. my.. the the the the.................... or or........', 'Prime the the you,,. or......,, or or....... the of,,. or or.... the or or.........', 'Facingacing F F F FUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU', 'When When Is Is Is Is Is Is is to or or or or or.................................']\n",
            "['It’s rocket science: North Korea lifts the hood on its newest missile', 'India’s Supreme Court is set to decide a closely watched case in a decadeslong dispute between Hindu and Muslim groups over control of an especially controversial religious site in northern India', 'Take an early look at the front page of The Wall Street Journal', 'T.a.T.u., a Russian pop duo with a lesbian image, is set to play Sochi Olympics opening ceremony.', 'REI’s new headquarters blur the boundaries between office and nature', 'Time for a rethink on Chinese property stocks — Heard on the Street', 'From @WSJopinion : “These freshmen give me faith that the post-#MeToo world might be a safer one.” Students reflect on the #MeToo era and its effect on college campuses.', 'In these world championships, competitors go for the gold in welding, baking, hairdressing and bricklaying', \"Netflix's Q2 earnings plummeted 91% as the company took in higher subscription costs. $NFLX\", 'Opinion: Air-traffic reform would reduce federal employees by more than 30,000 and federal spending by billions', \"Obama watched Mubarak's speech on Air Force One, then huddled with his national-security team\", 'Breaking: Bubba Watson wins the Masters, beating Louis Oosthuizen on the second hole of a playoff.', \"A company in Tokyo is using the 'Rocky' theme song to inspire Japanese workers to leave the office\", \"The NFL coaches best at ushering their teams into the future? They're the oldest ones\", 'All about the dark arts of cellphone signal-strength bars: \"It\\'s just a rough graphical presentation.\"', 'General Motors CEO Mary Barra finally reveals the whereabouts of the GM jetpacks', 'Trump drafted letter on why he wanted Comey out', 'Chesapeake Energy shares spiked after announcing CEO Aubrey K. McClendon will step down in April.', 'Goldman Sachs is considering shutting down its dark pool trading operation known as Sigma X.', 'More than 500 nuclear tests have been conducted above ground, but none since 1980', 'Centrist Democratic Sen. Joe Manchin of West Virginia stepped up his campaign to cut the price tag of Democrats’ $3.5 trillion healthcare, education and climate legislation, calling for a “strategic pause” in the effort', 'During World War I, a pigeon named Cher Ami delivered a message that saved nearly 200 U.S. soldiers despite losing a limb to enemy fire. Now members of his breed sell for big bucks.', 'Florida hospitals to settle Medicare-fraud allegations tied to unnecessary ambulance rides:', 'The FTC has ordered broadband providers to file detailed special reports on their data-handling and privacy practices within 45 days', 'Open surgeries did not see a sustained increase after the FDA warned of the risks of using power morcellators in hysterectomy operations, a study found', 'Rockets were fired from the Gaza Strip in what appeared to be an attack targeting a memorial ceremony for a slain Israeli soldier', 'US Airways to Cut Jobs, Raise Fees', 'As U.S. mulls moving embassy to Jerusalem, Palestinians feel helpless', 'Law schools are seeing the first significant uptick in applications in years. One of the reasons? Politics', 'BP\\'s \"top kill\" procedure has failed to stem the flow of oil into the Gulf; next step will take 4-7 days', 'The website that broke the story of the racist photo that is imperiling Virginia Democratic Gov. Ralph Northam is a pro-Trump outfit backed by Republican operatives', 'Jessica Alba’s Honest Co. has struggled to achieve sustainable growth after product woes', 'Stop counting calories. It’s the clock that counts.', 'Yum China is testing new technologies and menus in an effort to be more than finger lickin’ good', 'Is Twitter really worth $10 billion?', '7 myths (and truths) about olive oil:', 'The Domino’s tracking app delivers pizza to all, disillusionment to some', 'Opinion: Are foundations running state energy policy without transparency?', 'Grupo Mexico Hits Hurdle in Asarco Case 1', 'Want to go beyond your usual G&T? Here are 5 easy gin and tonic upgrades:', 'Three Deals Launched on TALF-Debut Day', 'Trump called Justin Trudeau “two-faced” after the Canadian prime minister and other world leaders appeared to talk about the U.S. president at a NATO reception', 'Opinion: While Washington is stuck in neutral, French and Dutch reformers are pushing radical change, writes @josephsternberg', 'Consumer demand for meat is surging, and supermarkets are bracing for shortages as the coronavirus forces meatpacking facilities to close', \"GM's Wagoner Will Step Down 1\", 'Ohio State president Gordon Gee will retire July 1, in the wake of recently released controversial comments.', 'Federal prosecutors say Anthony Weiner deserves about two years behind bars in his sex case', 'Prime minister Theresa May is expected to say Tuesday that the U.K. wants a clean break from the EU', 'Facing rivals with powerful mobile OS ecosystems, Amazon and Microsoft will let Alexa and Cortana talk to each other', 'When Snap said it wanted IPO investors to get powerless shares, few investors said no']\n",
            "12/02/2021 00:57:52 - INFO - __main__ -   ***** Metrics results *****\n",
            "12/02/2021 00:57:52 - INFO - __main__ -       bertscore_f1 = 0.752407431602478\n",
            "12/02/2021 00:57:52 - INFO - __main__ -       bertscore_precision = 0.7041760087013245\n",
            "12/02/2021 00:57:52 - INFO - __main__ -       bertscore_recall = 0.8088936805725098\n",
            "12/02/2021 00:57:52 - INFO - __main__ -       bleu_3_score = 0.43653898608080133\n",
            "12/02/2021 00:57:52 - INFO - __main__ -       bleu_4_score = 0.5194411025607476\n",
            "12/02/2021 00:57:52 - INFO - __main__ -       bleu_5_score = 0.5779087326973028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRatv_m9L_e"
      },
      "source": [
        "# CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ_aYD5i9Svg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d74d13-450c-46a2-b07c-2b6d901125e0"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/cnn\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --repetition_penalty 2.0\\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/08/2021 06:49:15 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/cnn', model_type='bert', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=2.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            " Why Do Did It it? : : & @ at At'risk s S Secretpoon Follow # # #a RaAhasashacha Cha Atadadasms 44XxIV V II\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "G B F L S R MMCH H DDSVS N NE U O IN ON,,, on off off to air Air withingings T t lweetweet \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            " The No Next next Coming Girl Girls girls boys, — \" \" \" Out out Find find the the weekend week'’ s s s theme Theme of to Thanksgiving Christmas Santa in In That this This Is is\n",
            "=== GENERATED SEQUENCE 4 ===\n",
            " insideall : : 1 Oneppm pm PM ETETET TTSG G ZIFITCAAA AAA &,????.... 3! < \" 2 Two - on off off\n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "Google AppleP is this here Here : and &!?? ca Nowld @ @ at toleamsmMrercCCGHNNNNNNNNNNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMnzBoXv9TDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af9c85e-85d5-4bbd-ab01-e0e90e27c017"
      },
      "source": [
        "!python run_metrics.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"models/cnn\" \\\n",
        "  --test_data_file \"data/cnn/test.txt\" \\\n",
        "  --length 50 \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_ref 50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "12/08/2021 06:49:37 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/cnn', model_type='bert', n_gpu=1, no_cuda=False, num_ref=50, output_dir='outputs', p=0.9, repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, test_data_file='data/cnn/test.txt')\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "Downloading: 100% 482/482 [00:00<00:00, 465kB/s]\n",
            "Downloading: 100% 878k/878k [00:00<00:00, 4.31MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 2.50MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 6.01MB/s]\n",
            "Downloading: 100% 1.33G/1.33G [00:43<00:00, 33.0MB/s]\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['Husbandsband says says VP VP VP VP VP VP VP VP VP VP VP VP VP VP VP VP VP VP VP,,,,,,,,,,,,,,,,,,,,,,,,', 'Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsin Wisconsi', 'U. S.............................................', 'U. S.............................................', 'RNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCNCY NE NE NE N', 'Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitterfe Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter Twitter media media media media media media media media medi', 'President President President President President President President President President President President President President President President President President Korea - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ', 'The The The The The The The The The The The The The The The The The The The The The The The The The The The The Theyyyyyyyyyyyyyyyyyyy', 'D. C.............................................', 'A A B B B B B B B B B B III III B B V I I... that what to to expect expect....................', 'CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN network network network network network network network network network network network network network network network network network networ', 'Poll : : Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll Poll exit negative exit Note Source......... a a one one one on', 'Police : : 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ', 'Transgenderder trans a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a ', 'What????????????????????????????????...............', \"Sheriff's s afterlay story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story story stor\", 'Astronomerss di dittottottottottottottottottottottottottotta,,,,,,,,,,,,,,,,,,,,,,,,,,,', 'Witness Witness Witness : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : ', \"CNN's s ’ ’ s s'''''''''''''''''''''''''''''''''''''''' \", 'Employeesees : : 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 1', \"What's s why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why why Why Wh\", 'New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New New,,, ', 'This This This,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,', 'Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Florida Floridagogogogogogogogog', 'Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smith Smit', 'BREAKINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGINGWtest last', \"Women's s'''''''''''''''''''''''''''''''''''''''''''' \", 'Ex - Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholic Catholi', '\" Sometimes sometimes \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ', 'AT & T T T T T T T T T T T T T family L L L L L L L L L L L L L I.......... that that that that....', 'Mayor Mayor Mayor Toronto Toronto Toronto Mayor Mayor Mayor and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and an', 'National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National National Nationa', 'Reid : :.............................................', 'UK : : :............................................', 'Report : :.............................................', 'From From From From From From From From From From From From From From From From From From From From From From From From From Of Are Are Are Are Are Are Are To..............', 'Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texas Texa', 'Knox : :.............................................', 'N. K.............................................', 'Rubin Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Why Whyiu', 'Nearly Nearly???????????????????????????????????de..........', 'Firefighterfighter rescue rescue rescue ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ', 'Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Second Secon', 'Canadian Canadian Canada Canada Canada Canada Canada Canada Canada Canada Canada Canada Canada Canada Canada Canada E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E ', 'Democrat :OSOSOS B BOSOSOSOSOSOS E EOSOSOSOS E E E E E E E E E E E E E E E E E E E E E E E E E E E E E ', \"Here's s s s best best best best best best,,,,,'''''''''''''''''''''''''''''''\", 'The The The The The The The The The The The The The The The The The The The The The The The The The The The The Theyyyyyyyyyyyyyyyyyyy', 'Sotomayor : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : ', 'The The The The The The The The The The The The The The The The The The The The The The The The The The The The The following same same same same same with with to or or or or or or or or or or a', \"Logic'''''''''''''''''''''''''''''''''''''''''''''''\"]\n",
            "[\"Husband wants answers after wife's death sparks abortion debate in Ireland:\", 'Wisconsin Senate bypasses Democrats, OKs union limits.', 'U.S. Embassy in Kabul attacked.', 'U.S. drone shot down in eastern Iran, state media reports, citing military source.', \"RNC chairman condemns 'Magic Negro' song 1\", 'Twitter has restored a feature that allows users to \"block\" unwanted followers. money.', 'President Trump: \"Amazon is gonna have to pay much more money to the Post Office. There\\'s no doubt about that\"', 'The Defense Department has confirmed that leaked photos and video of \"unidentified aerial phenomena\" taken in 2019 are indeed legitimate images of unexplained objects', 'D.C. school system fires 241 teachers.', 'A man was killed by a crocodile nicknamed \"Michael Jackson.\" Here\\'s what police say appears to have happened:', 'CNN wants YOUR input to choose the most intriguing people of 2012:', 'Poll: Clinton and Trump neck-and-neck in Iowa', 'Police: Spears has minor accident in Mercedes', 'Transgender teen: \"My death needs to mean something.\" #LeelahAlcorn', 'What do you want to know about pop star Britney Spears’ fight to take back her life? Watch CNN Special Report: Toxic – Britney Spears’ Battle For Freedom to find out all about it. Tonight at 8 p.m. ET', \"Sheriff's deputies in Los Angeles handcuffed rapper Wyclef Jean after mistaking him for a robbery suspect\", 'Astronomers have discovered auroras on a brown dwarf 20 light-years away.', 'Witness in France says truck driver appeared to accelerate as he hit bodies', 'CNN\\'s Chief Medical Correspondent Dr. Sanjay Gupta fulfilled a lifelong dream by hosting \"Jeopardy!\" He is one of several guest hosts for the show.', 'Employees sacked over Ford art on tied-up women.', \"What's hiding in your food? @MorganSpurlock uncovers the things you probably shouldn't be eating on #InsideManCNN\", \"New York state officials are attempting to have a federal court throw out President Trump's lawsuit over the possible release of his tax returns to Congress, according to a new court filing\", 'This is @Bourdain \\'s favorite \"vegetable.\" Is it yours? Explore the flavors of Madagascar on #PartsUnknown tonight. vine.co CNN\\'s post on Vine This is Anthony Bourdain\\'s favorite \"vegetable.\" Is it yours? Explore the flavors of Madagascar on #PartsUnknown.', 'Florida mother shoots and kills infant son in murder-suicide attempt, police say:', 'Smith & Wesson shares hit an all-time high Monday', 'BREAKING NEWS: The Alabama Crimson Tide caps undefeated season with 37-21 win over Texas Longhorns in BCS title game.', 'Women\\'s suffragist Susan B. Anthony was never able to vote, so people are leaving \"I voted\" stickers on her grave', 'Ex-Catholic priest in photo scandal marries 2', '\"Sometimes we\\'re treated like mushrooms, we\\'re kept in the dark\": Rep. Garamendi says Niger briefing lacked details', 'AT&T to activate faster 4G network:', 'Mayor cleared in drug raid that left dogs dead', 'National guardsman reunited with dog days after tornado destroys town.', 'Reid: \"Watching white nationalists celebrate while innocent Americans cry tears of fear does not feel like America\"', 'UK: China executes British citizen.', 'Report: Fugitive suspected in Jakarta blasts 2', \"From CNN's Matthew Chance: Clashes outside the hotel. #Rixos4 hunkered down. #Libya @mchancecnn\", 'Texas Senator Ted Cruz pheasant hunts, bashes Obamacare in Iowa', \"Knox: 'I'm afraid to go back' to Italy. @ChrisCuomo 's wide-ranging intv wAmanda Knox; 10p ET Tues on CNN.\", 'N.K. has made nuclear progress, experts say', \"Rubin 'Hurricane' Carter still fighting long after boxing days pass.\", \"Nearly half of Florida's counties have agreed to provide Spanish-language assistance and materials to voters after a years-long legal battle\", 'Firefighter arrives to find own house in flames.', 'Second storm snarls traffic in Argentina: 9', 'Canadian skier arrested, accused of stealing car at Winter Olympics', \"Democrat: 'Ask Tiger, not me.'\", \"Here's how to get all your holiday shopping done in one day #BlackFriday\", 'The space shuttle Enterprise is flying over New York City right now - watch it live on @CNN TV or', \"Sotomayor: 'Law is not legal theory' 2\", \"The changing face of America's youth.\", 'Logic rules drug murders, trafficker says 2']\n",
            "12/08/2021 06:51:18 - INFO - __main__ -   ***** Metrics results *****\n",
            "12/08/2021 06:51:18 - INFO - __main__ -       bertscore_f1 = 0.749073326587677\n",
            "12/08/2021 06:51:18 - INFO - __main__ -       bertscore_precision = 0.6987233757972717\n",
            "12/08/2021 06:51:18 - INFO - __main__ -       bertscore_recall = 0.809255838394165\n",
            "12/08/2021 06:51:18 - INFO - __main__ -       bleu_3_score = 0.2168216895319692\n",
            "12/08/2021 06:51:18 - INFO - __main__ -       bleu_4_score = 0.2788712726023858\n",
            "12/08/2021 06:51:18 - INFO - __main__ -       bleu_5_score = 0.3253437043220614\n"
          ]
        }
      ]
    }
  ]
}