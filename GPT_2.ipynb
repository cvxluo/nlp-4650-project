{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa9346ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f20345-d918-4a2e-de5f-abbb204989db"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/NLP/project"
      ],
      "id": "fa9346ed",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/NLP/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgNtwxjIVBg6",
        "outputId": "c19bdb8b-9467-4e91-acd9-6196b1efde71"
      },
      "source": [
        "%pip install transformers"
      ],
      "id": "ZgNtwxjIVBg6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90b_Z2ihbfdf",
        "outputId": "7234c639-0aee-4f87-9f2a-aa692e9811aa"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "90b_Z2ihbfdf",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 26 01:58:13 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96d57222",
        "outputId": "c29ab7f4-6419-421b-f980-ff1e41533708"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gpt2 import train, generate, save_model, load_model\n",
        "from analysis import loss_plot, perplexity_plot\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, DataCollatorForLanguageModeling, LineByLineTextDataset\n",
        "from transformers import AdamW \n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "id": "96d57222",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf9d2f59"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
      ],
      "id": "cf9d2f59",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khcZrryWtTQv"
      },
      "source": [
        "# Training"
      ],
      "id": "khcZrryWtTQv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd92f735"
      },
      "source": [
        "## Elon Musk"
      ],
      "id": "fd92f735"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f7c1901",
        "outputId": "ac5dee5e-1bab-456c-eb65-79c086b02d37"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=models/elonmusk --overwrite_output_dir --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=data/elonmusk/train.txt --eval_data_file=data/elonmusk/valid.txt --do_eval --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=5"
      ],
      "id": "8f7c1901",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/26/2021 02:47:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/26/2021 02:47:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/elonmusk/runs/Oct26_02-47-04_239f9388736f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "output_dir=models/elonmusk,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/elonmusk,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:47:04,782 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:47:04,783 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-26 02:47:04,877 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:47:04,970 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:47:04,971 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:47:05,756 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:47:05,757 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "[INFO|modeling_utils.py:1323] 2021-10-26 02:47:05,944 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1588] 2021-10-26 02:47:07,844 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-26 02:47:07,844 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:47:07,857 >> Creating features from dataset file at data/elonmusk/train.txt\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:47:08,326 >> Creating features from dataset file at data/elonmusk/valid.txt\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:47:12,266 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:47:12,266 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:47:12,266 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1196] 2021-10-26 02:47:12,628 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-26 02:47:12,629 >>   Num examples = 2067\n",
            "[INFO|trainer.py:1198] 2021-10-26 02:47:12,629 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1199] 2021-10-26 02:47:12,629 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-10-26 02:47:12,629 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-10-26 02:47:12,629 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-26 02:47:12,629 >>   Total optimization steps = 5170\n",
            "{'loss': 4.5065, 'learning_rate': 4.516441005802708e-05, 'epoch': 0.48}\n",
            " 10% 500/5170 [00:32<05:15, 14.81it/s][INFO|trainer.py:1995] 2021-10-26 02:47:45,565 >> Saving model checkpoint to models/elonmusk/checkpoint-500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:47:45,720 >> Configuration saved in models/elonmusk/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:47:51,734 >> Model weights saved in models/elonmusk/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 3.4529, 'learning_rate': 4.032882011605416e-05, 'epoch': 0.97}\n",
            " 19% 1000/5170 [01:29<04:26, 15.66it/s][INFO|trainer.py:1995] 2021-10-26 02:48:42,311 >> Saving model checkpoint to models/elonmusk/checkpoint-1000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:48:42,417 >> Configuration saved in models/elonmusk/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:48:49,873 >> Model weights saved in models/elonmusk/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 2.8308, 'learning_rate': 3.549323017408124e-05, 'epoch': 1.45}\n",
            " 29% 1500/5170 [02:26<04:02, 15.11it/s][INFO|trainer.py:1995] 2021-10-26 02:49:39,385 >> Saving model checkpoint to models/elonmusk/checkpoint-1500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:49:39,480 >> Configuration saved in models/elonmusk/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:49:45,697 >> Model weights saved in models/elonmusk/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 2.7859, 'learning_rate': 3.065764023210832e-05, 'epoch': 1.93}\n",
            " 39% 2000/5170 [03:21<03:34, 14.81it/s][INFO|trainer.py:1995] 2021-10-26 02:50:34,050 >> Saving model checkpoint to models/elonmusk/checkpoint-2000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:50:34,170 >> Configuration saved in models/elonmusk/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:50:40,094 >> Model weights saved in models/elonmusk/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 2.3705, 'learning_rate': 2.5822050290135396e-05, 'epoch': 2.42}\n",
            " 48% 2500/5170 [04:19<02:59, 14.87it/s][INFO|trainer.py:1995] 2021-10-26 02:51:32,013 >> Saving model checkpoint to models/elonmusk/checkpoint-2500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:51:32,121 >> Configuration saved in models/elonmusk/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:51:39,404 >> Model weights saved in models/elonmusk/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 2.3104, 'learning_rate': 2.0986460348162475e-05, 'epoch': 2.9}\n",
            " 58% 3000/5170 [05:15<02:28, 14.65it/s][INFO|trainer.py:1995] 2021-10-26 02:52:28,478 >> Saving model checkpoint to models/elonmusk/checkpoint-3000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:52:28,586 >> Configuration saved in models/elonmusk/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:52:35,006 >> Model weights saved in models/elonmusk/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.0929, 'learning_rate': 1.6150870406189557e-05, 'epoch': 3.38}\n",
            " 68% 3500/5170 [06:12<01:48, 15.39it/s][INFO|trainer.py:1995] 2021-10-26 02:53:25,415 >> Saving model checkpoint to models/elonmusk/checkpoint-3500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:53:25,512 >> Configuration saved in models/elonmusk/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:53:30,875 >> Model weights saved in models/elonmusk/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 1.9983, 'learning_rate': 1.1315280464216636e-05, 'epoch': 3.87}\n",
            " 77% 4000/5170 [07:08<01:15, 15.47it/s][INFO|trainer.py:1995] 2021-10-26 02:54:21,133 >> Saving model checkpoint to models/elonmusk/checkpoint-4000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:54:21,239 >> Configuration saved in models/elonmusk/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:54:27,247 >> Model weights saved in models/elonmusk/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 1.822, 'learning_rate': 6.479690522243714e-06, 'epoch': 4.35}\n",
            " 87% 4500/5170 [08:04<00:44, 15.16it/s][INFO|trainer.py:1995] 2021-10-26 02:55:17,510 >> Saving model checkpoint to models/elonmusk/checkpoint-4500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:55:17,619 >> Configuration saved in models/elonmusk/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:55:23,113 >> Model weights saved in models/elonmusk/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 1.8139, 'learning_rate': 1.644100580270793e-06, 'epoch': 4.84}\n",
            " 97% 5000/5170 [08:59<00:11, 14.86it/s][INFO|trainer.py:1995] 2021-10-26 02:56:11,677 >> Saving model checkpoint to models/elonmusk/checkpoint-5000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:56:11,775 >> Configuration saved in models/elonmusk/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:56:17,725 >> Model weights saved in models/elonmusk/checkpoint-5000/pytorch_model.bin\n",
            "100% 5169/5170 [09:34<00:00, 11.65it/s][INFO|trainer.py:1409] 2021-10-26 02:56:46,925 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 574.2961, 'train_samples_per_second': 17.996, 'train_steps_per_second': 9.002, 'train_loss': 2.5741099315178464, 'epoch': 5.0}\n",
            "100% 5170/5170 [09:34<00:00,  9.00it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-26 02:56:46,934 >> Saving model checkpoint to models/elonmusk\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:56:46,941 >> Configuration saved in models/elonmusk/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:56:49,510 >> Model weights saved in models/elonmusk/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-26 02:56:49,520 >> tokenizer config file saved in models/elonmusk/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-26 02:56:49,523 >> Special tokens file saved in models/elonmusk/special_tokens_map.json\n",
            "10/26/2021 02:56:49 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-10-26 02:56:49,671 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-10-26 02:56:49,671 >>   Num examples = 605\n",
            "[INFO|trainer.py:2248] 2021-10-26 02:56:49,671 >>   Batch size = 2\n",
            "100% 303/303 [00:04<00:00, 66.04it/s]\n",
            "10/26/2021 02:56:54 - INFO - __main__ - ***** Eval results *****\n",
            "10/26/2021 02:56:54 - INFO - __main__ -   perplexity = 35.93180509124075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kvLJnmU12VU"
      },
      "source": [
        "## Andrej Karpathy"
      ],
      "id": "7kvLJnmU12VU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMdpZVmZX9De",
        "outputId": "a94d99b9-45b2-4d1b-aa39-193488c55ca2"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=models/karpathy --overwrite_output_dir --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=data/karpathy/train.txt --eval_data_file=data/karpathy/valid.txt --do_eval --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=5"
      ],
      "id": "RMdpZVmZX9De",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/26/2021 02:16:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/26/2021 02:16:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/karpathy/runs/Oct26_02-16-07_239f9388736f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "output_dir=models/karpathy,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/karpathy,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:16:07,216 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:16:07,217 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-26 02:16:07,308 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:16:07,402 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:16:07,403 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:16:08,161 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:16:08,162 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "[INFO|modeling_utils.py:1323] 2021-10-26 02:16:08,349 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1588] 2021-10-26 02:16:10,236 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-26 02:16:10,237 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:16:10,248 >> Creating features from dataset file at data/karpathy/train.txt\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:16:10,498 >> Creating features from dataset file at data/karpathy/valid.txt\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:16:14,334 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:16:14,334 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:16:14,334 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1196] 2021-10-26 02:16:14,682 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-26 02:16:14,682 >>   Num examples = 2672\n",
            "[INFO|trainer.py:1198] 2021-10-26 02:16:14,682 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1199] 2021-10-26 02:16:14,682 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-10-26 02:16:14,682 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-10-26 02:16:14,682 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-26 02:16:14,682 >>   Total optimization steps = 6680\n",
            "{'loss': 4.88, 'learning_rate': 4.625748502994012e-05, 'epoch': 0.37}\n",
            "  7% 500/6680 [00:32<06:41, 15.38it/s][INFO|trainer.py:1995] 2021-10-26 02:16:47,334 >> Saving model checkpoint to models/karpathy/checkpoint-500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:16:47,340 >> Configuration saved in models/karpathy/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:16:49,345 >> Model weights saved in models/karpathy/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 3.9949, 'learning_rate': 4.251497005988024e-05, 'epoch': 0.75}\n",
            " 15% 1000/6680 [01:14<06:03, 15.62it/s][INFO|trainer.py:1995] 2021-10-26 02:17:29,559 >> Saving model checkpoint to models/karpathy/checkpoint-1000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:17:29,565 >> Configuration saved in models/karpathy/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:17:31,561 >> Model weights saved in models/karpathy/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 3.6761, 'learning_rate': 3.877245508982036e-05, 'epoch': 1.12}\n",
            " 22% 1500/6680 [01:58<05:37, 15.34it/s][INFO|trainer.py:1995] 2021-10-26 02:18:12,880 >> Saving model checkpoint to models/karpathy/checkpoint-1500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:18:12,886 >> Configuration saved in models/karpathy/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:18:14,997 >> Model weights saved in models/karpathy/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 3.3552, 'learning_rate': 3.502994011976048e-05, 'epoch': 1.5}\n",
            " 30% 2000/6680 [02:40<05:10, 15.05it/s][INFO|trainer.py:1995] 2021-10-26 02:18:55,650 >> Saving model checkpoint to models/karpathy/checkpoint-2000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:18:55,655 >> Configuration saved in models/karpathy/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:18:57,663 >> Model weights saved in models/karpathy/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 3.2824, 'learning_rate': 3.12874251497006e-05, 'epoch': 1.87}\n",
            " 37% 2500/6680 [03:23<04:33, 15.26it/s][INFO|trainer.py:1995] 2021-10-26 02:19:38,536 >> Saving model checkpoint to models/karpathy/checkpoint-2500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:19:38,543 >> Configuration saved in models/karpathy/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:19:41,610 >> Model weights saved in models/karpathy/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 3.0354, 'learning_rate': 2.754491017964072e-05, 'epoch': 2.25}\n",
            " 45% 3000/6680 [04:07<04:01, 15.21it/s][INFO|trainer.py:1995] 2021-10-26 02:20:22,090 >> Saving model checkpoint to models/karpathy/checkpoint-3000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:20:22,095 >> Configuration saved in models/karpathy/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:20:24,224 >> Model weights saved in models/karpathy/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.904, 'learning_rate': 2.3802395209580838e-05, 'epoch': 2.62}\n",
            " 52% 3500/6680 [04:51<03:26, 15.40it/s][INFO|trainer.py:1995] 2021-10-26 02:21:05,753 >> Saving model checkpoint to models/karpathy/checkpoint-3500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:21:05,760 >> Configuration saved in models/karpathy/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:21:07,857 >> Model weights saved in models/karpathy/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 2.9375, 'learning_rate': 2.0059880239520957e-05, 'epoch': 2.99}\n",
            " 60% 4000/6680 [05:33<02:59, 14.92it/s][INFO|trainer.py:1995] 2021-10-26 02:21:48,541 >> Saving model checkpoint to models/karpathy/checkpoint-4000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:21:48,547 >> Configuration saved in models/karpathy/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:21:50,678 >> Model weights saved in models/karpathy/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 2.5919, 'learning_rate': 1.631736526946108e-05, 'epoch': 3.37}\n",
            " 67% 4500/6680 [06:16<02:20, 15.54it/s][INFO|trainer.py:1995] 2021-10-26 02:22:31,583 >> Saving model checkpoint to models/karpathy/checkpoint-4500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:22:31,589 >> Configuration saved in models/karpathy/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:22:33,756 >> Model weights saved in models/karpathy/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 2.6278, 'learning_rate': 1.2574850299401197e-05, 'epoch': 3.74}\n",
            " 75% 5000/6680 [06:59<01:50, 15.14it/s][INFO|trainer.py:1995] 2021-10-26 02:23:14,631 >> Saving model checkpoint to models/karpathy/checkpoint-5000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:23:14,638 >> Configuration saved in models/karpathy/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:23:16,762 >> Model weights saved in models/karpathy/checkpoint-5000/pytorch_model.bin\n",
            "{'loss': 2.6039, 'learning_rate': 8.832335329341319e-06, 'epoch': 4.12}\n",
            " 82% 5500/6680 [07:43<01:14, 15.80it/s][INFO|trainer.py:1995] 2021-10-26 02:23:58,630 >> Saving model checkpoint to models/karpathy/checkpoint-5500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:23:58,636 >> Configuration saved in models/karpathy/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:24:00,781 >> Model weights saved in models/karpathy/checkpoint-5500/pytorch_model.bin\n",
            "{'loss': 2.4336, 'learning_rate': 5.0898203592814375e-06, 'epoch': 4.49}\n",
            " 90% 6000/6680 [08:26<00:44, 15.31it/s][INFO|trainer.py:1995] 2021-10-26 02:24:41,327 >> Saving model checkpoint to models/karpathy/checkpoint-6000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:24:41,333 >> Configuration saved in models/karpathy/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:24:43,509 >> Model weights saved in models/karpathy/checkpoint-6000/pytorch_model.bin\n",
            "{'loss': 2.4228, 'learning_rate': 1.3473053892215569e-06, 'epoch': 4.87}\n",
            " 97% 6500/6680 [09:10<00:11, 15.38it/s][INFO|trainer.py:1995] 2021-10-26 02:25:24,848 >> Saving model checkpoint to models/karpathy/checkpoint-6500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:25:24,854 >> Configuration saved in models/karpathy/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:25:27,109 >> Model weights saved in models/karpathy/checkpoint-6500/pytorch_model.bin\n",
            "100% 6679/6680 [09:31<00:00, 12.63it/s][INFO|trainer.py:1409] 2021-10-26 02:25:45,982 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 571.3001, 'train_samples_per_second': 23.385, 'train_steps_per_second': 11.693, 'train_loss': 3.115300504033437, 'epoch': 5.0}\n",
            "100% 6680/6680 [09:31<00:00, 11.69it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-26 02:25:46,195 >> Saving model checkpoint to models/karpathy\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:25:46,201 >> Configuration saved in models/karpathy/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:25:48,700 >> Model weights saved in models/karpathy/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-26 02:25:48,709 >> tokenizer config file saved in models/karpathy/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-26 02:25:48,713 >> Special tokens file saved in models/karpathy/special_tokens_map.json\n",
            "10/26/2021 02:25:49 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-10-26 02:25:49,193 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-10-26 02:25:49,193 >>   Num examples = 763\n",
            "[INFO|trainer.py:2248] 2021-10-26 02:25:49,194 >>   Batch size = 2\n",
            "100% 382/382 [00:06<00:00, 57.84it/s]\n",
            "10/26/2021 02:25:55 - INFO - __main__ - ***** Eval results *****\n",
            "10/26/2021 02:25:55 - INFO - __main__ -   perplexity = 57.795531818144575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvp55vqy2N8V"
      },
      "source": [
        "## Barack Obama"
      ],
      "id": "Uvp55vqy2N8V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNrb23NFYsKD",
        "outputId": "e889711f-819d-4096-dcb0-2f44bed807c1"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=models/BarackObama --overwrite_output_dir --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=data/BarackObama/train.txt --eval_data_file=data/BarackObama/valid.txt --do_eval --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=3"
      ],
      "id": "oNrb23NFYsKD",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/26/2021 02:26:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/26/2021 02:26:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/BarackObama/runs/Oct26_02-26-06_239f9388736f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=models/BarackObama,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/BarackObama,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:26:06,767 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:26:06,768 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-26 02:26:06,862 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:26:06,961 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:26:06,961 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:26:07,885 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:26:07,886 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "[INFO|modeling_utils.py:1323] 2021-10-26 02:26:08,071 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1588] 2021-10-26 02:26:09,992 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-26 02:26:09,992 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:26:10,004 >> Creating features from dataset file at data/BarackObama/train.txt\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:26:11,030 >> Creating features from dataset file at data/BarackObama/valid.txt\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:26:15,038 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:26:15,038 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:26:15,038 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1196] 2021-10-26 02:26:15,387 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-26 02:26:15,387 >>   Num examples = 9415\n",
            "[INFO|trainer.py:1198] 2021-10-26 02:26:15,388 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-10-26 02:26:15,388 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-10-26 02:26:15,388 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-10-26 02:26:15,388 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-26 02:26:15,388 >>   Total optimization steps = 14124\n",
            "{'loss': 3.4845, 'learning_rate': 4.822996318323421e-05, 'epoch': 0.11}\n",
            "  4% 500/14124 [00:32<14:32, 15.61it/s][INFO|trainer.py:1995] 2021-10-26 02:26:47,721 >> Saving model checkpoint to models/BarackObama/checkpoint-500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:26:47,726 >> Configuration saved in models/BarackObama/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:26:49,886 >> Model weights saved in models/BarackObama/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 2.4585, 'learning_rate': 4.645992636646843e-05, 'epoch': 0.21}\n",
            "  7% 1000/14124 [01:14<14:06, 15.51it/s][INFO|trainer.py:1995] 2021-10-26 02:27:30,136 >> Saving model checkpoint to models/BarackObama/checkpoint-1000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:27:30,143 >> Configuration saved in models/BarackObama/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:27:32,238 >> Model weights saved in models/BarackObama/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 2.3968, 'learning_rate': 4.4689889549702635e-05, 'epoch': 0.32}\n",
            " 11% 1500/14124 [01:57<13:48, 15.24it/s][INFO|trainer.py:1995] 2021-10-26 02:28:13,005 >> Saving model checkpoint to models/BarackObama/checkpoint-1500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:28:13,011 >> Configuration saved in models/BarackObama/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:28:15,065 >> Model weights saved in models/BarackObama/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 2.3066, 'learning_rate': 4.291985273293685e-05, 'epoch': 0.42}\n",
            " 14% 2000/14124 [02:39<13:34, 14.89it/s][INFO|trainer.py:1995] 2021-10-26 02:28:55,060 >> Saving model checkpoint to models/BarackObama/checkpoint-2000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:28:55,075 >> Configuration saved in models/BarackObama/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:28:57,163 >> Model weights saved in models/BarackObama/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 2.2558, 'learning_rate': 4.114981591617106e-05, 'epoch': 0.53}\n",
            " 18% 2500/14124 [03:21<12:19, 15.71it/s][INFO|trainer.py:1995] 2021-10-26 02:29:37,274 >> Saving model checkpoint to models/BarackObama/checkpoint-2500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:29:37,279 >> Configuration saved in models/BarackObama/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:29:39,261 >> Model weights saved in models/BarackObama/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 2.227, 'learning_rate': 3.937977909940527e-05, 'epoch': 0.64}\n",
            " 21% 3000/14124 [04:05<12:33, 14.77it/s][INFO|trainer.py:1995] 2021-10-26 02:30:20,449 >> Saving model checkpoint to models/BarackObama/checkpoint-3000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:30:20,455 >> Configuration saved in models/BarackObama/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:30:22,505 >> Model weights saved in models/BarackObama/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.1996, 'learning_rate': 3.760974228263948e-05, 'epoch': 0.74}\n",
            " 25% 3500/14124 [04:47<11:33, 15.31it/s][INFO|trainer.py:1995] 2021-10-26 02:31:02,797 >> Saving model checkpoint to models/BarackObama/checkpoint-3500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:31:02,804 >> Configuration saved in models/BarackObama/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:31:04,841 >> Model weights saved in models/BarackObama/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 2.1712, 'learning_rate': 3.583970546587369e-05, 'epoch': 0.85}\n",
            " 28% 4000/14124 [05:29<10:51, 15.54it/s][INFO|trainer.py:1995] 2021-10-26 02:31:44,942 >> Saving model checkpoint to models/BarackObama/checkpoint-4000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:31:44,948 >> Configuration saved in models/BarackObama/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:31:46,968 >> Model weights saved in models/BarackObama/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 2.1557, 'learning_rate': 3.4069668649107906e-05, 'epoch': 0.96}\n",
            " 32% 4500/14124 [06:13<10:16, 15.60it/s][INFO|trainer.py:1995] 2021-10-26 02:32:28,427 >> Saving model checkpoint to models/BarackObama/checkpoint-4500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:32:28,432 >> Configuration saved in models/BarackObama/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:32:30,395 >> Model weights saved in models/BarackObama/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 1.983, 'learning_rate': 3.229963183234211e-05, 'epoch': 1.06}\n",
            " 35% 5000/14124 [06:56<09:58, 15.25it/s][INFO|trainer.py:1995] 2021-10-26 02:33:11,566 >> Saving model checkpoint to models/BarackObama/checkpoint-5000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:33:11,571 >> Configuration saved in models/BarackObama/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:33:13,662 >> Model weights saved in models/BarackObama/checkpoint-5000/pytorch_model.bin\n",
            "{'loss': 1.811, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}\n",
            " 39% 5500/14124 [07:39<09:14, 15.55it/s][INFO|trainer.py:1995] 2021-10-26 02:33:54,627 >> Saving model checkpoint to models/BarackObama/checkpoint-5500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:33:54,633 >> Configuration saved in models/BarackObama/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:33:56,804 >> Model weights saved in models/BarackObama/checkpoint-5500/pytorch_model.bin\n",
            "{'loss': 1.7891, 'learning_rate': 2.875955819881054e-05, 'epoch': 1.27}\n",
            " 42% 6000/14124 [08:22<08:41, 15.57it/s][INFO|trainer.py:1995] 2021-10-26 02:34:37,620 >> Saving model checkpoint to models/BarackObama/checkpoint-6000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:34:37,626 >> Configuration saved in models/BarackObama/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:34:39,797 >> Model weights saved in models/BarackObama/checkpoint-6000/pytorch_model.bin\n",
            "{'loss': 1.8294, 'learning_rate': 2.6989521382044748e-05, 'epoch': 1.38}\n",
            " 46% 6500/14124 [09:05<08:13, 15.46it/s][INFO|trainer.py:1995] 2021-10-26 02:35:20,768 >> Saving model checkpoint to models/BarackObama/checkpoint-6500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:35:20,773 >> Configuration saved in models/BarackObama/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:35:22,956 >> Model weights saved in models/BarackObama/checkpoint-6500/pytorch_model.bin\n",
            "{'loss': 1.783, 'learning_rate': 2.521948456527896e-05, 'epoch': 1.49}\n",
            " 50% 7000/14124 [09:48<07:32, 15.74it/s][INFO|trainer.py:1995] 2021-10-26 02:36:04,198 >> Saving model checkpoint to models/BarackObama/checkpoint-7000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:36:04,204 >> Configuration saved in models/BarackObama/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:36:06,483 >> Model weights saved in models/BarackObama/checkpoint-7000/pytorch_model.bin\n",
            "{'loss': 1.8171, 'learning_rate': 2.344944774851317e-05, 'epoch': 1.59}\n",
            " 53% 7500/14124 [10:32<07:06, 15.53it/s][INFO|trainer.py:1995] 2021-10-26 02:36:47,727 >> Saving model checkpoint to models/BarackObama/checkpoint-7500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:36:47,732 >> Configuration saved in models/BarackObama/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:36:49,860 >> Model weights saved in models/BarackObama/checkpoint-7500/pytorch_model.bin\n",
            "{'loss': 1.805, 'learning_rate': 2.167941093174738e-05, 'epoch': 1.7}\n",
            " 57% 8000/14124 [11:15<06:43, 15.16it/s][INFO|trainer.py:1995] 2021-10-26 02:37:31,146 >> Saving model checkpoint to models/BarackObama/checkpoint-8000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:37:31,151 >> Configuration saved in models/BarackObama/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:37:33,311 >> Model weights saved in models/BarackObama/checkpoint-8000/pytorch_model.bin\n",
            "{'loss': 1.81, 'learning_rate': 1.9909374114981594e-05, 'epoch': 1.81}\n",
            " 60% 8500/14124 [11:59<05:57, 15.71it/s][INFO|trainer.py:1995] 2021-10-26 02:38:14,888 >> Saving model checkpoint to models/BarackObama/checkpoint-8500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:38:14,895 >> Configuration saved in models/BarackObama/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:38:17,123 >> Model weights saved in models/BarackObama/checkpoint-8500/pytorch_model.bin\n",
            "{'loss': 1.8083, 'learning_rate': 1.8139337298215803e-05, 'epoch': 1.91}\n",
            " 64% 9000/14124 [12:41<05:29, 15.56it/s][INFO|trainer.py:1995] 2021-10-26 02:38:57,067 >> Saving model checkpoint to models/BarackObama/checkpoint-9000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:38:57,073 >> Configuration saved in models/BarackObama/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:38:59,512 >> Model weights saved in models/BarackObama/checkpoint-9000/pytorch_model.bin\n",
            "{'loss': 1.7524, 'learning_rate': 1.6369300481450013e-05, 'epoch': 2.02}\n",
            " 67% 9500/14124 [13:25<04:52, 15.79it/s][INFO|trainer.py:1995] 2021-10-26 02:39:41,030 >> Saving model checkpoint to models/BarackObama/checkpoint-9500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:39:41,036 >> Configuration saved in models/BarackObama/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:39:43,191 >> Model weights saved in models/BarackObama/checkpoint-9500/pytorch_model.bin\n",
            "{'loss': 1.5483, 'learning_rate': 1.4599263664684226e-05, 'epoch': 2.12}\n",
            " 71% 10000/14124 [14:09<04:24, 15.61it/s][INFO|trainer.py:1995] 2021-10-26 02:40:24,709 >> Saving model checkpoint to models/BarackObama/checkpoint-10000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:40:24,716 >> Configuration saved in models/BarackObama/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:40:26,834 >> Model weights saved in models/BarackObama/checkpoint-10000/pytorch_model.bin\n",
            "{'loss': 1.6027, 'learning_rate': 1.2829226847918437e-05, 'epoch': 2.23}\n",
            " 74% 10500/14124 [14:52<03:53, 15.50it/s][INFO|trainer.py:1995] 2021-10-26 02:41:07,944 >> Saving model checkpoint to models/BarackObama/checkpoint-10500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:41:07,950 >> Configuration saved in models/BarackObama/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:41:10,231 >> Model weights saved in models/BarackObama/checkpoint-10500/pytorch_model.bin\n",
            "{'loss': 1.6084, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}\n",
            " 78% 11000/14124 [15:36<03:25, 15.19it/s][INFO|trainer.py:1995] 2021-10-26 02:41:51,989 >> Saving model checkpoint to models/BarackObama/checkpoint-11000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:41:51,995 >> Configuration saved in models/BarackObama/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:41:54,110 >> Model weights saved in models/BarackObama/checkpoint-11000/pytorch_model.bin\n",
            "{'loss': 1.5527, 'learning_rate': 9.28915321438686e-06, 'epoch': 2.44}\n",
            " 81% 11500/14124 [16:20<02:48, 15.54it/s][INFO|trainer.py:1995] 2021-10-26 02:42:35,853 >> Saving model checkpoint to models/BarackObama/checkpoint-11500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:42:35,860 >> Configuration saved in models/BarackObama/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:42:38,040 >> Model weights saved in models/BarackObama/checkpoint-11500/pytorch_model.bin\n",
            "{'loss': 1.5679, 'learning_rate': 7.519116397621071e-06, 'epoch': 2.55}\n",
            " 85% 12000/14124 [17:02<02:17, 15.46it/s][INFO|trainer.py:1995] 2021-10-26 02:43:18,402 >> Saving model checkpoint to models/BarackObama/checkpoint-12000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:43:18,409 >> Configuration saved in models/BarackObama/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:43:20,609 >> Model weights saved in models/BarackObama/checkpoint-12000/pytorch_model.bin\n",
            "{'loss': 1.5484, 'learning_rate': 5.749079580855282e-06, 'epoch': 2.66}\n",
            " 89% 12500/14124 [17:46<01:43, 15.62it/s][INFO|trainer.py:1995] 2021-10-26 02:44:01,899 >> Saving model checkpoint to models/BarackObama/checkpoint-12500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:44:01,905 >> Configuration saved in models/BarackObama/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:44:04,330 >> Model weights saved in models/BarackObama/checkpoint-12500/pytorch_model.bin\n",
            "{'loss': 1.5762, 'learning_rate': 3.979042764089493e-06, 'epoch': 2.76}\n",
            " 92% 13000/14124 [18:29<01:11, 15.78it/s][INFO|trainer.py:1995] 2021-10-26 02:44:44,950 >> Saving model checkpoint to models/BarackObama/checkpoint-13000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:44:44,956 >> Configuration saved in models/BarackObama/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:44:47,038 >> Model weights saved in models/BarackObama/checkpoint-13000/pytorch_model.bin\n",
            "{'loss': 1.5978, 'learning_rate': 2.2090059473237042e-06, 'epoch': 2.87}\n",
            " 96% 13500/14124 [19:14<00:40, 15.31it/s][INFO|trainer.py:1995] 2021-10-26 02:45:29,581 >> Saving model checkpoint to models/BarackObama/checkpoint-13500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:45:29,587 >> Configuration saved in models/BarackObama/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:45:31,814 >> Model weights saved in models/BarackObama/checkpoint-13500/pytorch_model.bin\n",
            "{'loss': 1.5566, 'learning_rate': 4.389691305579156e-07, 'epoch': 2.97}\n",
            " 99% 14000/14124 [19:56<00:07, 15.99it/s][INFO|trainer.py:1995] 2021-10-26 02:46:12,358 >> Saving model checkpoint to models/BarackObama/checkpoint-14000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:46:12,364 >> Configuration saved in models/BarackObama/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:46:15,324 >> Model weights saved in models/BarackObama/checkpoint-14000/pytorch_model.bin\n",
            "100% 14124/14124 [20:14<00:00, 11.74it/s][INFO|trainer.py:1409] 2021-10-26 02:46:29,682 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1214.2943, 'train_samples_per_second': 23.26, 'train_steps_per_second': 11.631, 'train_loss': 1.9257499394488382, 'epoch': 3.0}\n",
            "100% 14124/14124 [20:14<00:00, 11.63it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-26 02:46:29,696 >> Saving model checkpoint to models/BarackObama\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:46:29,704 >> Configuration saved in models/BarackObama/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:46:32,352 >> Model weights saved in models/BarackObama/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-26 02:46:32,361 >> tokenizer config file saved in models/BarackObama/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-26 02:46:32,365 >> Special tokens file saved in models/BarackObama/special_tokens_map.json\n",
            "10/26/2021 02:46:32 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-10-26 02:46:32,519 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-10-26 02:46:32,519 >>   Num examples = 2690\n",
            "[INFO|trainer.py:2248] 2021-10-26 02:46:32,519 >>   Batch size = 2\n",
            "100% 1345/1345 [00:23<00:00, 58.03it/s]\n",
            "10/26/2021 02:46:55 - INFO - __main__ - ***** Eval results *****\n",
            "10/26/2021 02:46:55 - INFO - __main__ -   perplexity = 7.608226665133082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKKvXpJitYSY"
      },
      "source": [
        "# Generation"
      ],
      "id": "kKKvXpJitYSY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22bigVoOtay_"
      },
      "source": [
        "## Elon Musk"
      ],
      "id": "22bigVoOtay_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrR5jhhoY5A9",
        "outputId": "d2f87a94-fcf5-4987-8b1c-eb05c5f46b29"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type gpt2 \\\n",
        "  --model_name_or_path \"models/elonmusk\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "id": "MrR5jhhoY5A9",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/26/2021 03:01:28 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/elonmusk', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "Am excited about the Model 3 unveil and road trip this weekend. There are some huge changes coming to the design as well, so this one is easy & fast. \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "To be clear, I am against artificial intelligence. AI should not be in charge. I want it to be. \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The last remaining soul mate of the late King Arthur was actually Dragan Motavalli. He was one of the greats. \n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "My kids love  @SpaceX  and  @NASA. \n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "It's not just me: I have several cats. I also own a chicken. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdzR6cgpu8WT"
      },
      "source": [
        "## Andrej Karpathy"
      ],
      "id": "HdzR6cgpu8WT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aW_89ZCZr4S",
        "outputId": "67cfd9c9-0a19-4488-ce54-2862492eab77"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type gpt2 \\\n",
        "  --model_name_or_path \"models/karpathy\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "id": "9aW_89ZCZr4S",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/26/2021 03:01:40 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/karpathy', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "\"How to Start a Startup\" from  @OpenAI  great resource, tips & tricks! \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "A new Deep Learning tutorial for beginners:  the DeepMind GTC231n video   (w)’s A New Challenge for Computer Visionists \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The Deep Web is a Hotbed of Violence and Corruption  soooo excited! \n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "#cvpr2016  a week of hacking on the ConvNetCVPR code. Looks like I'm on a roll with the demos \n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "It's quite amusing to watch what is happening behind closed doors with no public hearings or public comment. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIqrFCqPvPit"
      },
      "source": [
        "## Barack Obama"
      ],
      "id": "OIqrFCqPvPit"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kPgVWmZZwGz",
        "outputId": "af6b68e1-0df5-49db-b185-6acd3e4dd817"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type gpt2 \\\n",
        "  --model_name_or_path \"models/BarackObama\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "id": "6kPgVWmZZwGz",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/26/2021 03:01:52 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/BarackObama', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "“They’re going to try to scare us off education entirely, because no one wants to go to college.” \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "\"That said, I know that people can still make their voices heard in Washington. I know it's not easy to do that.\" —President Obama \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The Senate must make a timely vote on an American Jobs Act now. The Jobs Act needs to be included in any deal. \n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "There's still more work to do, but this is progress. \n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "It's up to us to work with leaders in both parties to help communities build communities of their own. \n"
          ]
        }
      ]
    }
  ]
}