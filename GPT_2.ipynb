{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa9346ed",
        "outputId": "92bf4d03-459a-43db-b766-7a2520da3faa"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/NLP/project"
      ],
      "id": "fa9346ed",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/NLP/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgNtwxjIVBg6",
        "outputId": "d8ca49fd-7aae-4b4f-8177-742a1ef9eafc"
      },
      "source": [
        "%pip install transformers datasets rouge_score"
      ],
      "id": "ZgNtwxjIVBg6",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.15.1)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90b_Z2ihbfdf",
        "outputId": "98625aaa-3c3f-4ee9-9498-ac130aa3c0f4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "90b_Z2ihbfdf",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 11 22:22:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96d57222"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import GPT2LMHeadModel,  GPT2TokenizerFast, GPT2Config, DataCollatorForLanguageModeling, LineByLineTextDataset\n",
        "from transformers import AdamW, top_k_top_p_filtering\n",
        "from datasets import load_metric\n",
        "\n",
        "from tqdm import tqdm\n",
        "import nltk"
      ],
      "id": "96d57222",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQjAY_o1Q-CC"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "xQjAY_o1Q-CC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khcZrryWtTQv"
      },
      "source": [
        "# Training"
      ],
      "id": "khcZrryWtTQv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd92f735"
      },
      "source": [
        "## Elon Musk"
      ],
      "id": "fd92f735"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f7c1901",
        "outputId": "ac5dee5e-1bab-456c-eb65-79c086b02d37"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=models/elonmusk --overwrite_output_dir --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=data/elonmusk/train.txt --eval_data_file=data/elonmusk/valid.txt --do_eval --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=5"
      ],
      "id": "8f7c1901",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 02:47:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/26/2021 02:47:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/elonmusk/runs/Oct26_02-47-04_239f9388736f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "output_dir=models/elonmusk,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/elonmusk,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:47:04,782 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:47:04,783 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-26 02:47:04,877 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:47:04,970 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:47:04,971 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:47:05,661 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:47:05,756 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:47:05,757 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "[INFO|modeling_utils.py:1323] 2021-10-26 02:47:05,944 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1588] 2021-10-26 02:47:07,844 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-26 02:47:07,844 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:47:07,857 >> Creating features from dataset file at data/elonmusk/train.txt\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:47:08,326 >> Creating features from dataset file at data/elonmusk/valid.txt\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:47:12,266 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:47:12,266 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:47:12,266 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1196] 2021-10-26 02:47:12,628 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-26 02:47:12,629 >>   Num examples = 2067\n",
            "[INFO|trainer.py:1198] 2021-10-26 02:47:12,629 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1199] 2021-10-26 02:47:12,629 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-10-26 02:47:12,629 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-10-26 02:47:12,629 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-26 02:47:12,629 >>   Total optimization steps = 5170\n",
            "{'loss': 4.5065, 'learning_rate': 4.516441005802708e-05, 'epoch': 0.48}\n",
            " 10% 500/5170 [00:32<05:15, 14.81it/s][INFO|trainer.py:1995] 2021-10-26 02:47:45,565 >> Saving model checkpoint to models/elonmusk/checkpoint-500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:47:45,720 >> Configuration saved in models/elonmusk/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:47:51,734 >> Model weights saved in models/elonmusk/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 3.4529, 'learning_rate': 4.032882011605416e-05, 'epoch': 0.97}\n",
            " 19% 1000/5170 [01:29<04:26, 15.66it/s][INFO|trainer.py:1995] 2021-10-26 02:48:42,311 >> Saving model checkpoint to models/elonmusk/checkpoint-1000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:48:42,417 >> Configuration saved in models/elonmusk/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:48:49,873 >> Model weights saved in models/elonmusk/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 2.8308, 'learning_rate': 3.549323017408124e-05, 'epoch': 1.45}\n",
            " 29% 1500/5170 [02:26<04:02, 15.11it/s][INFO|trainer.py:1995] 2021-10-26 02:49:39,385 >> Saving model checkpoint to models/elonmusk/checkpoint-1500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:49:39,480 >> Configuration saved in models/elonmusk/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:49:45,697 >> Model weights saved in models/elonmusk/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 2.7859, 'learning_rate': 3.065764023210832e-05, 'epoch': 1.93}\n",
            " 39% 2000/5170 [03:21<03:34, 14.81it/s][INFO|trainer.py:1995] 2021-10-26 02:50:34,050 >> Saving model checkpoint to models/elonmusk/checkpoint-2000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:50:34,170 >> Configuration saved in models/elonmusk/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:50:40,094 >> Model weights saved in models/elonmusk/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 2.3705, 'learning_rate': 2.5822050290135396e-05, 'epoch': 2.42}\n",
            " 48% 2500/5170 [04:19<02:59, 14.87it/s][INFO|trainer.py:1995] 2021-10-26 02:51:32,013 >> Saving model checkpoint to models/elonmusk/checkpoint-2500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:51:32,121 >> Configuration saved in models/elonmusk/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:51:39,404 >> Model weights saved in models/elonmusk/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 2.3104, 'learning_rate': 2.0986460348162475e-05, 'epoch': 2.9}\n",
            " 58% 3000/5170 [05:15<02:28, 14.65it/s][INFO|trainer.py:1995] 2021-10-26 02:52:28,478 >> Saving model checkpoint to models/elonmusk/checkpoint-3000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:52:28,586 >> Configuration saved in models/elonmusk/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:52:35,006 >> Model weights saved in models/elonmusk/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.0929, 'learning_rate': 1.6150870406189557e-05, 'epoch': 3.38}\n",
            " 68% 3500/5170 [06:12<01:48, 15.39it/s][INFO|trainer.py:1995] 2021-10-26 02:53:25,415 >> Saving model checkpoint to models/elonmusk/checkpoint-3500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:53:25,512 >> Configuration saved in models/elonmusk/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:53:30,875 >> Model weights saved in models/elonmusk/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 1.9983, 'learning_rate': 1.1315280464216636e-05, 'epoch': 3.87}\n",
            " 77% 4000/5170 [07:08<01:15, 15.47it/s][INFO|trainer.py:1995] 2021-10-26 02:54:21,133 >> Saving model checkpoint to models/elonmusk/checkpoint-4000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:54:21,239 >> Configuration saved in models/elonmusk/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:54:27,247 >> Model weights saved in models/elonmusk/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 1.822, 'learning_rate': 6.479690522243714e-06, 'epoch': 4.35}\n",
            " 87% 4500/5170 [08:04<00:44, 15.16it/s][INFO|trainer.py:1995] 2021-10-26 02:55:17,510 >> Saving model checkpoint to models/elonmusk/checkpoint-4500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:55:17,619 >> Configuration saved in models/elonmusk/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:55:23,113 >> Model weights saved in models/elonmusk/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 1.8139, 'learning_rate': 1.644100580270793e-06, 'epoch': 4.84}\n",
            " 97% 5000/5170 [08:59<00:11, 14.86it/s][INFO|trainer.py:1995] 2021-10-26 02:56:11,677 >> Saving model checkpoint to models/elonmusk/checkpoint-5000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:56:11,775 >> Configuration saved in models/elonmusk/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:56:17,725 >> Model weights saved in models/elonmusk/checkpoint-5000/pytorch_model.bin\n",
            "100% 5169/5170 [09:34<00:00, 11.65it/s][INFO|trainer.py:1409] 2021-10-26 02:56:46,925 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 574.2961, 'train_samples_per_second': 17.996, 'train_steps_per_second': 9.002, 'train_loss': 2.5741099315178464, 'epoch': 5.0}\n",
            "100% 5170/5170 [09:34<00:00,  9.00it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-26 02:56:46,934 >> Saving model checkpoint to models/elonmusk\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:56:46,941 >> Configuration saved in models/elonmusk/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:56:49,510 >> Model weights saved in models/elonmusk/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-26 02:56:49,520 >> tokenizer config file saved in models/elonmusk/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-26 02:56:49,523 >> Special tokens file saved in models/elonmusk/special_tokens_map.json\n",
            "10/26/2021 02:56:49 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-10-26 02:56:49,671 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-10-26 02:56:49,671 >>   Num examples = 605\n",
            "[INFO|trainer.py:2248] 2021-10-26 02:56:49,671 >>   Batch size = 2\n",
            "100% 303/303 [00:04<00:00, 66.04it/s]\n",
            "10/26/2021 02:56:54 - INFO - __main__ - ***** Eval results *****\n",
            "10/26/2021 02:56:54 - INFO - __main__ -   perplexity = 35.93180509124075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kvLJnmU12VU"
      },
      "source": [
        "## Andrej Karpathy"
      ],
      "id": "7kvLJnmU12VU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMdpZVmZX9De",
        "outputId": "a94d99b9-45b2-4d1b-aa39-193488c55ca2"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=models/karpathy --overwrite_output_dir --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=data/karpathy/train.txt --eval_data_file=data/karpathy/valid.txt --do_eval --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=5"
      ],
      "id": "RMdpZVmZX9De",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 02:16:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/26/2021 02:16:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/karpathy/runs/Oct26_02-16-07_239f9388736f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "output_dir=models/karpathy,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/karpathy,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:16:07,216 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:16:07,217 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-26 02:16:07,308 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:16:07,402 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:16:07,403 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:16:08,068 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:16:08,161 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:16:08,162 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "[INFO|modeling_utils.py:1323] 2021-10-26 02:16:08,349 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1588] 2021-10-26 02:16:10,236 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-26 02:16:10,237 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:16:10,248 >> Creating features from dataset file at data/karpathy/train.txt\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:16:10,498 >> Creating features from dataset file at data/karpathy/valid.txt\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:16:14,334 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:16:14,334 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:16:14,334 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1196] 2021-10-26 02:16:14,682 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-26 02:16:14,682 >>   Num examples = 2672\n",
            "[INFO|trainer.py:1198] 2021-10-26 02:16:14,682 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1199] 2021-10-26 02:16:14,682 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-10-26 02:16:14,682 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-10-26 02:16:14,682 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-26 02:16:14,682 >>   Total optimization steps = 6680\n",
            "{'loss': 4.88, 'learning_rate': 4.625748502994012e-05, 'epoch': 0.37}\n",
            "  7% 500/6680 [00:32<06:41, 15.38it/s][INFO|trainer.py:1995] 2021-10-26 02:16:47,334 >> Saving model checkpoint to models/karpathy/checkpoint-500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:16:47,340 >> Configuration saved in models/karpathy/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:16:49,345 >> Model weights saved in models/karpathy/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 3.9949, 'learning_rate': 4.251497005988024e-05, 'epoch': 0.75}\n",
            " 15% 1000/6680 [01:14<06:03, 15.62it/s][INFO|trainer.py:1995] 2021-10-26 02:17:29,559 >> Saving model checkpoint to models/karpathy/checkpoint-1000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:17:29,565 >> Configuration saved in models/karpathy/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:17:31,561 >> Model weights saved in models/karpathy/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 3.6761, 'learning_rate': 3.877245508982036e-05, 'epoch': 1.12}\n",
            " 22% 1500/6680 [01:58<05:37, 15.34it/s][INFO|trainer.py:1995] 2021-10-26 02:18:12,880 >> Saving model checkpoint to models/karpathy/checkpoint-1500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:18:12,886 >> Configuration saved in models/karpathy/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:18:14,997 >> Model weights saved in models/karpathy/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 3.3552, 'learning_rate': 3.502994011976048e-05, 'epoch': 1.5}\n",
            " 30% 2000/6680 [02:40<05:10, 15.05it/s][INFO|trainer.py:1995] 2021-10-26 02:18:55,650 >> Saving model checkpoint to models/karpathy/checkpoint-2000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:18:55,655 >> Configuration saved in models/karpathy/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:18:57,663 >> Model weights saved in models/karpathy/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 3.2824, 'learning_rate': 3.12874251497006e-05, 'epoch': 1.87}\n",
            " 37% 2500/6680 [03:23<04:33, 15.26it/s][INFO|trainer.py:1995] 2021-10-26 02:19:38,536 >> Saving model checkpoint to models/karpathy/checkpoint-2500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:19:38,543 >> Configuration saved in models/karpathy/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:19:41,610 >> Model weights saved in models/karpathy/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 3.0354, 'learning_rate': 2.754491017964072e-05, 'epoch': 2.25}\n",
            " 45% 3000/6680 [04:07<04:01, 15.21it/s][INFO|trainer.py:1995] 2021-10-26 02:20:22,090 >> Saving model checkpoint to models/karpathy/checkpoint-3000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:20:22,095 >> Configuration saved in models/karpathy/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:20:24,224 >> Model weights saved in models/karpathy/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.904, 'learning_rate': 2.3802395209580838e-05, 'epoch': 2.62}\n",
            " 52% 3500/6680 [04:51<03:26, 15.40it/s][INFO|trainer.py:1995] 2021-10-26 02:21:05,753 >> Saving model checkpoint to models/karpathy/checkpoint-3500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:21:05,760 >> Configuration saved in models/karpathy/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:21:07,857 >> Model weights saved in models/karpathy/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 2.9375, 'learning_rate': 2.0059880239520957e-05, 'epoch': 2.99}\n",
            " 60% 4000/6680 [05:33<02:59, 14.92it/s][INFO|trainer.py:1995] 2021-10-26 02:21:48,541 >> Saving model checkpoint to models/karpathy/checkpoint-4000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:21:48,547 >> Configuration saved in models/karpathy/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:21:50,678 >> Model weights saved in models/karpathy/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 2.5919, 'learning_rate': 1.631736526946108e-05, 'epoch': 3.37}\n",
            " 67% 4500/6680 [06:16<02:20, 15.54it/s][INFO|trainer.py:1995] 2021-10-26 02:22:31,583 >> Saving model checkpoint to models/karpathy/checkpoint-4500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:22:31,589 >> Configuration saved in models/karpathy/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:22:33,756 >> Model weights saved in models/karpathy/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 2.6278, 'learning_rate': 1.2574850299401197e-05, 'epoch': 3.74}\n",
            " 75% 5000/6680 [06:59<01:50, 15.14it/s][INFO|trainer.py:1995] 2021-10-26 02:23:14,631 >> Saving model checkpoint to models/karpathy/checkpoint-5000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:23:14,638 >> Configuration saved in models/karpathy/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:23:16,762 >> Model weights saved in models/karpathy/checkpoint-5000/pytorch_model.bin\n",
            "{'loss': 2.6039, 'learning_rate': 8.832335329341319e-06, 'epoch': 4.12}\n",
            " 82% 5500/6680 [07:43<01:14, 15.80it/s][INFO|trainer.py:1995] 2021-10-26 02:23:58,630 >> Saving model checkpoint to models/karpathy/checkpoint-5500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:23:58,636 >> Configuration saved in models/karpathy/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:24:00,781 >> Model weights saved in models/karpathy/checkpoint-5500/pytorch_model.bin\n",
            "{'loss': 2.4336, 'learning_rate': 5.0898203592814375e-06, 'epoch': 4.49}\n",
            " 90% 6000/6680 [08:26<00:44, 15.31it/s][INFO|trainer.py:1995] 2021-10-26 02:24:41,327 >> Saving model checkpoint to models/karpathy/checkpoint-6000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:24:41,333 >> Configuration saved in models/karpathy/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:24:43,509 >> Model weights saved in models/karpathy/checkpoint-6000/pytorch_model.bin\n",
            "{'loss': 2.4228, 'learning_rate': 1.3473053892215569e-06, 'epoch': 4.87}\n",
            " 97% 6500/6680 [09:10<00:11, 15.38it/s][INFO|trainer.py:1995] 2021-10-26 02:25:24,848 >> Saving model checkpoint to models/karpathy/checkpoint-6500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:25:24,854 >> Configuration saved in models/karpathy/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:25:27,109 >> Model weights saved in models/karpathy/checkpoint-6500/pytorch_model.bin\n",
            "100% 6679/6680 [09:31<00:00, 12.63it/s][INFO|trainer.py:1409] 2021-10-26 02:25:45,982 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 571.3001, 'train_samples_per_second': 23.385, 'train_steps_per_second': 11.693, 'train_loss': 3.115300504033437, 'epoch': 5.0}\n",
            "100% 6680/6680 [09:31<00:00, 11.69it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-26 02:25:46,195 >> Saving model checkpoint to models/karpathy\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:25:46,201 >> Configuration saved in models/karpathy/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:25:48,700 >> Model weights saved in models/karpathy/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-26 02:25:48,709 >> tokenizer config file saved in models/karpathy/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-26 02:25:48,713 >> Special tokens file saved in models/karpathy/special_tokens_map.json\n",
            "10/26/2021 02:25:49 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-10-26 02:25:49,193 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-10-26 02:25:49,193 >>   Num examples = 763\n",
            "[INFO|trainer.py:2248] 2021-10-26 02:25:49,194 >>   Batch size = 2\n",
            "100% 382/382 [00:06<00:00, 57.84it/s]\n",
            "10/26/2021 02:25:55 - INFO - __main__ - ***** Eval results *****\n",
            "10/26/2021 02:25:55 - INFO - __main__ -   perplexity = 57.795531818144575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvp55vqy2N8V"
      },
      "source": [
        "## Barack Obama"
      ],
      "id": "Uvp55vqy2N8V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNrb23NFYsKD",
        "outputId": "e889711f-819d-4096-dcb0-2f44bed807c1"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=models/BarackObama --overwrite_output_dir --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=data/BarackObama/train.txt --eval_data_file=data/BarackObama/valid.txt --do_eval --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=3"
      ],
      "id": "oNrb23NFYsKD",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 02:26:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/26/2021 02:26:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/BarackObama/runs/Oct26_02-26-06_239f9388736f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=models/BarackObama,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/BarackObama,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:26:06,767 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:26:06,768 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-26 02:26:06,862 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:26:06,961 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:26:06,961 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 02:26:07,791 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 02:26:07,885 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 02:26:07,886 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "[INFO|modeling_utils.py:1323] 2021-10-26 02:26:08,071 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1588] 2021-10-26 02:26:09,992 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-26 02:26:09,992 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:26:10,004 >> Creating features from dataset file at data/BarackObama/train.txt\n",
            "[INFO|language_modeling.py:130] 2021-10-26 02:26:11,030 >> Creating features from dataset file at data/BarackObama/valid.txt\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:26:15,038 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:26:15,038 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 02:26:15,038 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1196] 2021-10-26 02:26:15,387 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-26 02:26:15,387 >>   Num examples = 9415\n",
            "[INFO|trainer.py:1198] 2021-10-26 02:26:15,388 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-10-26 02:26:15,388 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-10-26 02:26:15,388 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-10-26 02:26:15,388 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-26 02:26:15,388 >>   Total optimization steps = 14124\n",
            "{'loss': 3.4845, 'learning_rate': 4.822996318323421e-05, 'epoch': 0.11}\n",
            "  4% 500/14124 [00:32<14:32, 15.61it/s][INFO|trainer.py:1995] 2021-10-26 02:26:47,721 >> Saving model checkpoint to models/BarackObama/checkpoint-500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:26:47,726 >> Configuration saved in models/BarackObama/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:26:49,886 >> Model weights saved in models/BarackObama/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 2.4585, 'learning_rate': 4.645992636646843e-05, 'epoch': 0.21}\n",
            "  7% 1000/14124 [01:14<14:06, 15.51it/s][INFO|trainer.py:1995] 2021-10-26 02:27:30,136 >> Saving model checkpoint to models/BarackObama/checkpoint-1000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:27:30,143 >> Configuration saved in models/BarackObama/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:27:32,238 >> Model weights saved in models/BarackObama/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 2.3968, 'learning_rate': 4.4689889549702635e-05, 'epoch': 0.32}\n",
            " 11% 1500/14124 [01:57<13:48, 15.24it/s][INFO|trainer.py:1995] 2021-10-26 02:28:13,005 >> Saving model checkpoint to models/BarackObama/checkpoint-1500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:28:13,011 >> Configuration saved in models/BarackObama/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:28:15,065 >> Model weights saved in models/BarackObama/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 2.3066, 'learning_rate': 4.291985273293685e-05, 'epoch': 0.42}\n",
            " 14% 2000/14124 [02:39<13:34, 14.89it/s][INFO|trainer.py:1995] 2021-10-26 02:28:55,060 >> Saving model checkpoint to models/BarackObama/checkpoint-2000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:28:55,075 >> Configuration saved in models/BarackObama/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:28:57,163 >> Model weights saved in models/BarackObama/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 2.2558, 'learning_rate': 4.114981591617106e-05, 'epoch': 0.53}\n",
            " 18% 2500/14124 [03:21<12:19, 15.71it/s][INFO|trainer.py:1995] 2021-10-26 02:29:37,274 >> Saving model checkpoint to models/BarackObama/checkpoint-2500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:29:37,279 >> Configuration saved in models/BarackObama/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:29:39,261 >> Model weights saved in models/BarackObama/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 2.227, 'learning_rate': 3.937977909940527e-05, 'epoch': 0.64}\n",
            " 21% 3000/14124 [04:05<12:33, 14.77it/s][INFO|trainer.py:1995] 2021-10-26 02:30:20,449 >> Saving model checkpoint to models/BarackObama/checkpoint-3000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:30:20,455 >> Configuration saved in models/BarackObama/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:30:22,505 >> Model weights saved in models/BarackObama/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.1996, 'learning_rate': 3.760974228263948e-05, 'epoch': 0.74}\n",
            " 25% 3500/14124 [04:47<11:33, 15.31it/s][INFO|trainer.py:1995] 2021-10-26 02:31:02,797 >> Saving model checkpoint to models/BarackObama/checkpoint-3500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:31:02,804 >> Configuration saved in models/BarackObama/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:31:04,841 >> Model weights saved in models/BarackObama/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 2.1712, 'learning_rate': 3.583970546587369e-05, 'epoch': 0.85}\n",
            " 28% 4000/14124 [05:29<10:51, 15.54it/s][INFO|trainer.py:1995] 2021-10-26 02:31:44,942 >> Saving model checkpoint to models/BarackObama/checkpoint-4000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:31:44,948 >> Configuration saved in models/BarackObama/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:31:46,968 >> Model weights saved in models/BarackObama/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 2.1557, 'learning_rate': 3.4069668649107906e-05, 'epoch': 0.96}\n",
            " 32% 4500/14124 [06:13<10:16, 15.60it/s][INFO|trainer.py:1995] 2021-10-26 02:32:28,427 >> Saving model checkpoint to models/BarackObama/checkpoint-4500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:32:28,432 >> Configuration saved in models/BarackObama/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:32:30,395 >> Model weights saved in models/BarackObama/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 1.983, 'learning_rate': 3.229963183234211e-05, 'epoch': 1.06}\n",
            " 35% 5000/14124 [06:56<09:58, 15.25it/s][INFO|trainer.py:1995] 2021-10-26 02:33:11,566 >> Saving model checkpoint to models/BarackObama/checkpoint-5000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:33:11,571 >> Configuration saved in models/BarackObama/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:33:13,662 >> Model weights saved in models/BarackObama/checkpoint-5000/pytorch_model.bin\n",
            "{'loss': 1.811, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}\n",
            " 39% 5500/14124 [07:39<09:14, 15.55it/s][INFO|trainer.py:1995] 2021-10-26 02:33:54,627 >> Saving model checkpoint to models/BarackObama/checkpoint-5500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:33:54,633 >> Configuration saved in models/BarackObama/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:33:56,804 >> Model weights saved in models/BarackObama/checkpoint-5500/pytorch_model.bin\n",
            "{'loss': 1.7891, 'learning_rate': 2.875955819881054e-05, 'epoch': 1.27}\n",
            " 42% 6000/14124 [08:22<08:41, 15.57it/s][INFO|trainer.py:1995] 2021-10-26 02:34:37,620 >> Saving model checkpoint to models/BarackObama/checkpoint-6000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:34:37,626 >> Configuration saved in models/BarackObama/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:34:39,797 >> Model weights saved in models/BarackObama/checkpoint-6000/pytorch_model.bin\n",
            "{'loss': 1.8294, 'learning_rate': 2.6989521382044748e-05, 'epoch': 1.38}\n",
            " 46% 6500/14124 [09:05<08:13, 15.46it/s][INFO|trainer.py:1995] 2021-10-26 02:35:20,768 >> Saving model checkpoint to models/BarackObama/checkpoint-6500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:35:20,773 >> Configuration saved in models/BarackObama/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:35:22,956 >> Model weights saved in models/BarackObama/checkpoint-6500/pytorch_model.bin\n",
            "{'loss': 1.783, 'learning_rate': 2.521948456527896e-05, 'epoch': 1.49}\n",
            " 50% 7000/14124 [09:48<07:32, 15.74it/s][INFO|trainer.py:1995] 2021-10-26 02:36:04,198 >> Saving model checkpoint to models/BarackObama/checkpoint-7000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:36:04,204 >> Configuration saved in models/BarackObama/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:36:06,483 >> Model weights saved in models/BarackObama/checkpoint-7000/pytorch_model.bin\n",
            "{'loss': 1.8171, 'learning_rate': 2.344944774851317e-05, 'epoch': 1.59}\n",
            " 53% 7500/14124 [10:32<07:06, 15.53it/s][INFO|trainer.py:1995] 2021-10-26 02:36:47,727 >> Saving model checkpoint to models/BarackObama/checkpoint-7500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:36:47,732 >> Configuration saved in models/BarackObama/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:36:49,860 >> Model weights saved in models/BarackObama/checkpoint-7500/pytorch_model.bin\n",
            "{'loss': 1.805, 'learning_rate': 2.167941093174738e-05, 'epoch': 1.7}\n",
            " 57% 8000/14124 [11:15<06:43, 15.16it/s][INFO|trainer.py:1995] 2021-10-26 02:37:31,146 >> Saving model checkpoint to models/BarackObama/checkpoint-8000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:37:31,151 >> Configuration saved in models/BarackObama/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:37:33,311 >> Model weights saved in models/BarackObama/checkpoint-8000/pytorch_model.bin\n",
            "{'loss': 1.81, 'learning_rate': 1.9909374114981594e-05, 'epoch': 1.81}\n",
            " 60% 8500/14124 [11:59<05:57, 15.71it/s][INFO|trainer.py:1995] 2021-10-26 02:38:14,888 >> Saving model checkpoint to models/BarackObama/checkpoint-8500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:38:14,895 >> Configuration saved in models/BarackObama/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:38:17,123 >> Model weights saved in models/BarackObama/checkpoint-8500/pytorch_model.bin\n",
            "{'loss': 1.8083, 'learning_rate': 1.8139337298215803e-05, 'epoch': 1.91}\n",
            " 64% 9000/14124 [12:41<05:29, 15.56it/s][INFO|trainer.py:1995] 2021-10-26 02:38:57,067 >> Saving model checkpoint to models/BarackObama/checkpoint-9000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:38:57,073 >> Configuration saved in models/BarackObama/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:38:59,512 >> Model weights saved in models/BarackObama/checkpoint-9000/pytorch_model.bin\n",
            "{'loss': 1.7524, 'learning_rate': 1.6369300481450013e-05, 'epoch': 2.02}\n",
            " 67% 9500/14124 [13:25<04:52, 15.79it/s][INFO|trainer.py:1995] 2021-10-26 02:39:41,030 >> Saving model checkpoint to models/BarackObama/checkpoint-9500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:39:41,036 >> Configuration saved in models/BarackObama/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:39:43,191 >> Model weights saved in models/BarackObama/checkpoint-9500/pytorch_model.bin\n",
            "{'loss': 1.5483, 'learning_rate': 1.4599263664684226e-05, 'epoch': 2.12}\n",
            " 71% 10000/14124 [14:09<04:24, 15.61it/s][INFO|trainer.py:1995] 2021-10-26 02:40:24,709 >> Saving model checkpoint to models/BarackObama/checkpoint-10000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:40:24,716 >> Configuration saved in models/BarackObama/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:40:26,834 >> Model weights saved in models/BarackObama/checkpoint-10000/pytorch_model.bin\n",
            "{'loss': 1.6027, 'learning_rate': 1.2829226847918437e-05, 'epoch': 2.23}\n",
            " 74% 10500/14124 [14:52<03:53, 15.50it/s][INFO|trainer.py:1995] 2021-10-26 02:41:07,944 >> Saving model checkpoint to models/BarackObama/checkpoint-10500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:41:07,950 >> Configuration saved in models/BarackObama/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:41:10,231 >> Model weights saved in models/BarackObama/checkpoint-10500/pytorch_model.bin\n",
            "{'loss': 1.6084, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}\n",
            " 78% 11000/14124 [15:36<03:25, 15.19it/s][INFO|trainer.py:1995] 2021-10-26 02:41:51,989 >> Saving model checkpoint to models/BarackObama/checkpoint-11000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:41:51,995 >> Configuration saved in models/BarackObama/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:41:54,110 >> Model weights saved in models/BarackObama/checkpoint-11000/pytorch_model.bin\n",
            "{'loss': 1.5527, 'learning_rate': 9.28915321438686e-06, 'epoch': 2.44}\n",
            " 81% 11500/14124 [16:20<02:48, 15.54it/s][INFO|trainer.py:1995] 2021-10-26 02:42:35,853 >> Saving model checkpoint to models/BarackObama/checkpoint-11500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:42:35,860 >> Configuration saved in models/BarackObama/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:42:38,040 >> Model weights saved in models/BarackObama/checkpoint-11500/pytorch_model.bin\n",
            "{'loss': 1.5679, 'learning_rate': 7.519116397621071e-06, 'epoch': 2.55}\n",
            " 85% 12000/14124 [17:02<02:17, 15.46it/s][INFO|trainer.py:1995] 2021-10-26 02:43:18,402 >> Saving model checkpoint to models/BarackObama/checkpoint-12000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:43:18,409 >> Configuration saved in models/BarackObama/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:43:20,609 >> Model weights saved in models/BarackObama/checkpoint-12000/pytorch_model.bin\n",
            "{'loss': 1.5484, 'learning_rate': 5.749079580855282e-06, 'epoch': 2.66}\n",
            " 89% 12500/14124 [17:46<01:43, 15.62it/s][INFO|trainer.py:1995] 2021-10-26 02:44:01,899 >> Saving model checkpoint to models/BarackObama/checkpoint-12500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:44:01,905 >> Configuration saved in models/BarackObama/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:44:04,330 >> Model weights saved in models/BarackObama/checkpoint-12500/pytorch_model.bin\n",
            "{'loss': 1.5762, 'learning_rate': 3.979042764089493e-06, 'epoch': 2.76}\n",
            " 92% 13000/14124 [18:29<01:11, 15.78it/s][INFO|trainer.py:1995] 2021-10-26 02:44:44,950 >> Saving model checkpoint to models/BarackObama/checkpoint-13000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:44:44,956 >> Configuration saved in models/BarackObama/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:44:47,038 >> Model weights saved in models/BarackObama/checkpoint-13000/pytorch_model.bin\n",
            "{'loss': 1.5978, 'learning_rate': 2.2090059473237042e-06, 'epoch': 2.87}\n",
            " 96% 13500/14124 [19:14<00:40, 15.31it/s][INFO|trainer.py:1995] 2021-10-26 02:45:29,581 >> Saving model checkpoint to models/BarackObama/checkpoint-13500\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:45:29,587 >> Configuration saved in models/BarackObama/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:45:31,814 >> Model weights saved in models/BarackObama/checkpoint-13500/pytorch_model.bin\n",
            "{'loss': 1.5566, 'learning_rate': 4.389691305579156e-07, 'epoch': 2.97}\n",
            " 99% 14000/14124 [19:56<00:07, 15.99it/s][INFO|trainer.py:1995] 2021-10-26 02:46:12,358 >> Saving model checkpoint to models/BarackObama/checkpoint-14000\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:46:12,364 >> Configuration saved in models/BarackObama/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:46:15,324 >> Model weights saved in models/BarackObama/checkpoint-14000/pytorch_model.bin\n",
            "100% 14124/14124 [20:14<00:00, 11.74it/s][INFO|trainer.py:1409] 2021-10-26 02:46:29,682 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1214.2943, 'train_samples_per_second': 23.26, 'train_steps_per_second': 11.631, 'train_loss': 1.9257499394488382, 'epoch': 3.0}\n",
            "100% 14124/14124 [20:14<00:00, 11.63it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-26 02:46:29,696 >> Saving model checkpoint to models/BarackObama\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 02:46:29,704 >> Configuration saved in models/BarackObama/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 02:46:32,352 >> Model weights saved in models/BarackObama/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-26 02:46:32,361 >> tokenizer config file saved in models/BarackObama/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-26 02:46:32,365 >> Special tokens file saved in models/BarackObama/special_tokens_map.json\n",
            "10/26/2021 02:46:32 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-10-26 02:46:32,519 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-10-26 02:46:32,519 >>   Num examples = 2690\n",
            "[INFO|trainer.py:2248] 2021-10-26 02:46:32,519 >>   Batch size = 2\n",
            "100% 1345/1345 [00:23<00:00, 58.03it/s]\n",
            "10/26/2021 02:46:55 - INFO - __main__ - ***** Eval results *****\n",
            "10/26/2021 02:46:55 - INFO - __main__ -   perplexity = 7.608226665133082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iXKpwZ9msHv"
      },
      "source": [
        "## WSJ"
      ],
      "id": "6iXKpwZ9msHv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwNnLGSyl_Bs",
        "outputId": "0873246b-bfe7-4fac-d110-e8151ef27fb4"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=models/wsj --overwrite_output_dir --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=data/wsj/train.txt --eval_data_file=data/wsj/valid.txt --do_eval --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=2 --save_strategy=epoch"
      ],
      "id": "PwNnLGSyl_Bs",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 03:16:38 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/26/2021 03:16:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/wsj/runs/Oct26_03-16-38_239f9388736f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "output_dir=models/wsj,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/wsj,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 03:16:39,082 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 03:16:39,083 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-26 03:16:39,176 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 03:16:39,268 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 03:16:39,269 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:16:39,933 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:16:39,934 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:16:39,934 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:16:39,934 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:16:39,934 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:16:39,934 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 03:16:40,025 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 03:16:40,026 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "[INFO|modeling_utils.py:1323] 2021-10-26 03:16:40,224 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1588] 2021-10-26 03:16:48,525 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-26 03:16:48,525 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:130] 2021-10-26 03:16:48,691 >> Creating features from dataset file at data/wsj/train.txt\n",
            "[INFO|language_modeling.py:130] 2021-10-26 03:16:50,308 >> Creating features from dataset file at data/wsj/valid.txt\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 03:16:54,718 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 03:16:54,719 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 03:16:54,719 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1196] 2021-10-26 03:16:55,081 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-26 03:16:55,081 >>   Num examples = 20645\n",
            "[INFO|trainer.py:1198] 2021-10-26 03:16:55,081 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1199] 2021-10-26 03:16:55,081 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-10-26 03:16:55,082 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-10-26 03:16:55,082 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-26 03:16:55,082 >>   Total optimization steps = 20646\n",
            "{'loss': 4.1196, 'learning_rate': 4.8789111692337504e-05, 'epoch': 0.05}\n",
            "{'loss': 3.1997, 'learning_rate': 4.7578223384675e-05, 'epoch': 0.1}\n",
            "{'loss': 3.1738, 'learning_rate': 4.6367335077012495e-05, 'epoch': 0.15}\n",
            "{'loss': 3.1136, 'learning_rate': 4.5156446769349996e-05, 'epoch': 0.19}\n",
            "{'loss': 3.11, 'learning_rate': 4.39455584616875e-05, 'epoch': 0.24}\n",
            "{'loss': 3.104, 'learning_rate': 4.2734670154025e-05, 'epoch': 0.29}\n",
            "{'loss': 3.0556, 'learning_rate': 4.152378184636249e-05, 'epoch': 0.34}\n",
            "{'loss': 3.0135, 'learning_rate': 4.031289353869999e-05, 'epoch': 0.39}\n",
            "{'loss': 3.046, 'learning_rate': 3.910200523103749e-05, 'epoch': 0.44}\n",
            "{'loss': 3.0353, 'learning_rate': 3.7891116923374994e-05, 'epoch': 0.48}\n",
            "{'loss': 2.9826, 'learning_rate': 3.668022861571249e-05, 'epoch': 0.53}\n",
            "{'loss': 2.9752, 'learning_rate': 3.5469340308049985e-05, 'epoch': 0.58}\n",
            "{'loss': 3.0194, 'learning_rate': 3.4258452000387486e-05, 'epoch': 0.63}\n",
            "{'loss': 3.0033, 'learning_rate': 3.304756369272499e-05, 'epoch': 0.68}\n",
            "{'loss': 2.9832, 'learning_rate': 3.1836675385062484e-05, 'epoch': 0.73}\n",
            "{'loss': 2.9807, 'learning_rate': 3.062578707739998e-05, 'epoch': 0.77}\n",
            "{'loss': 2.974, 'learning_rate': 2.941489876973748e-05, 'epoch': 0.82}\n",
            "{'loss': 2.968, 'learning_rate': 2.8204010462074982e-05, 'epoch': 0.87}\n",
            "{'loss': 2.9074, 'learning_rate': 2.6993122154412474e-05, 'epoch': 0.92}\n",
            "{'loss': 2.9287, 'learning_rate': 2.5782233846749976e-05, 'epoch': 0.97}\n",
            " 50% 10322/20646 [10:58<11:12, 15.36it/s][INFO|trainer.py:1995] 2021-10-26 03:27:53,887 >> Saving model checkpoint to models/wsj/checkpoint-10323\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 03:27:53,893 >> Configuration saved in models/wsj/checkpoint-10323/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 03:27:56,207 >> Model weights saved in models/wsj/checkpoint-10323/pytorch_model.bin\n",
            "{'loss': 2.7971, 'learning_rate': 2.4571345539087475e-05, 'epoch': 1.02}\n",
            "{'loss': 2.5176, 'learning_rate': 2.3360457231424973e-05, 'epoch': 1.07}\n",
            "{'loss': 2.5095, 'learning_rate': 2.214956892376247e-05, 'epoch': 1.11}\n",
            "{'loss': 2.5899, 'learning_rate': 2.0938680616099974e-05, 'epoch': 1.16}\n",
            "{'loss': 2.5328, 'learning_rate': 1.9727792308437472e-05, 'epoch': 1.21}\n",
            "{'loss': 2.5264, 'learning_rate': 1.851690400077497e-05, 'epoch': 1.26}\n",
            "{'loss': 2.5618, 'learning_rate': 1.730601569311247e-05, 'epoch': 1.31}\n",
            "{'loss': 2.5573, 'learning_rate': 1.6095127385449968e-05, 'epoch': 1.36}\n",
            "{'loss': 2.5464, 'learning_rate': 1.4884239077787464e-05, 'epoch': 1.4}\n",
            "{'loss': 2.5305, 'learning_rate': 1.3673350770124965e-05, 'epoch': 1.45}\n",
            "{'loss': 2.522, 'learning_rate': 1.2462462462462463e-05, 'epoch': 1.5}\n",
            "{'loss': 2.5239, 'learning_rate': 1.1251574154799962e-05, 'epoch': 1.55}\n",
            "{'loss': 2.5799, 'learning_rate': 1.004068584713746e-05, 'epoch': 1.6}\n",
            "{'loss': 2.5355, 'learning_rate': 8.82979753947496e-06, 'epoch': 1.65}\n",
            "{'loss': 2.5051, 'learning_rate': 7.618909231812459e-06, 'epoch': 1.7}\n",
            "{'loss': 2.5522, 'learning_rate': 6.408020924149957e-06, 'epoch': 1.74}\n",
            "{'loss': 2.53, 'learning_rate': 5.197132616487455e-06, 'epoch': 1.79}\n",
            "{'loss': 2.5193, 'learning_rate': 3.986244308824954e-06, 'epoch': 1.84}\n",
            "{'loss': 2.5447, 'learning_rate': 2.775356001162453e-06, 'epoch': 1.89}\n",
            "{'loss': 2.5192, 'learning_rate': 1.5644676934999515e-06, 'epoch': 1.94}\n",
            "{'loss': 2.5439, 'learning_rate': 3.5357938583745037e-07, 'epoch': 1.99}\n",
            "100% 20645/20646 [22:09<00:00, 15.64it/s][INFO|trainer.py:1995] 2021-10-26 03:39:05,073 >> Saving model checkpoint to models/wsj/checkpoint-20646\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 03:39:05,079 >> Configuration saved in models/wsj/checkpoint-20646/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 03:39:07,436 >> Model weights saved in models/wsj/checkpoint-20646/pytorch_model.bin\n",
            "[INFO|trainer.py:1409] 2021-10-26 03:39:13,881 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1338.7993, 'train_samples_per_second': 30.841, 'train_steps_per_second': 15.421, 'train_loss': 2.8086351584778453, 'epoch': 2.0}\n",
            "100% 20646/20646 [22:18<00:00, 15.42it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-26 03:39:13,885 >> Saving model checkpoint to models/wsj\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 03:39:13,889 >> Configuration saved in models/wsj/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 03:39:16,566 >> Model weights saved in models/wsj/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-26 03:39:16,579 >> tokenizer config file saved in models/wsj/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-26 03:39:16,582 >> Special tokens file saved in models/wsj/special_tokens_map.json\n",
            "10/26/2021 03:39:16 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-10-26 03:39:16,760 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-10-26 03:39:16,760 >>   Num examples = 5900\n",
            "[INFO|trainer.py:2248] 2021-10-26 03:39:16,760 >>   Batch size = 2\n",
            "100% 2950/2950 [00:47<00:00, 61.89it/s]\n",
            "10/26/2021 03:40:04 - INFO - __main__ - ***** Eval results *****\n",
            "10/26/2021 03:40:04 - INFO - __main__ -   perplexity = 16.970271849356735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2VYLXzKmuE5"
      },
      "source": [
        "## CNN"
      ],
      "id": "c2VYLXzKmuE5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A5lCpa5mvQF",
        "outputId": "15889dad-19ae-4892-97a2-e0a36c9e6e01"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=models/cnn --overwrite_output_dir --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=data/cnn/train.txt --eval_data_file=data/cnn/valid.txt --do_eval --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --line_by_line --learning_rate=5e-5 --num_train_epochs=2 --save_strategy=epoch"
      ],
      "id": "8A5lCpa5mvQF",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 03:40:09 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/26/2021 03:40:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/cnn/runs/Oct26_03-40-09_239f9388736f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "output_dir=models/cnn,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/cnn,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 03:40:09,846 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 03:40:09,847 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-26 03:40:09,938 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 03:40:10,099 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 03:40:10,100 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:40:11,161 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:40:11,161 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:40:11,161 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:40:11,161 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:40:11,161 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-26 03:40:11,161 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-26 03:40:11,320 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:620] 2021-10-26 03:40:11,321 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "[INFO|modeling_utils.py:1323] 2021-10-26 03:40:11,570 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1588] 2021-10-26 03:40:13,515 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-26 03:40:13,515 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:130] 2021-10-26 03:40:13,697 >> Creating features from dataset file at data/cnn/train.txt\n",
            "[INFO|language_modeling.py:130] 2021-10-26 03:40:15,289 >> Creating features from dataset file at data/cnn/valid.txt\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 03:40:19,729 >> Assigning <BOS> to the bos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 03:40:19,730 >> Assigning <EOS> to the eos_token key of the tokenizer\n",
            "[INFO|tokenization_utils_base.py:888] 2021-10-26 03:40:19,730 >> Assigning <PAD> to the pad_token key of the tokenizer\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1054: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1196] 2021-10-26 03:40:20,075 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-26 03:40:20,075 >>   Num examples = 21868\n",
            "[INFO|trainer.py:1198] 2021-10-26 03:40:20,075 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1199] 2021-10-26 03:40:20,075 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-10-26 03:40:20,076 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-10-26 03:40:20,076 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-26 03:40:20,076 >>   Total optimization steps = 21868\n",
            "{'loss': 4.0857, 'learning_rate': 4.885677702579111e-05, 'epoch': 0.05}\n",
            "{'loss': 3.1725, 'learning_rate': 4.7713554051582224e-05, 'epoch': 0.09}\n",
            "{'loss': 3.1187, 'learning_rate': 4.657033107737334e-05, 'epoch': 0.14}\n",
            "{'loss': 3.073, 'learning_rate': 4.5427108103164445e-05, 'epoch': 0.18}\n",
            "{'loss': 3.0674, 'learning_rate': 4.428388512895555e-05, 'epoch': 0.23}\n",
            "{'loss': 3.0045, 'learning_rate': 4.3140662154746666e-05, 'epoch': 0.27}\n",
            "{'loss': 2.9758, 'learning_rate': 4.1997439180537773e-05, 'epoch': 0.32}\n",
            "{'loss': 2.9982, 'learning_rate': 4.085421620632888e-05, 'epoch': 0.37}\n",
            "{'loss': 2.9529, 'learning_rate': 3.9710993232119995e-05, 'epoch': 0.41}\n",
            "{'loss': 2.9375, 'learning_rate': 3.856777025791111e-05, 'epoch': 0.46}\n",
            "{'loss': 2.967, 'learning_rate': 3.7424547283702216e-05, 'epoch': 0.5}\n",
            "{'loss': 2.9477, 'learning_rate': 3.628132430949332e-05, 'epoch': 0.55}\n",
            "{'loss': 2.9012, 'learning_rate': 3.513810133528444e-05, 'epoch': 0.59}\n",
            "{'loss': 2.9031, 'learning_rate': 3.3994878361075544e-05, 'epoch': 0.64}\n",
            "{'loss': 2.9166, 'learning_rate': 3.285165538686666e-05, 'epoch': 0.69}\n",
            "{'loss': 2.9151, 'learning_rate': 3.1708432412657766e-05, 'epoch': 0.73}\n",
            "{'loss': 2.8802, 'learning_rate': 3.056520943844888e-05, 'epoch': 0.78}\n",
            "{'loss': 2.9012, 'learning_rate': 2.9421986464239987e-05, 'epoch': 0.82}\n",
            "{'loss': 2.8984, 'learning_rate': 2.8278763490031098e-05, 'epoch': 0.87}\n",
            "{'loss': 2.8615, 'learning_rate': 2.7135540515822205e-05, 'epoch': 0.91}\n",
            "{'loss': 2.8583, 'learning_rate': 2.599231754161332e-05, 'epoch': 0.96}\n",
            " 50% 10934/21868 [11:35<11:38, 15.65it/s][INFO|trainer.py:1995] 2021-10-26 03:51:55,782 >> Saving model checkpoint to models/cnn/checkpoint-10934\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 03:51:55,787 >> Configuration saved in models/cnn/checkpoint-10934/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 03:51:58,083 >> Model weights saved in models/cnn/checkpoint-10934/pytorch_model.bin\n",
            "{'loss': 2.8352, 'learning_rate': 2.4849094567404426e-05, 'epoch': 1.01}\n",
            "{'loss': 2.4634, 'learning_rate': 2.370587159319554e-05, 'epoch': 1.05}\n",
            "{'loss': 2.5131, 'learning_rate': 2.2562648618986647e-05, 'epoch': 1.1}\n",
            "{'loss': 2.4991, 'learning_rate': 2.1419425644777758e-05, 'epoch': 1.14}\n",
            "{'loss': 2.4836, 'learning_rate': 2.027620267056887e-05, 'epoch': 1.19}\n",
            "{'loss': 2.4856, 'learning_rate': 1.913297969635998e-05, 'epoch': 1.23}\n",
            "{'loss': 2.4798, 'learning_rate': 1.7989756722151087e-05, 'epoch': 1.28}\n",
            "{'loss': 2.4925, 'learning_rate': 1.68465337479422e-05, 'epoch': 1.33}\n",
            "{'loss': 2.4693, 'learning_rate': 1.570331077373331e-05, 'epoch': 1.37}\n",
            "{'loss': 2.5012, 'learning_rate': 1.4560087799524418e-05, 'epoch': 1.42}\n",
            "{'loss': 2.4808, 'learning_rate': 1.341686482531553e-05, 'epoch': 1.46}\n",
            "{'loss': 2.47, 'learning_rate': 1.227364185110664e-05, 'epoch': 1.51}\n",
            "{'loss': 2.4818, 'learning_rate': 1.113041887689775e-05, 'epoch': 1.55}\n",
            "{'loss': 2.4684, 'learning_rate': 9.987195902688861e-06, 'epoch': 1.6}\n",
            "{'loss': 2.483, 'learning_rate': 8.843972928479972e-06, 'epoch': 1.65}\n",
            "{'loss': 2.4625, 'learning_rate': 7.700749954271082e-06, 'epoch': 1.69}\n",
            "{'loss': 2.4659, 'learning_rate': 6.557526980062191e-06, 'epoch': 1.74}\n",
            "{'loss': 2.4539, 'learning_rate': 5.414304005853302e-06, 'epoch': 1.78}\n",
            "{'loss': 2.4859, 'learning_rate': 4.271081031644412e-06, 'epoch': 1.83}\n",
            "{'loss': 2.4985, 'learning_rate': 3.1278580574355227e-06, 'epoch': 1.87}\n",
            "{'loss': 2.4834, 'learning_rate': 1.9846350832266325e-06, 'epoch': 1.92}\n",
            "{'loss': 2.4811, 'learning_rate': 8.414121090177429e-07, 'epoch': 1.97}\n",
            "100% 21868/21868 [23:26<00:00, 15.91it/s][INFO|trainer.py:1995] 2021-10-26 04:03:46,648 >> Saving model checkpoint to models/cnn/checkpoint-21868\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 04:03:46,654 >> Configuration saved in models/cnn/checkpoint-21868/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 04:03:48,917 >> Model weights saved in models/cnn/checkpoint-21868/pytorch_model.bin\n",
            "[INFO|trainer.py:1409] 2021-10-26 04:03:53,474 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1413.3985, 'train_samples_per_second': 30.944, 'train_steps_per_second': 15.472, 'train_loss': 2.7460515015474596, 'epoch': 2.0}\n",
            "100% 21868/21868 [23:33<00:00, 15.47it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-26 04:03:53,478 >> Saving model checkpoint to models/cnn\n",
            "[INFO|configuration_utils.py:413] 2021-10-26 04:03:53,482 >> Configuration saved in models/cnn/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-26 04:03:55,839 >> Model weights saved in models/cnn/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-26 04:03:55,849 >> tokenizer config file saved in models/cnn/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-26 04:03:55,852 >> Special tokens file saved in models/cnn/special_tokens_map.json\n",
            "10/26/2021 04:03:58 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-10-26 04:03:58,505 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-10-26 04:03:58,505 >>   Num examples = 6249\n",
            "[INFO|trainer.py:2248] 2021-10-26 04:03:58,505 >>   Batch size = 2\n",
            "100% 3125/3125 [00:50<00:00, 62.49it/s]\n",
            "10/26/2021 04:04:48 - INFO - __main__ - ***** Eval results *****\n",
            "10/26/2021 04:04:48 - INFO - __main__ -   perplexity = 15.634414238534891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKKvXpJitYSY"
      },
      "source": [
        "# Generation and Metrics"
      ],
      "id": "kKKvXpJitYSY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGraCS0wPDr5"
      },
      "source": [
        "def compute_bleu_score(model, dataset):\n",
        "    bleu = load_metric('bleu')\n",
        "    with torch.no_grad():\n",
        "        for i, input in enumerate(test_dataloader):\n",
        "            input = input.to(device)\n",
        "            outputs = model(**input)\n",
        "            last_layer_logits = outputs.logits[:, -1, :]\n",
        "            top_logits = top_k_top_p_filtering(last_layer_logits, top_k=50, top_p=1.0)\n",
        "            probabilities = F.softmax(top_logits, dim=-1)\n",
        "            generated_next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "            predictions = torch.cat([input.input_ids, generated_next_token], dim=-1).squeeze()\n",
        "            references = input.input_ids.squeeze()\n",
        "            bleu.add_batch(predictions=[generated_ids], references=[[input_ids]])\n",
        "    return bleu.compute()\n",
        "\n",
        "def compute_rouge_score(model, dataset):\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    with torch.no_grad():\n",
        "        for i, input in enumerate(test_dataloader):\n",
        "            input = input.to(device)\n",
        "            outputs = model(**input)\n",
        "            last_layer_logits = outputs.logits[:, -1, :]\n",
        "            top_logits = top_k_top_p_filtering(last_layer_logits, top_k=50, top_p=1.0)\n",
        "            probabilities = F.softmax(top_logits, dim=-1)\n",
        "            generated_next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "            predictions = torch.cat([input.input_ids, generated_next_token], dim=-1).squeeze()\n",
        "            references = input.input_ids.squeeze()\n",
        "            rouge.add_batch(predictions=[generated_ids], references=[input_ids])\n",
        "    return rouge.compute()"
      ],
      "id": "yGraCS0wPDr5",
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22bigVoOtay_"
      },
      "source": [
        "## Elon Musk"
      ],
      "id": "22bigVoOtay_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrR5jhhoY5A9",
        "outputId": "d2f87a94-fcf5-4987-8b1c-eb05c5f46b29"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type gpt2 \\\n",
        "  --model_name_or_path \"models/elonmusk\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "id": "MrR5jhhoY5A9",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 03:01:28 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/elonmusk', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "Am excited about the Model 3 unveil and road trip this weekend. There are some huge changes coming to the design as well, so this one is easy & fast. \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "To be clear, I am against artificial intelligence. AI should not be in charge. I want it to be. \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The last remaining soul mate of the late King Arthur was actually Dragan Motavalli. He was one of the greats. \n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "My kids love  @SpaceX  and  @NASA. \n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "It's not just me: I have several cats. I also own a chicken. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUnMq8NbGcUr",
        "outputId": "192fbd6e-0961-4c40-a8fb-9290eb519605"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('models/elonmusk').to(device)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('models/elonmusk')\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "test_dataset = LineByLineTextDataset(tokenizer, 'data/elonmusk/test.txt', tokenizer.model_max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)"
      ],
      "id": "rUnMq8NbGcUr",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egoNjSD2Gsc7",
        "outputId": "32488e1a-a117-402e-9e8e-697f5869641b"
      },
      "source": [
        "bleu_score = compute_bleu_score(model, test_dataloader)\n",
        "bleu_score['bleu']"
      ],
      "id": "egoNjSD2Gsc7",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9621954581957615"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jcpg5S5QIY_",
        "outputId": "56895964-b55e-4ed0-b081-195662786094"
      },
      "source": [
        "rouge_score = compute_rouge_score(model, test_dataloader)\n",
        "rouge_score['rouge1'].mid"
      ],
      "id": "6Jcpg5S5QIY_",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Score(precision=0.9642857142857175, recall=1.0, fmeasure=0.9818181818181791)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdzR6cgpu8WT"
      },
      "source": [
        "## Andrej Karpathy"
      ],
      "id": "HdzR6cgpu8WT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aW_89ZCZr4S",
        "outputId": "67cfd9c9-0a19-4488-ce54-2862492eab77"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type gpt2 \\\n",
        "  --model_name_or_path \"models/karpathy\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "id": "9aW_89ZCZr4S",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 03:01:40 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/karpathy', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "\"How to Start a Startup\" from  @OpenAI  great resource, tips & tricks! \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "A new Deep Learning tutorial for beginners:  the DeepMind GTC231n video   (w)â€™s A New Challenge for Computer Visionists \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The Deep Web is a Hotbed of Violence and Corruption  soooo excited! \n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "#cvpr2016  a week of hacking on the ConvNetCVPR code. Looks like I'm on a roll with the demos \n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "It's quite amusing to watch what is happening behind closed doors with no public hearings or public comment. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58T4wRo7QcOi",
        "outputId": "820ff19b-7054-4c71-b443-f2c1b3161cc0"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('models/karpathy').to(device)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('models/karpathy')\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "test_dataset = LineByLineTextDataset(tokenizer, 'data/karpathy/test.txt', tokenizer.model_max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)"
      ],
      "id": "58T4wRo7QcOi",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n5iXYVYQexQ",
        "outputId": "6a7315e6-dcf7-41ab-ed2f-14a5c3479238"
      },
      "source": [
        "bleu_score = compute_bleu_score(model, test_dataloader)\n",
        "bleu_score['bleu']"
      ],
      "id": "5n5iXYVYQexQ",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9621954581957615"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPCxqCS9QgaO",
        "outputId": "8e0dfc38-acd6-48e3-9b2a-45baac31d11b"
      },
      "source": [
        "rouge_score = compute_rouge_score(model, test_dataloader)\n",
        "rouge_score['rouge1'].mid"
      ],
      "id": "NPCxqCS9QgaO",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Score(precision=0.9642857142857187, recall=1.0, fmeasure=0.9818181818181837)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIqrFCqPvPit"
      },
      "source": [
        "## Barack Obama"
      ],
      "id": "OIqrFCqPvPit"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kPgVWmZZwGz",
        "outputId": "af6b68e1-0df5-49db-b185-6acd3e4dd817"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type gpt2 \\\n",
        "  --model_name_or_path \"models/BarackObama\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "id": "6kPgVWmZZwGz",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 03:01:52 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/BarackObama', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "â€œTheyâ€™re going to try to scare us off education entirely, because no one wants to go to college.â€ \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "\"That said, I know that people can still make their voices heard in Washington. I know it's not easy to do that.\" â€”President Obama \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The Senate must make a timely vote on an American Jobs Act now. The Jobs Act needs to be included in any deal. \n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "There's still more work to do, but this is progress. \n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "It's up to us to work with leaders in both parties to help communities build communities of their own. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imCDY_GLQkhA",
        "outputId": "f19942b2-b23a-41ea-857b-aa7c43953f3d"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('models/BarackObama').to(device)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('models/BarackObama')\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "test_dataset = LineByLineTextDataset(tokenizer, 'data/BarackObama/test.txt', tokenizer.model_max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)"
      ],
      "id": "imCDY_GLQkhA",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWoOG7_bQrAd",
        "outputId": "3e3fa94e-d0b3-4124-ebb2-94c48c07ae7f"
      },
      "source": [
        "bleu_score = compute_bleu_score(model, test_dataloader)\n",
        "bleu_score['bleu']"
      ],
      "id": "mWoOG7_bQrAd",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9621954581957615"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAF36nxKQsCj",
        "outputId": "6857bd47-9d3a-4fdc-91d4-aed908853130"
      },
      "source": [
        "rouge_score = compute_rouge_score(model, test_dataloader)\n",
        "rouge_score['rouge1'].mid"
      ],
      "id": "xAF36nxKQsCj",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Score(precision=0.9642857142857107, recall=1.0, fmeasure=0.9818181818181941)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk2H3NNMm3Uz"
      },
      "source": [
        "## CNN"
      ],
      "id": "Uk2H3NNMm3Uz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okWvdNKBm6PQ",
        "outputId": "0c244aa0-08bb-41a6-d3e9-6e2f20389b90"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type gpt2 \\\n",
        "  --model_name_or_path \"models/cnn\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "id": "okWvdNKBm6PQ",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 04:04:59 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/cnn', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "Obama: The \"threat\" to national security from North Korea is not unique, \"from my conversations with all my colleagues, from this administration, and from the President himself\" \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "A new study finds that babies born with a rare form of brain cancer can develop better health than their non-nuclear counterparts \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The Senate has passed a resolution to censure President Trump for calling former President Joe Biden a 'narcissistic prick' \n",
            "=== GENERATED SEQUENCE 4 ===\n",
            " @HillaryClinton  and Donald Trump campaign to attend the #Election Roundtable at 7p ET - #CNNElection \n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "It's not all fun and games on this tropical storm. Get ready for some epic rain and snow \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5jkbaGDQtpG",
        "outputId": "54a72d55-42bf-438f-b712-817dadd7b91c"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('models/cnn').to(device)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('models/cnn')\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "test_dataset = LineByLineTextDataset(tokenizer, 'data/cnn/test.txt', tokenizer.model_max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)"
      ],
      "id": "M5jkbaGDQtpG",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctF0t-rDQzfS",
        "outputId": "3c1e439e-e1e0-411e-c208-93772e12efcc"
      },
      "source": [
        "bleu_score = compute_bleu_score(model, test_dataloader)\n",
        "bleu_score['bleu']"
      ],
      "id": "ctF0t-rDQzfS",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9621954581957615"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxmI4NCrQ0l3",
        "outputId": "41164855-483c-4ba6-c4fa-e1ab206c9078"
      },
      "source": [
        "rouge_score = compute_rouge_score(model, test_dataloader)\n",
        "rouge_score['rouge1'].mid"
      ],
      "id": "AxmI4NCrQ0l3",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Score(precision=0.9642857142857498, recall=1.0, fmeasure=0.9818181818181965)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqCW96A6m4oa"
      },
      "source": [
        "## WSJ"
      ],
      "id": "pqCW96A6m4oa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VjoCeSIm8q1",
        "outputId": "e084c8cb-16c2-4b87-904d-7dd1c5f4c60e"
      },
      "source": [
        "!python run_generation.py \\\n",
        "  --model_type gpt2 \\\n",
        "  --model_name_or_path \"models/wsj\" \\\n",
        "  --length 50 \\\n",
        "  --prompt \"<BOS>\" \\\n",
        "  --stop_token \"<EOS>\" \\\n",
        "  --k 50 \\\n",
        "  --num_return_sequences 5"
      ],
      "id": "9VjoCeSIm8q1",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/26/2021 04:05:10 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=50, length=50, model_name_or_path='models/wsj', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=5, p=0.9, padding_text='', prompt='<BOS>', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "â€˜Theyâ€™re going to attack again, and itâ€™s going to be a real bloodbath,â€™ says one Republican in Tennessee. â€˜Itâ€™s going to be devastating.â€™ <\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "A new study finds that the number of overweight people is rising.  via  @WSJNY \n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The Senate tax bill was expected to pass Thursday, but Democrats are delaying key provisions \n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "Opinion: Why the Supreme Court hasnâ€™t heard the case on gay marriage. writes  @wjmcgurn \n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "How to be more ambitious: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRkKw3t3Q3c6",
        "outputId": "60ca5b43-348b-49f9-dd19-5456fd0af44b"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('models/wsj').to(device)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('models/wsj')\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "test_dataset = LineByLineTextDataset(tokenizer, 'data/wsj/test.txt', tokenizer.model_max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)"
      ],
      "id": "rRkKw3t3Q3c6",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fBtcHEzRElg",
        "outputId": "777e305a-b8dd-4294-aa15-ff65176773e7"
      },
      "source": [
        "bleu_score = compute_bleu_score(model, test_dataloader)\n",
        "bleu_score['bleu']"
      ],
      "id": "1fBtcHEzRElg",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9621954581957615"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYvyycxRRFnU",
        "outputId": "2439d3db-60e1-4774-b076-b73230151c2f"
      },
      "source": [
        "rouge_score = compute_rouge_score(model, test_dataloader)\n",
        "rouge_score['rouge1'].mid"
      ],
      "id": "YYvyycxRRFnU",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Score(precision=0.9642857142857482, recall=1.0, fmeasure=0.9818181818181964)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    }
  ]
}